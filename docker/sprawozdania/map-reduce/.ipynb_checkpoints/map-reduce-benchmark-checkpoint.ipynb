{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "220be811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docker\n",
    "import uuid\n",
    "import paramiko\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "import requests\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class HadoopRunResult:\n",
    "    name: str\n",
    "    output_path: str\n",
    "    stdout: list[str]\n",
    "    stderr: list[str]\n",
    "    elapsed: str\n",
    "        \n",
    "@dataclass\n",
    "class MultiRunResult:\n",
    "    name: str\n",
    "    results: list[HadoopRunResult]\n",
    "    average: float\n",
    "    output_path: str\n",
    "        \n",
    "    @staticmethod\n",
    "    def fromResults(results: list[HadoopRunResult]):\n",
    "        avg = statistics.mean(list(map(lambda x: int(x.elapsed), results)))\n",
    "        return MultiRunResult(results[0].name, results, avg, results[0].output_path)\n",
    "    \n",
    "def run_n(f, n):\n",
    "    results = []\n",
    "    for i in range(n):\n",
    "        results.append(f())\n",
    "    return results\n",
    "        \n",
    "def get_elapsed_time(res):\n",
    "    def get_id_from_res(res):\n",
    "        for line in res[1]:\n",
    "            m = re.search('job_([0-9_]*)', line)\n",
    "            if m != None and m.group(1) != '':\n",
    "                return m.group(1)\n",
    "        return None\n",
    "    x = requests.get(f'http://resourcemanager:8088/ws/v1/cluster/apps/application_{get_id_from_res(res)}')\n",
    "    return x.json()['app']['elapsedTime']\n",
    "\n",
    "def run_in_master(command):\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(\"namenode\", username=\"root\", password=\"pass\")\n",
    "    ssh_stdin, ssh_stdout, ssh_stderr = ssh.exec_command(f\"cd /app/ && . env_var.sh && {command}\")\n",
    "    return (ssh_stdout.readlines(), ssh_stderr.readlines())\n",
    "\n",
    "def get_data_from_output_path(path):\n",
    "    return f\"{path}/merged.txt\"\n",
    "\n",
    "def print_hdfs_output(path):\n",
    "    raw = run_in_master(f\"hdfs dfs -cat {get_data_from_output_path(path)}\")[0]\n",
    "    print(\"\\n\".join(raw[0:1000]))\n",
    "\n",
    "def merge_results(path):\n",
    "    run_in_master(f\"hdfs dfs -cat {path}/part-r-* | hdfs dfs -put - {path}/merged.txt\")\n",
    "    \n",
    "    \n",
    "def convert_results(all_results: list[MultiRunResult])-> pd.DataFrame:\n",
    "    def convert_single_result(multirun_res: MultiRunResult) -> list[int]:\n",
    "        return [res.elapsed for res in multirun_res.results] + [multirun_res.average]\n",
    "    \n",
    "    runs = len(all_results[0].results)\n",
    "    labels = list(map(lambda x: f\"run_{x} (ms)\", range(runs))) + ['average (ms)']\n",
    "    indexes = []\n",
    "    converted_results = []\n",
    "    for multirun_res in all_results:\n",
    "        indexes.append(multirun_res.name)\n",
    "        converted_results.append(convert_single_result(multirun_res))\n",
    "\n",
    "    return pd.DataFrame(np.array(converted_results), columns = labels, index=indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8879621",
   "metadata": {},
   "source": [
    "# Wp≈Çyw replikacji danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4881d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_covid_01():\n",
    "    covid_01_jar_path = \"/data/master_volume/map_reduce_jars/covid_01.jar\"\n",
    "    covid_01_input_path = \"/datasets/covid-dataset.jsonl\"\n",
    "    covid_01_output_path = \"/out_covid_1\" + str(uuid.uuid4())\n",
    "    res = run_in_master(f\"yarn jar {covid_01_jar_path} {covid_01_input_path} {covid_01_output_path}\")\n",
    "    merge_results(covid_01_output_path)\n",
    "    return HadoopRunResult(\"Covid01\", covid_01_output_path, res[0], res[1], get_elapsed_time(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7f9efe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_covid_02(covid_01_output_path):\n",
    "    covid_02_jar_path = \"/data/master_volume/map_reduce_jars/covid_02.jar\"\n",
    "    covid_02_input_path = get_data_from_output_path(covid_01_output_path)\n",
    "    covid_02_output_path = \"/out_covid_2\" + str(uuid.uuid4())\n",
    "    res = run_in_master(f\"yarn jar {covid_02_jar_path} {covid_02_input_path} {covid_02_output_path}\")\n",
    "    merge_results(covid_02_output_path)\n",
    "    return HadoopRunResult(\"Covid02\",covid_02_output_path, res[0], res[1], get_elapsed_time(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e438ffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_covid_03(covid_01_output_path):\n",
    "    covid_03_jar_path = \"/data/master_volume/map_reduce_jars/covid_03.jar\"\n",
    "    covid_03_input_path = get_data_from_output_path(covid_01_output_path)\n",
    "    covid_03_output_path = \"/out_covid_3\" + str(uuid.uuid4())\n",
    "    res = run_in_master(f\"yarn jar {covid_03_jar_path} {covid_03_input_path} {covid_03_output_path}\")\n",
    "    merge_results(covid_03_output_path)\n",
    "    return HadoopRunResult(\"Covid03\",covid_03_output_path, res[0], res[1], get_elapsed_time(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eddf013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_steam_01():\n",
    "    steam_01_jar_path = \"/data/master_volume/map_reduce_jars/steam_01_combine.jar\"\n",
    "    steam_01_input_path = \"/datasets/steam-dataset/steam_dataset/appinfo/store_data/steam_store_data.jsonl\"\n",
    "    steam_01_input_path2 = \"/datasets/steam-dataset/steam_dataset/steamspy/basic/steam_spy_scrap.jsonl\"\n",
    "    steam_01_output_path = \"/out_steam_1\" + str(uuid.uuid4())\n",
    "    res = run_in_master(f\"yarn jar {steam_01_jar_path} {steam_01_input_path} {steam_01_input_path2} {steam_01_output_path}\")\n",
    "    merge_results(steam_01_output_path)\n",
    "    return HadoopRunResult(\"Steam01\",steam_01_output_path, res[0], res[1], get_elapsed_time(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7dbbd6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_steam_02(steam_01_output_path):\n",
    "    steam_02_jar_path = \"/data/master_volume/map_reduce_jars/steam_02_choose.jar\"\n",
    "    steam_02_input_path = get_data_from_output_path(steam_01_output_path)\n",
    "    steam_02_output_path = \"/out_steam_2\" + str(uuid.uuid4())\n",
    "    res = run_in_master(f\"yarn jar {steam_02_jar_path} {steam_02_input_path} {steam_02_output_path}\")\n",
    "    merge_results(steam_02_output_path)\n",
    "    return HadoopRunResult(\"Steam02\",steam_02_output_path, res[0], res[1], get_elapsed_time(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d14340e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_steam_03(steam_02_output_path):\n",
    "    steam_03_jar_path = \"/data/master_volume/map_reduce_jars/steam_03_takeN.jar\"\n",
    "    steam_03_input_path = get_data_from_output_path(steam_02_output_path)\n",
    "    steam_03_output_path = \"/out_steam_3\" + str(uuid.uuid4())\n",
    "    res = run_in_master(f\"yarn jar {steam_03_jar_path} {steam_03_input_path} {steam_03_output_path}\")\n",
    "    merge_results(steam_03_output_path)\n",
    "    return HadoopRunResult(\"Steam03\",steam_03_output_path, res[0], res[1], get_elapsed_time(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b94f519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_steam_04(steam_03_output_path):\n",
    "    steam_04_jar_path = \"/data/master_volume/map_reduce_jars/steam_04_fetch.jar\"\n",
    "    steam_04_input_path = get_data_from_output_path(steam_03_output_path)\n",
    "    steam_04_output_path = \"/out_steam_4\" + str(uuid.uuid4())\n",
    "    res = run_in_master(f\"yarn jar {steam_04_jar_path} {steam_04_input_path} {steam_04_output_path}\")\n",
    "    merge_results(steam_04_output_path)\n",
    "    return HadoopRunResult(\"Steam04\",steam_04_output_path, res[0], res[1], get_elapsed_time(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d6b99294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_steam_05(steam_04_output_path, covid_02_output_path):\n",
    "    steam_05_jar_path = \"/data/master_volume/map_reduce_jars/steam_05_merge_time.jar\"\n",
    "    steam_05_input_path = get_data_from_output_path(steam_04_output_path)\n",
    "    steam_05_input_path2 = get_data_from_output_path(covid_02_output_path)\n",
    "    steam_05_output_path = \"/out_steam_5\" + str(uuid.uuid4())\n",
    "    res = run_in_master(f\"yarn jar {steam_05_jar_path} {steam_05_input_path} {steam_05_input_path2} {steam_05_output_path}\")\n",
    "    merge_results(steam_05_output_path)\n",
    "    return HadoopRunResult(\"Steam05\",steam_05_output_path, res[0], res[1], get_elapsed_time(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3810f57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(repeat_count: int):\n",
    "    STEP_COUNT = repeat_count\n",
    "    covid_01 = MultiRunResult.fromResults(run_n(run_covid_01, STEP_COUNT))\n",
    "    covid_02 = MultiRunResult.fromResults(run_n(lambda: run_covid_02(covid_01.output_path), STEP_COUNT))\n",
    "    covid_03 = MultiRunResult.fromResults(run_n(lambda: run_covid_03(covid_01.output_path), STEP_COUNT))\n",
    "    steam_01 = MultiRunResult.fromResults(run_n(run_steam_01, STEP_COUNT))\n",
    "    steam_02 = MultiRunResult.fromResults(run_n(lambda: run_steam_02(steam_01.output_path), STEP_COUNT))\n",
    "    steam_03 = MultiRunResult.fromResults(run_n(lambda: run_steam_03(steam_02.output_path), STEP_COUNT))\n",
    "    steam_04 = MultiRunResult.fromResults(run_n(lambda: run_steam_04(steam_03.output_path), STEP_COUNT))\n",
    "    steam_05 = MultiRunResult.fromResults(run_n(lambda: run_steam_05(steam_04.output_path, covid_02.output_path), STEP_COUNT))\n",
    "    return [covid_01, covid_02, covid_03, steam_01, steam_02, steam_03, steam_04, steam_05]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6c4253",
   "metadata": {},
   "source": [
    "## 1 replika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e74455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_in_master(\"hdfs dfs -setrep -R 1 /\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b45322",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_result = run_pipeline(3)\n",
    "convert_results(pipeline_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64560676",
   "metadata": {},
   "source": [
    "## 3 repliki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ecc1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_in_master(\"hdfs dfs -setrep -R 3 /\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf16e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_result = run_pipeline(3)\n",
    "convert_results(pipeline_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
