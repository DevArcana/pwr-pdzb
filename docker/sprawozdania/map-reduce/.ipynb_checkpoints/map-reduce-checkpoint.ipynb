{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import docker\n",
    "import uuid\n",
    "import paramiko\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def run_in_master(command):\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(\"namenode\", username=\"root\", password=\"pass\")\n",
    "    ssh_stdin, ssh_stdout, ssh_stderr = ssh.exec_command(f\"cd /app/ && . /env_var_path.sh && {command}\")\n",
    "    return (ssh_stdout.readlines(), ssh_stderr.readlines())\n",
    "\n",
    "def get_data_from_output_path(path):\n",
    "    return f\"{path}/merged.txt\"\n",
    "\n",
    "def print_hdfs_output(path):\n",
    "    raw = run_in_master(f\"hdfs dfs -cat {get_data_from_output_path(path)}\")[0]\n",
    "    print(\"\\n\".join(raw[0:1000]))\n",
    "\n",
    "def merge_results(path):\n",
    "    run_in_master(f\"hdfs dfs -cat {path}/part-r-* | hdfs dfs -put - {path}/merged.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wybór kolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([], ['bash: env_var_path.sh: No such file or directory\\n'])\n"
     ]
    }
   ],
   "source": [
    "covid_01_jar_path = \"/data/master_volume/map_reduce_jars/covid_01.jar\"\n",
    "covid_01_input_path = \"/datasets/covid-dataset.jsonl\"\n",
    "covid_01_output_path = \"/out_covid_1\" + str(uuid.uuid4())\n",
    "\n",
    "res = run_in_master(f\"yarn jar {covid_01_jar_path} {covid_01_input_path} {covid_01_output_path}\")\n",
    "print(res)\n",
    "merge_results(covid_01_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print_hdfs_output(covid_01_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wybór wymiaru czasu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([], ['bash: env_var_path.sh: No such file or directory\\n'])\n"
     ]
    }
   ],
   "source": [
    "covid_02_jar_path = \"/data/master_volume/map_reduce_jars/covid_02.jar\"\n",
    "covid_02_input_path = get_data_from_output_path(covid_01_output_path)\n",
    "covid_02_output_path = \"/out_covid_2\" + str(uuid.uuid4())\n",
    "\n",
    "res = run_in_master(f\"yarn jar {covid_02_jar_path} {covid_02_input_path} {covid_02_output_path}\")\n",
    "print(res)\n",
    "merge_results(covid_02_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print_hdfs_output(covid_02_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covid final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([], ['bash: env_var_path.sh: No such file or directory\\n'])\n"
     ]
    }
   ],
   "source": [
    "covid_03_jar_path = \"/data/master_volume/map_reduce_jars/covid_03.jar\"\n",
    "covid_03_input_path = get_data_from_output_path(covid_01_output_path)\n",
    "covid_03_output_path = \"/out_covid_3\" + str(uuid.uuid4())\n",
    "\n",
    "res = run_in_master(f\"yarn jar {covid_03_jar_path} {covid_03_input_path} {covid_03_output_path}\")\n",
    "print(res)\n",
    "merge_results(covid_03_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print_hdfs_output(covid_03_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregacja danych ze źródeł Steam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([], ['bash: env_var_path.sh: No such file or directory\\n'])\n"
     ]
    }
   ],
   "source": [
    "steam_01_jar_path = \"/data/master_volume/map_reduce_jars/steam_01_combine.jar\"\n",
    "steam_01_input_path = \"/datasets/steam-dataset/steam_dataset/appinfo/store_data/steam_store_data.jsonl\"\n",
    "steam_01_input_path2 = \"/datasets/steam-dataset/steam_dataset/steamspy/basic/steam_spy_scrap.jsonl\"\n",
    "steam_01_output_path = \"/out_steam_1\" + str(uuid.uuid4())\n",
    "\n",
    "res = run_in_master(f\"yarn jar {steam_01_jar_path} {steam_01_input_path} {steam_01_input_path2} {steam_01_output_path}\")\n",
    "print(res)\n",
    "merge_results(steam_01_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print_hdfs_output(steam_01_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selekcja gier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([], ['bash: env_var_path.sh: No such file or directory\\n'])\n"
     ]
    }
   ],
   "source": [
    "steam_02_jar_path = \"/data/master_volume/map_reduce_jars/steam_02_choose.jar\"\n",
    "steam_02_input_path = get_data_from_output_path(steam_01_output_path)\n",
    "steam_02_output_path = \"/out_steam_2\" + str(uuid.uuid4())\n",
    "\n",
    "res = run_in_master(f\"yarn jar {steam_02_jar_path} {steam_02_input_path} {steam_02_output_path}\")\n",
    "print(res)\n",
    "merge_results(steam_02_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print_hdfs_output(steam_02_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wybór pierwszych N gier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([], ['bash: env_var_path.sh: No such file or directory\\n'])\n"
     ]
    }
   ],
   "source": [
    "steam_03_jar_path = \"/data/master_volume/map_reduce_jars/steam_03_takeN.jar\"\n",
    "steam_03_input_path = get_data_from_output_path(steam_02_output_path)\n",
    "steam_03_output_path = \"/out_steam_3\" + str(uuid.uuid4())\n",
    "\n",
    "res = run_in_master(f\"yarn jar {steam_03_jar_path} {steam_03_input_path} {steam_03_output_path}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print_hdfs_output(steam_03_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pobranie danych ze Steam Charts API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([], ['bash: env_var_path.sh: No such file or directory\\n'])\n"
     ]
    }
   ],
   "source": [
    "steam_04_jar_path = \"/data/master_volume/map_reduce_jars/steam_04_fetch.jar\"\n",
    "steam_04_input_path = get_data_from_output_path(steam_03_output_path)\n",
    "steam_04_output_path = \"/out_steam_4\" + str(uuid.uuid4())\n",
    "\n",
    "res = run_in_master(f\"yarn jar {steam_04_jar_path} {steam_04_input_path} {steam_04_output_path}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print_hdfs_output(steam_04_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Złączenie z wymiarem czasu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([], ['bash: env_var_path.sh: No such file or directory\\n'])\n"
     ]
    }
   ],
   "source": [
    "steam_05_jar_path = \"/data/master_volume/map_reduce_jars/steam_05_merge_time.jar\"\n",
    "steam_05_input_path = get_data_from_output_path(steam_04_output_path)\n",
    "steam_05_input_path2 = get_data_from_output_path(covid_02_output_path)\n",
    "steam_05_output_path = \"/out_steam_5\" + str(uuid.uuid4())\n",
    "\n",
    "res = run_in_master(f\"yarn jar {steam_05_jar_path} {steam_05_input_path} {steam_05_input_path2} {steam_05_output_path}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print_hdfs_output(steam_05_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fragmenty kodu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selekcja gier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "  class MyMapper extends HadoopJob.HadoopMapper[AnyRef, Text, Text, Text] {\n",
    "    override def myMap(key: AnyRef, value: Text, emit: (Text, Text) => Unit): Unit = {\n",
    "      value.toString\n",
    "        .split(\"\\n\")\n",
    "        .map(x => x.dropWhile(!_.isWhitespace))\n",
    "        .toList\n",
    "        .flatMap(x => SteamJoined.decoder.decodeJson(x).toOption)\n",
    "        .filter(game => before2020(game.release_date))\n",
    "        .filter(game => overwhelminglyPositive(game))\n",
    "        .filter(game => game.ccu > 5000)\n",
    "        .filter(game => has_many_owners(game))\n",
    "        .foreach(x => emit(Text(x.game_id.toString), Text(x.toJson)))\n",
    "    }\n",
    "\n",
    "    private def overwhelminglyPositive(steamJoined: SteamJoined): Boolean =\n",
    "      steamJoined.positive > 30000 &&\n",
    "        (steamJoined.positive.toDouble / (steamJoined.positive + steamJoined.negative)) > 0.8\n",
    "\n",
    "    private def before2020(date: String): Boolean = {\n",
    "      Try(isoFormat.parse(date)) match\n",
    "        case Failure(_)     => false\n",
    "        case Success(value) => value.get(java.time.temporal.ChronoField.YEAR) < 2020\n",
    "    }\n",
    "\n",
    "    private def has_many_owners(steamJoined: SteamJoined): Boolean =\n",
    "      Try(steamJoined.owners.replace(\",\", \"\").split('.').head.trim.toInt) match\n",
    "        case Failure(_)     => false\n",
    "        case Success(value) => value > 5000000\n",
    "  }\n",
    "\n",
    "  class MyReducer extends HadoopJob.HadoopReducer[Text, Text, Text] {\n",
    "    override def myReduce(key: Text, values: List[String], emit: (Text, Text) => Unit): Unit = {\n",
    "      values.foreach(x => emit(key, Text(x)))\n",
    "    }\n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wybór pierwszych N gier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "  class MyMapper extends HadoopJob.HadoopMapper[AnyRef, Text, Text, Text] {\n",
    "    override def myMap(key: AnyRef, value: Text, emit: (Text, Text) => Unit): Unit = {\n",
    "      val jsons  = value.toString.split(\"\\n\").map(x => x.dropWhile(!_.isWhitespace)).toList\n",
    "      val mapped = jsons.flatMap(x => SteamJoined.decoder.decodeJson(x).toOption)\n",
    "      val result = mapped.map(x => Result(x.game_id, x.name))\n",
    "\n",
    "      result.foreach(x => emit(Text(\"id\"), Text(x.toJson)))\n",
    "    }\n",
    "  }\n",
    "\n",
    "  class MyReducer extends HadoopJob.HadoopReducer[Text, Text, Text] {\n",
    "    override def myReduce(key: Text, values: List[String], emit: (Text, Text) => Unit): Unit = {\n",
    "      values.take(20).foreach(x => emit(key, Text(x)))\n",
    "    }\n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pobranie danych ze Steam Charts API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "  class MyMapper extends HadoopJob.HadoopMapper[AnyRef, Text, Text, Text] {\n",
    "    override def myMap(key: AnyRef, value: Text, emit: (Text, Text) => Unit): Unit = {\n",
    "      val jsons  = value.toString.split(\"\\n\").map(x => x.dropWhile(!_.isWhitespace)).map(_.trim).toList\n",
    "      val mapped = jsons.flatMap(x => Input.decoder.decodeJson(x).toOption)\n",
    "\n",
    "      def downloadTimestamps(id: Int) = for {\n",
    "        res  <- zio.http.Client.request(f\"\"\"https://steamcharts.com/app/$id/chart-data.json\"\"\")\n",
    "        data <- res.body.asString\n",
    "        json  = data.fromJson[ApiResult].getOrElse(List.empty)\n",
    "      } yield json\n",
    "\n",
    "      val workflow = ZIO.foreach(mapped)(x => downloadTimestamps(x.game_id).map(res => (x, res)))\n",
    "\n",
    "      val result = runZIO(\n",
    "        workflow\n",
    "          .tapError(err => { ZIO.succeed(emit(Text(\"error!\"), Text(err.getMessage))) })\n",
    "          .orElse(ZIO.succeed(List.empty))\n",
    "          .provide(zio.http.Client.default)\n",
    "      ).map(x => Result(x._1.game_id, x._2.map(y => PlayCount(y.head, y.last))))\n",
    "\n",
    "      result.foreach { x =>\n",
    "        emit(new Text(x.game_id.toString), Text(x.toJson))\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  class MyReducer extends HadoopJob.HadoopReducer[Text, Text, Text] {\n",
    "    override def myReduce(key: Text, values: List[String], emit: (Text, Text) => Unit): Unit = {\n",
    "      values.foreach(x => emit(key, Text(x)))\n",
    "    }\n",
    "  }\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wybór kolumn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "  class MyMapper extends HadoopJob.HadoopMapper[AnyRef, Text, Text, Text] {\n",
    "    override def myMap(key: AnyRef, value: Text, emit: (Text, Text) => Unit): Unit = {\n",
    "      val rawInput    = value.toString.fromJson[Input].toOption.get\n",
    "      val parsedInput = InputParsed(\n",
    "        date = LocalDate.parse(rawInput.date),\n",
    "        country = rawInput.location,\n",
    "        total_cases = rawInput.total_cases.toFloatOption.getOrElse(0),\n",
    "        new_cases = rawInput.new_cases.toFloatOption.getOrElse(0),\n",
    "        total_deaths = rawInput.total_cases.toFloatOption.getOrElse(0),\n",
    "        new_deaths = rawInput.new_deaths.toFloatOption.getOrElse(0),\n",
    "        new_cases_per_million = rawInput.new_cases_per_million.toFloatOption.getOrElse(0.0f)\n",
    "      )\n",
    "      emit(Text(\"id\"), Text(parsedInput.toJson))\n",
    "    }\n",
    "  }\n",
    "\n",
    "  class MyReducer extends HadoopJob.HadoopReducer[Text, Text, Text] {\n",
    "    override def myReduce(key: Text, values: List[String], emit: (Text, Text) => Unit): Unit = {\n",
    "\n",
    "      logRemoteInfo(\"http://195.22.98.134:2138\", \"ReducerStarted\")\n",
    "\n",
    "      val inputs = values.flatMap(_.fromJson[InputParsed].toOption)\n",
    "\n",
    "      logRemoteInfo(\"http://195.22.98.134:2138\", \"ReducerParsedInput\")\n",
    "\n",
    "      val average =  inputs.map(y => y.new_cases_per_million).sum / inputs.length\n",
    "\n",
    "      logRemoteInfo(\"http://195.22.98.134:2138\", \"AverageCalculated\");\n",
    "\n",
    "      inputs\n",
    "        .map(x =>\n",
    "          Output(\n",
    "            x.date,\n",
    "            x.country,\n",
    "            x.total_cases.toInt,\n",
    "            x.new_cases.toInt,\n",
    "            x.total_deaths.toInt,\n",
    "            x.new_deaths.toInt,\n",
    "            average\n",
    "          )\n",
    "        )\n",
    "        .foreach(x => emit(key, Text(x.toJson)))\n",
    "    }\n",
    "  }\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
