{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprawozdanie 6 - akwizycja danych\n",
    "\n",
    "## Środowisko\n",
    "\n",
    "Mamy maszynę wirtualną z Ubuntu postawioną za pomocą Vagrant'a (korzystającego pod spodem z VirtualBox'a). Na tej maszynie wirtualnej stawiamy kontenery Docker'a.\n",
    "\n",
    "Aby ułatwić sobie późniejszą pracę z obrazami na których postawiony jest hadoop postanowiliśmy dodać do master-node volumen na dane (modyfikując skrypty generujące docker-compose). Dzięki temu możemy w wygodny sposób (tj. poprzez wrzucenie do odpowiedniego folderu) przenosić pliki do miejsca, do którego możemy się dostać z poziomu maszyny z hadoopem. Warto zwrócić uwagę, że maszyna wirtualna także posiada taki wolumen, który zapewnia wykorzystanie Vagrant'a.\n",
    "\n",
    "```yaml\n",
    "master:\n",
    "    image: hjben/hadoop-eco:$hadoop_version\n",
    "    hostname: master\n",
    "    container_name: master\n",
    "    privileged: true\n",
    "    ports:\n",
    "      - 8088:8088\n",
    "      - 9870:9870\n",
    "      - 8042:8042\n",
    "      - 10000:10000\n",
    "      - 10002:10002\n",
    "      - 16010:16010\n",
    "    volumes:\n",
    "      - /sys/fs/cgroup:/sys/fs/cgroup\n",
    "      - $hdfs_path:/data/hadoop\n",
    "      - $hadoop_log_path:/usr/local/hadoop/logs\n",
    "      - $hbase_log_path/master:/usr/local/hbase/logs\n",
    "      - $hive_log_path:/usr/local/hive/logs\n",
    "      - $sqoop_log_path:/usr/local/sqoop/logs\n",
    "      - /vagrant/master_volume:/data/master_volume <-------------- dodany volumen\n",
    "    networks:\n",
    "      hadoop-cluster:\n",
    "        ipv4_address: 10.1.2.3\n",
    "    extra_hosts:\n",
    "      - \"mariadb:10.1.2.2\"\n",
    "      - \"master:10.1.2.3\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pobieranie danych\n",
    "Niestety przez potrzebę generowania i podania klucza API do serwisu kaggle przed pobraniem danych należy wykonać kilka czynności.\n",
    "\n",
    "1. Pobrać ze strony kaggle klucz API (kaggle.json)\n",
    "2. Stworzyć folder .kaggle w głównym katalogu użytkownika i skopiować tam klucz API\n",
    "3. Poniższe funkcje:\n",
    "    1. Pobierają z kaggle:\n",
    "       * [YouTube Trending Video Dataset](https://www.kaggle.com/datasets/rsrishav/youtube-trending-video-dataset)\n",
    "       * [Steam Dataset](https://www.kaggle.com/datasets/souyama/steam-dataset)\n",
    "    2. Pobierają z sieci [dane Covid'owe](https://covid.ourworldindata.org/data/owid-covid-data.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-06T14:16:38.652199300Z",
     "start_time": "2023-05-06T14:16:38.050540100Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from timeit import default_timer as timer\n",
    "import requests\n",
    "import docker\n",
    "import json\n",
    "import opendatasets as od\n",
    "import csv\n",
    "import pandas\n",
    "import paramiko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-06T14:11:36.137358Z",
     "start_time": "2023-05-06T14:11:36.121207700Z"
    }
   },
   "outputs": [],
   "source": [
    "output_dir = \"/data/master_volume/datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets\n",
      "map_reduce_jars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"ls /data/master_volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T12:42:05.534496Z",
     "start_time": "2023-04-23T12:42:05.522809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset youtube-trending-video-dataset already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(f\"{output_dir}/youtube-trending-video-dataset\"):\n",
    "    od.download(\"https://www.kaggle.com/datasets/rsrishav/youtube-trending-video-dataset\", f\"{output_dir}\")\n",
    "else:\n",
    "    print(\"Dataset youtube-trending-video-dataset already exists, skipping download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset steam-dataset already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(f\"{output_dir}/steam-dataset\"):\n",
    "    od.download(\"https://www.kaggle.com/datasets/souyama/steam-dataset\", f\"{output_dir}\")\n",
    "else:\n",
    "    print(\"Dataset steam-dataset already exists, skipping download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T12:42:10.223587Z",
     "start_time": "2023-04-23T12:42:10.207948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset covid-dataset already exists, skipping download\n",
      "covid-dataset.csv: 77.76MB\n"
     ]
    }
   ],
   "source": [
    "path = f\"{output_dir}/covid-dataset.csv\"\n",
    "\n",
    "if not os.path.isfile(path):\n",
    "    print(f\"Downloading covid-dataset to {path}\")\n",
    "    start = timer()\n",
    "    r = requests.get(\"https://covid.ourworldindata.org/data/owid-covid-data.csv\", allow_redirects=True)\n",
    "    with open(path, 'wb') as file:\n",
    "        file.write(r.content)\n",
    "    end = timer()\n",
    "    print(f\"Download finished in {end - start:.02f}s\")\n",
    "else:\n",
    "    print(\"Dataset covid-dataset already exists, skipping download\")\n",
    "\n",
    "print(f\"covid-dataset.csv: {os.stat(path).st_size / (1024 * 1024):.02f}MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatowanie danych\n",
    "Po rozpakowaniu danych widać, że część z nich ma format trudny do późniejszej pracy. Ostatecznie postanowiliśmy przed wrzuceniem plików do hdfs wszystkie przetransformować do dormatu .jsonl. Format .jsonl zawiera obiekty json, każdy w kolejnej linii. Dzięki zastosowaniu takiego formatu łatwo będzie można implementować procesy map-reduce.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-06T14:37:42.716725600Z",
     "start_time": "2023-05-06T14:37:31.658548100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting CSV to JSONL\n",
      "336 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "\n",
    "print(\"Converting CSV to JSONL\")\n",
    "for root, directories, files in os.walk(f\"{output_dir}\"):\n",
    "        for filename in files:\n",
    "            path = os.path.join(root,filename)\n",
    "            output_path = path.replace(\".csv\", \".jsonl\")\n",
    "\n",
    "            if os.path.isfile(output_path):\n",
    "                # os.remove(output_path)\n",
    "                continue\n",
    "\n",
    "            if path.endswith(\".csv\"):\n",
    "                try:\n",
    "                    print(path)\n",
    "                    with open(path, 'r', encoding='utf-8', errors='replace') as infile, open(output_path, 'w', encoding='utf-8', errors='replace') as outfile:\n",
    "                        reader = csv.reader(x.replace('\\0', '') for x in infile)\n",
    "                        headers = next(reader)\n",
    "                        data = list(reader)\n",
    "                        df = pandas.DataFrame(data, columns=headers)\n",
    "                        for row in df.to_dict('records'):\n",
    "                            json.dump(row, outfile)\n",
    "                            outfile.write('\\n')\n",
    "                    print(output_path)\n",
    "                except Exception as e:\n",
    "                    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T12:42:59.178869Z",
     "start_time": "2023-04-23T12:42:59.162844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting JSON to JSONL\n",
      "Done\n",
      "430 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "\n",
    "print(\"Converting JSON to JSONL\")\n",
    "for root, directories, files in os.walk(f\"{output_dir}\"):\n",
    "        for filename in files:\n",
    "            path = os.path.join(root,filename)\n",
    "            output_path = f\"{path}l\"\n",
    "\n",
    "            if os.path.isfile(output_path):\n",
    "                continue\n",
    "\n",
    "            if path.endswith(\".json\"):\n",
    "                print(path)\n",
    "                with open(path, \"r\") as file:\n",
    "                    data = json.load(file)\n",
    "                    if type(data) is dict:\n",
    "                        data = [{\"key\": key, \"value\": data[key]} for key in data]\n",
    "\n",
    "                    with open(output_path, \"w\") as jsonl_file:\n",
    "                        for obj in data:\n",
    "                            json.dump(obj, jsonl_file)\n",
    "                            jsonl_file.write(\"\\n\")\n",
    "                    print(output_path)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dodanie plików do hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_in_master(command):\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(\"namenode\", username=\"root\", password=\"pass\")\n",
    "    ssh_stdin, ssh_stdout, ssh_stderr = ssh.exec_command(f\"cd /app/ && . /env_var_path.sh && {command}\")\n",
    "    return (ssh_stdout.readlines(), ssh_stderr.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdfs_mkdir(path):\n",
    "    run_in_master(f\"hdfs dfs -mkdir -p /{path}/\")\n",
    "\n",
    "def hdfs_upload(path):\n",
    "    directory = \"/\".join(path.split(\"/\")[:-1])\n",
    "    hdfs_mkdir(directory)\n",
    "    cmd = f\"hdfs dfs -put /data/master_volume/{path} /{directory}\"\n",
    "    print(cmd)\n",
    "    code, output = run_in_master(cmd)\n",
    "    print(f\"exit code {code}\")\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T12:43:31.190297Z",
     "start_time": "2023-04-23T12:43:31.174676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading to HDFS\n",
      "hdfs dfs -put /data/master_volume/datasets/covid-dataset.jsonl /datasets\n",
      "exit code []\n",
      "[]\n",
      "HDFS upload of 601.45MB took 3.25s\n",
      "hdfs dfs -put /data/master_volume/datasets/covid-dataset.csv /datasets\n",
      "exit code []\n",
      "[]\n",
      "HDFS upload of 77.76MB took 2.38s\n",
      "hdfs dfs -put /data/master_volume/datasets/steam-dataset/steam_dataset/steamspy/basic/steam_spy_scrap.jsonl /datasets/steam-dataset/steam_dataset/steamspy/basic\n",
      "exit code []\n",
      "[]\n",
      "HDFS upload of 19.11MB took 2.31s\n",
      "hdfs dfs -put /data/master_volume/datasets/steam-dataset/steam_dataset/appinfo/store_data/steam_store_data.jsonl /datasets/steam-dataset/steam_dataset/appinfo/store_data\n",
      "exit code []\n",
      "[]\n",
      "HDFS upload of 552.30MB took 3.05s\n",
      "hdfs dfs -put /data/master_volume/datasets/youtube-trending-video-dataset/US_youtube_trending_data.csv /datasets/youtube-trending-video-dataset\n",
      "exit code []\n",
      "[]\n",
      "HDFS upload of 283.76MB took 2.72s\n",
      "hdfs dfs -put /data/master_volume/datasets/youtube-trending-video-dataset/US_category_id.jsonl /datasets/youtube-trending-video-dataset\n",
      "exit code []\n",
      "[]\n",
      "HDFS upload of 0.01MB took 2.27s\n",
      "hdfs dfs -put /data/master_volume/datasets/youtube-trending-video-dataset/US_youtube_trending_data.jsonl /datasets/youtube-trending-video-dataset\n",
      "exit code []\n",
      "[]\n",
      "HDFS upload of 338.89MB took 2.79s\n"
     ]
    }
   ],
   "source": [
    "print(\"Uploading to HDFS\")\n",
    "\n",
    "for root, directories, files in os.walk(f\"{output_dir}\"):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".json\"):\n",
    "                continue # skip JSON files, we have JSONL from previous step\n",
    "            filepath = path = os.path\\\n",
    "                .join(root,filename)\n",
    "\n",
    "            path = filepath\\\n",
    "                .replace(\"/data/master_volume/\", \"\")\\\n",
    "                .replace(\"\\\\\", \"/\")\n",
    "\n",
    "            start_single = timer()\n",
    "            hdfs_upload(path)\n",
    "            end_single = timer()\n",
    "            print(f\"HDFS upload of {os.stat(filepath).st_size / (1024 * 1024):.02f}MB took {end_single - start_single:.02f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], ['bash: env_var.sh: No such file or directory\\n'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_in_master(\"hdfs dfs -setrep -R 3 /\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
