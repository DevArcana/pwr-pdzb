{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ef2223e",
   "metadata": {},
   "source": [
    "# Sprawozdanie 11\n",
    "\n",
    "**Grupa A3:**\n",
    "\n",
    "inż. Michał Liss\n",
    "\n",
    "inż. Marceli Sokólski\n",
    "\n",
    "inż. Piotr Krzystanek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d323e4c6",
   "metadata": {},
   "source": [
    "## Definicje funkcji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e9afb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paramiko\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaaca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_in_master(command):\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(\"namenode\", username=\"root\", password=\"pass\")\n",
    "    ssh_stdin, ssh_stdout, ssh_stderr = ssh.exec_command(f\"cd /app/ && . /env_var_path.sh && {command}\")\n",
    "    return (ssh_stdout.readlines(), ssh_stderr.readlines())\n",
    "\n",
    "def merge_results(path):\n",
    "    run_in_master(f\"hdfs dfs -cat {path}/part-* | hdfs dfs -put - {path}/merged.txt\")\n",
    "    \n",
    "def get_data_from_output_path(path):\n",
    "    return f\"{path}/merged.txt\"\n",
    "\n",
    "def print_hdfs_output(path):\n",
    "    raw = run_in_master(f\"hdfs dfs -cat {get_data_from_output_path(path)}\")[0]\n",
    "    print(\"\\n\".join(raw[0:11]))\n",
    "    \n",
    "def get_time(res, max_attempts: int = 6):\n",
    "    def get_id(res):\n",
    "        for line in res[1]:\n",
    "            m = re.search('tracking URL: http://resourcemanager:8088/proxy/(.*)/', line)\n",
    "            if m != None and m.group(1) != '':\n",
    "                return m.group(1)\n",
    "        return ''\n",
    "\n",
    "    def get_time_from_data(data):\n",
    "        sum = 0\n",
    "        for attemp in data['attempts']:\n",
    "            if attemp['completed'] is False:\n",
    "                # print('spark history server has not updated (yet)')\n",
    "                return -1\n",
    "            sum = sum + attemp['duration']\n",
    "        return sum\n",
    "        \n",
    "    id = get_id(res)\n",
    "    if id == -1:\n",
    "        return -1\n",
    "\n",
    "    attempt = 0\n",
    "\n",
    "    while attempt < max_attempts:\n",
    "        attempt = attempt + 1\n",
    "        response = requests.get(f'http://namenode:18080/api/v1/applications/{id}')\n",
    "        if not response.ok:\n",
    "            print('WARNING: application error')\n",
    "            return -1\n",
    "            \n",
    "        data = json.loads(response.text)\n",
    "        t = get_time_from_data(data)\n",
    "\n",
    "        if t >= 0:\n",
    "            return t\n",
    "        time.sleep(5)\n",
    "    print('WARNING: maximum attempts exceeded')\n",
    "    return -1\n",
    "\n",
    "runs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f4de77",
   "metadata": {},
   "source": [
    "# Covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ebc643",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements_covid_sql = []\n",
    "measurements_covid_df = []\n",
    "measurements_covid_scala = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3b5b2c",
   "metadata": {},
   "source": [
    "## SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c046400-316d-493f-b65f-959c5959e400",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for r in range(runs):\n",
    "    run_in_master(f\"hdfs dfs -rm -r /spark-result/covid/sql\")\n",
    "    measurements_covid_sql.append(get_time(run_in_master(\"spark-submit --master yarn --deploy-mode cluster /data/master_volume/spark_scripts/covid_sql.py\")))\n",
    "    print(f\"run {r} took {measurements_covid_sql[-1]}ms\")\n",
    "    \n",
    "merge_results(\"/spark-result/covid/sql\")\n",
    "print_hdfs_output(\"/spark-result/covid/sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6a8ca5",
   "metadata": {},
   "source": [
    "## DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88760500",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(runs):\n",
    "    run_in_master(f\"hdfs dfs -rm -r /spark-result/covid/df\")\n",
    "    measurements_covid_df.append(get_time(run_in_master(\"spark-submit --master yarn --deploy-mode cluster /data/master_volume/spark_scripts/covid_df.py\")))\n",
    "    print(f\"run {r} took {measurements_covid_df[-1]}ms\")\n",
    "    \n",
    "merge_results(\"/spark-result/covid/df\")\n",
    "print_hdfs_output(\"/spark-result/covid/df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c165253b",
   "metadata": {},
   "source": [
    "## Scala Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823ec653",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(runs):\n",
    "    run_in_master(f\"hdfs dfs -rm -r /spark-result/covid01\")\n",
    "    measurements_covid_scala.append(get_time(run_in_master(\"spark-submit \\\n",
    "--master yarn \\\n",
    "--deploy-mode cluster \\\n",
    "--class covid01.Main \\\n",
    "/data/master_volume/spark_scripts/spark.jar \")))\n",
    "    print(f\"run {r} took {measurements_covid_scala[-1]}ms\")\n",
    "    \n",
    "merge_results(\"/spark-result/covid01\")\n",
    "print_hdfs_output(\"/spark-result/covid01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d686c3",
   "metadata": {},
   "source": [
    "# Steam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2898d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements_steam_sql = []\n",
    "measurements_steam_df = []\n",
    "measurements_steam_scala = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec69493",
   "metadata": {},
   "source": [
    "## SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c55026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(runs):\n",
    "    run_in_master(f\"hdfs dfs -rm -r /spark-result/steam/sql\")\n",
    "    measurements_steam_sql.append(get_time(run_in_master(\"spark-submit --master yarn --deploy-mode cluster /data/master_volume/spark_scripts/steam_sql.py\")))\n",
    "    print(f\"run {r} took {measurements_steam_sql[-1]}ms\")\n",
    "    \n",
    "merge_results(\"/spark-result/steam/sql\")\n",
    "print_hdfs_output(\"/spark-result/steam/sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5297f9",
   "metadata": {},
   "source": [
    "## DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbc603e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for r in range(runs):\n",
    "    run_in_master(f\"hdfs dfs -rm -r /spark-result/steam/df\")\n",
    "    measurements_steam_df.append(get_time(run_in_master(\"spark-submit --master yarn --deploy-mode cluster /data/master_volume/spark_scripts/steam_df.py\")))\n",
    "    print(f\"run {r} took {measurements_steam_df[-1]}ms\")\n",
    "    \n",
    "merge_results(\"/spark-result/steam/df\")\n",
    "print_hdfs_output(\"/spark-result/steam/df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cd0f0a",
   "metadata": {},
   "source": [
    "## Scala Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a64c587",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(runs):\n",
    "    run_in_master(f\"hdfs dfs -rm -r /spark-result/steam01\")\n",
    "    measurements_steam_scala.append(get_time(run_in_master(\"spark-submit \\\n",
    "--master yarn \\\n",
    "--deploy-mode cluster \\\n",
    "--class steam01.Main \\\n",
    "/data/master_volume/spark_scripts/spark.jar \")))\n",
    "    print(f\"run {r} took {measurements_steam_scala[-1]}ms\")\n",
    "    \n",
    "merge_results(\"/spark-result/steam01\")\n",
    "print_hdfs_output(\"/spark-result/steam01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a62e7e1",
   "metadata": {},
   "source": [
    "# Wyniki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e661985",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_s_d = \" | \".join([str(x) for x in measurements_steam_df])\n",
    "m_s_s = \" | \".join([str(x) for x in measurements_steam_sql])\n",
    "m_c_d = \" | \".join([str(x) for x in measurements_covid_df])\n",
    "m_c_s = \" | \".join([str(x) for x in measurements_covid_sql])\n",
    "\n",
    "print(f\"|     |  covid  |  steam  |\")\n",
    "print(f\"|-----|---------|---------|\")\n",
    "print(f\"| df  | {sum(measurements_covid_df) / len(measurements_covid_df)} | {sum(measurements_steam_df) / len(measurements_steam_df)} |\")\n",
    "print(f\"| sql | {sum(measurements_covid_sql) / len(measurements_covid_sql)} | {sum(measurements_steam_sql) / len(measurements_steam_sql)} |\")\n",
    "\n",
    "print()\n",
    "r = \" | \".join([f\"run {x}\" for x in range(runs)])\n",
    "print(f\"|         | {r} |\")\n",
    "print(f\"|steam df |{m_s_d}|\")\n",
    "print(f\"|steam sql|{m_s_s}|\")\n",
    "print(f\"|covid df |{m_c_d}|\")\n",
    "print(f\"|covid sql|{m_c_s}|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5637f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements_steam_scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6b6226",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_s_d = \" | \".join([str(x) for x in measurements_steam_df])\n",
    "m_s_s = \" | \".join([str(x) for x in measurements_steam_sql])\n",
    "m_s_scala = \" | \".join([str(x) for x in measurements_steam_scala])\n",
    "m_c_d = \" | \".join([str(x) for x in measurements_covid_df])\n",
    "m_c_s = \" | \".join([str(x) for x in measurements_covid_sql])\n",
    "m_c_scala =\" | \".join([str(x) for x in measurements_covid_scala])\n",
    "\n",
    "print(f\"|       |  covid  |  steam  |\")\n",
    "print(f\"|-------|---------|---------|\")\n",
    "print(f\"| df    | {sum(measurements_covid_df) / len(measurements_covid_df)} | {sum(measurements_steam_df) / len(measurements_steam_df)} |\")\n",
    "print(f\"| sql   | {sum(measurements_covid_sql) / len(measurements_covid_sql)} | {sum(measurements_steam_sql) / len(measurements_steam_sql)} |\")\n",
    "print(f\"| scala | {sum(measurements_covid_scala) / len(measurements_covid_scala)} | {sum(measurements_steam_scala) / len(measurements_steam_scala)} |\")\n",
    "\n",
    "print()\n",
    "r = \" | \".join([f\"run {x}\" for x in range(runs)])\n",
    "print(f\"|           | {r} |\")\n",
    "print(f\"|steam df   |{m_s_d}|\")\n",
    "print(f\"|steam sql  |{m_s_s}|\")\n",
    "print(f\"|steam sca  |{m_s_scala}|\")\n",
    "print(f\"|covid df   |{m_c_d}|\")\n",
    "print(f\"|covid sql  |{m_c_s}|\")\n",
    "print(f\"|covid scala|{m_c_scala}|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafe21dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
