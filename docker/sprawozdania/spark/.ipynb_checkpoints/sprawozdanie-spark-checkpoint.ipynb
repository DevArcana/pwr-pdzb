{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ef2223e",
   "metadata": {},
   "source": [
    "# Sprawozdanie 11\n",
    "\n",
    "**Grupa A3:**\n",
    "\n",
    "inż. Michał Liss\n",
    "\n",
    "inż. Marceli Sokólski\n",
    "\n",
    "inż. Piotr Krzystanek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d323e4c6",
   "metadata": {},
   "source": [
    "## Definicje funkcji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93e9afb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paramiko\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eaaca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_in_master(command):\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(\"namenode\", username=\"root\", password=\"pass\")\n",
    "    ssh_stdin, ssh_stdout, ssh_stderr = ssh.exec_command(f\"cd /app/ && . /env_var_path.sh && {command}\")\n",
    "    return (ssh_stdout.readlines(), ssh_stderr.readlines())\n",
    "\n",
    "def merge_results(path):\n",
    "    run_in_master(f\"hdfs dfs -cat {path}/part-* | hdfs dfs -put - {path}/merged.txt\")\n",
    "    \n",
    "def get_data_from_output_path(path):\n",
    "    return f\"{path}/merged.txt\"\n",
    "\n",
    "def print_hdfs_output(path):\n",
    "    raw = run_in_master(f\"hdfs dfs -cat {get_data_from_output_path(path)}\")[0]\n",
    "    print(\"\\n\".join(raw[0:11]))\n",
    "    \n",
    "def get_time(res, max_attempts: int = 6):\n",
    "    def get_id(res):\n",
    "        for line in res[1]:\n",
    "            m = re.search('tracking URL: http://resourcemanager:8088/proxy/(.*)/', line)\n",
    "            if m != None and m.group(1) != '':\n",
    "                return m.group(1)\n",
    "        return ''\n",
    "\n",
    "    def get_time_from_data(data):\n",
    "        sum = 0\n",
    "        for attemp in data['attempts']:\n",
    "            if attemp['completed'] is False:\n",
    "                # print('spark history server has not updated (yet)')\n",
    "                return -1\n",
    "            sum = sum + attemp['duration']\n",
    "        return sum\n",
    "        \n",
    "    id = get_id(res)\n",
    "    if id == -1:\n",
    "        return -1\n",
    "\n",
    "    attempt = 0\n",
    "\n",
    "    while attempt < max_attempts:\n",
    "        attempt = attempt + 1\n",
    "        response = requests.get(f'http://namenode:18080/api/v1/applications/{id}')\n",
    "        if not response.ok:\n",
    "            print('WARNING: application error')\n",
    "            return -1\n",
    "            \n",
    "        data = json.loads(response.text)\n",
    "        t = get_time_from_data(data)\n",
    "\n",
    "        if t >= 0:\n",
    "            return t\n",
    "        time.sleep(5)\n",
    "    print('WARNING: maximum attempts exceeded')\n",
    "    return -1\n",
    "\n",
    "runs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f4de77",
   "metadata": {},
   "source": [
    "# Covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51ebc643",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements_covid_sql = []\n",
    "measurements_covid_df = []\n",
    "measurements_covid_scala = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3b5b2c",
   "metadata": {},
   "source": [
    "## SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c046400-316d-493f-b65f-959c5959e400",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 0 took 14605ms\n",
      "date,location,total_cases,new_cases,total_deaths,new_deaths,new_cases_per_million,average_new_cases_per_million\n",
      "\n",
      "2020-01-03,Afghanistan,,0,,0,0,160.00584439903545\n",
      "\n",
      "2020-01-04,Afghanistan,,0,,0,0,160.00584439903545\n",
      "\n",
      "2020-01-05,Afghanistan,,0,,0,0,160.00584439903545\n",
      "\n",
      "2020-01-06,Afghanistan,,0,,0,0,160.00584439903545\n",
      "\n",
      "2020-01-07,Afghanistan,,0,,0,0,160.00584439903545\n",
      "\n",
      "2020-01-08,Afghanistan,,0,,0,0,160.00584439903545\n",
      "\n",
      "2020-01-09,Afghanistan,,0,,0,0,160.00584439903545\n",
      "\n",
      "2020-01-10,Afghanistan,,0,,0,0,160.00584439903545\n",
      "\n",
      "2020-01-11,Afghanistan,,0,,0,0,160.00584439903545\n",
      "\n",
      "2020-01-12,Afghanistan,,0,,0,0,160.00584439903545\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r in range(runs):\n",
    "    run_in_master(f\"hdfs dfs -rm -r /spark-result/covid/sql\")\n",
    "    measurements_covid_sql.append(get_time(run_in_master(\"spark-submit --master yarn --deploy-mode cluster /data/master_volume/spark_scripts/covid_sql.py\")))\n",
    "    print(f\"run {r} took {measurements_covid_sql[-1]}ms\")\n",
    "    \n",
    "merge_results(\"/spark-result/covid/sql\")\n",
    "print_hdfs_output(\"/spark-result/covid/sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6a8ca5",
   "metadata": {},
   "source": [
    "## DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88760500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 0 took 14352ms\n",
      "date,location,total_cases,new_cases,total_deaths,new_deaths,new_cases_per_million,average_new_cases_per_million\n",
      "\n",
      "2020-01-03,Afghanistan,0,0,0,0,0,160.00584439903545\n",
      "\n",
      "2020-01-04,Afghanistan,0,0,0,0,0,160.00584439903545\n",
      "\n",
      "2020-01-05,Afghanistan,0,0,0,0,0,160.00584439903545\n",
      "\n",
      "2020-01-06,Afghanistan,0,0,0,0,0,160.00584439903545\n",
      "\n",
      "2020-01-07,Afghanistan,0,0,0,0,0,160.00584439903545\n",
      "\n",
      "2020-01-08,Afghanistan,0,0,0,0,0,160.00584439903545\n",
      "\n",
      "2020-01-09,Afghanistan,0,0,0,0,0,160.00584439903545\n",
      "\n",
      "2020-01-10,Afghanistan,0,0,0,0,0,160.00584439903545\n",
      "\n",
      "2020-01-11,Afghanistan,0,0,0,0,0,160.00584439903545\n",
      "\n",
      "2020-01-12,Afghanistan,0,0,0,0,0,160.00584439903545\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r in range(runs):\n",
    "    run_in_master(f\"hdfs dfs -rm -r /spark-result/covid/df\")\n",
    "    measurements_covid_df.append(get_time(run_in_master(\"spark-submit --master yarn --deploy-mode cluster /data/master_volume/spark_scripts/covid_df.py\")))\n",
    "    print(f\"run {r} took {measurements_covid_df[-1]}ms\")\n",
    "    \n",
    "merge_results(\"/spark-result/covid/df\")\n",
    "print_hdfs_output(\"/spark-result/covid/df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a5cfde",
   "metadata": {},
   "source": [
    "## Scala Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e70272de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 0 took 16273ms\n",
      "{\"date\":\"2020-01-03\",\"location\":\"Afghanistan\",\"total_cases\":0,\"new_cases\":0,\"total_deaths\":0,\"new_deaths\":0,\"new_cases_per_million\":0.0,\"average_new_cases_per_million\":160.29720723141685}\n",
      "\n",
      "{\"date\":\"2020-01-04\",\"location\":\"Afghanistan\",\"total_cases\":0,\"new_cases\":0,\"total_deaths\":0,\"new_deaths\":0,\"new_cases_per_million\":0.0,\"average_new_cases_per_million\":160.29720723141685}\n",
      "\n",
      "{\"date\":\"2020-01-05\",\"location\":\"Afghanistan\",\"total_cases\":0,\"new_cases\":0,\"total_deaths\":0,\"new_deaths\":0,\"new_cases_per_million\":0.0,\"average_new_cases_per_million\":160.29720723141685}\n",
      "\n",
      "{\"date\":\"2020-01-06\",\"location\":\"Afghanistan\",\"total_cases\":0,\"new_cases\":0,\"total_deaths\":0,\"new_deaths\":0,\"new_cases_per_million\":0.0,\"average_new_cases_per_million\":160.29720723141685}\n",
      "\n",
      "{\"date\":\"2020-01-07\",\"location\":\"Afghanistan\",\"total_cases\":0,\"new_cases\":0,\"total_deaths\":0,\"new_deaths\":0,\"new_cases_per_million\":0.0,\"average_new_cases_per_million\":160.29720723141685}\n",
      "\n",
      "{\"date\":\"2020-01-08\",\"location\":\"Afghanistan\",\"total_cases\":0,\"new_cases\":0,\"total_deaths\":0,\"new_deaths\":0,\"new_cases_per_million\":0.0,\"average_new_cases_per_million\":160.29720723141685}\n",
      "\n",
      "{\"date\":\"2020-01-09\",\"location\":\"Afghanistan\",\"total_cases\":0,\"new_cases\":0,\"total_deaths\":0,\"new_deaths\":0,\"new_cases_per_million\":0.0,\"average_new_cases_per_million\":160.29720723141685}\n",
      "\n",
      "{\"date\":\"2020-01-10\",\"location\":\"Afghanistan\",\"total_cases\":0,\"new_cases\":0,\"total_deaths\":0,\"new_deaths\":0,\"new_cases_per_million\":0.0,\"average_new_cases_per_million\":160.29720723141685}\n",
      "\n",
      "{\"date\":\"2020-01-11\",\"location\":\"Afghanistan\",\"total_cases\":0,\"new_cases\":0,\"total_deaths\":0,\"new_deaths\":0,\"new_cases_per_million\":0.0,\"average_new_cases_per_million\":160.29720723141685}\n",
      "\n",
      "{\"date\":\"2020-01-12\",\"location\":\"Afghanistan\",\"total_cases\":0,\"new_cases\":0,\"total_deaths\":0,\"new_deaths\":0,\"new_cases_per_million\":0.0,\"average_new_cases_per_million\":160.29720723141685}\n",
      "\n",
      "{\"date\":\"2020-01-13\",\"location\":\"Afghanistan\",\"total_cases\":0,\"new_cases\":0,\"total_deaths\":0,\"new_deaths\":0,\"new_cases_per_million\":0.0,\"average_new_cases_per_million\":160.29720723141685}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r in range(runs):\n",
    "    run_in_master(f\"hdfs dfs -rm -r /spark-result/covid01\")\n",
    "    measurements_covid_scala.append(get_time(run_in_master(\"spark-submit \\\n",
    "--master yarn \\\n",
    "--deploy-mode cluster \\\n",
    "--class covid01.Main \\\n",
    "/data/master_volume/spark_scripts/spark.jar \")))\n",
    "    print(f\"run {r} took {measurements_covid_scala[-1]}ms\")\n",
    "    \n",
    "merge_results(\"/spark-result/covid01\")\n",
    "print_hdfs_output(\"/spark-result/covid01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d686c3",
   "metadata": {},
   "source": [
    "# Steam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2898d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements_steam_sql = []\n",
    "measurements_steam_df = []\n",
    "measurements_steam_scala = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec69493",
   "metadata": {},
   "source": [
    "## SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c55026d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 0 took 13701ms\n",
      "steam_appid,coming_soon,date,appid,name,positive,negative,owners,ccu\n",
      "\n",
      "907680,false,\"Aug 17, 2018\",907680,Wwbit,710,566,\"100,000 .. 200,000\",0\n",
      "\n",
      "823550,false,\"Sep 18, 2018\",823550,Booty Calls,255,294,\"100,000 .. 200,000\",134\n",
      "\n",
      "639780,false,\"Dec 7, 2017\",639780,Deep Space Waifu: FLAT JUSTICE,1449,67,\"50,000 .. 100,000\",4\n",
      "\n",
      "steam_appid,coming_soon,date,appid,name,positive,negative,owners,ccu\n",
      "\n",
      "896890,false,\"Dec 23, 2019\",896890,VR Paradise - Steam Edition,138,50,\"20,000 .. 50,000\",11\n",
      "\n",
      "726360,false,\"May 22, 2020\",726360,BOOBS SAGA: Prepare To Hentai Edition,367,103,\"20,000 .. 50,000\",0\n",
      "\n",
      "723090,false,\"Oct 24, 2017\",723090,Meltys Quest,446,10,\"20,000 .. 50,000\",36\n",
      "\n",
      "712790,false,\"Oct 2, 2017\",712790,Crimson Memories,51,21,\"20,000 .. 50,000\",0\n",
      "\n",
      "825300,false,\"Nov 22, 2018\",825300,To Trust an Incubus,43,5,\"0 .. 20,000\",1\n",
      "\n",
      "937730,false,\"Dec 24, 2018\",937730,Lady's Hentai Mosaic,21,4,\"0 .. 20,000\",0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r in range(runs):\n",
    "    run_in_master(f\"hdfs dfs -rm -r /spark-result/steam/sql\")\n",
    "    measurements_steam_sql.append(get_time(run_in_master(\"spark-submit --master yarn --deploy-mode cluster /data/master_volume/spark_scripts/steam_sql.py\")))\n",
    "    print(f\"run {r} took {measurements_steam_sql[-1]}ms\")\n",
    "    \n",
    "merge_results(\"/spark-result/steam/sql\")\n",
    "print_hdfs_output(\"/spark-result/steam/sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5297f9",
   "metadata": {},
   "source": [
    "## DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bbc603e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 0 took 13904ms\n",
      "steam_appid,coming_soon,date,appid,name,positive,negative,owners,ccu\n",
      "\n",
      "907680,false,\"Aug 17, 2018\",907680,Wwbit,710,566,\"100,000 .. 200,000\",0\n",
      "\n",
      "823550,false,\"Sep 18, 2018\",823550,Booty Calls,255,294,\"100,000 .. 200,000\",134\n",
      "\n",
      "639780,false,\"Dec 7, 2017\",639780,Deep Space Waifu: FLAT JUSTICE,1449,67,\"50,000 .. 100,000\",4\n",
      "\n",
      "steam_appid,coming_soon,date,appid,name,positive,negative,owners,ccu\n",
      "\n",
      "896890,false,\"Dec 23, 2019\",896890,VR Paradise - Steam Edition,138,50,\"20,000 .. 50,000\",11\n",
      "\n",
      "726360,false,\"May 22, 2020\",726360,BOOBS SAGA: Prepare To Hentai Edition,367,103,\"20,000 .. 50,000\",0\n",
      "\n",
      "723090,false,\"Oct 24, 2017\",723090,Meltys Quest,446,10,\"20,000 .. 50,000\",36\n",
      "\n",
      "712790,false,\"Oct 2, 2017\",712790,Crimson Memories,51,21,\"20,000 .. 50,000\",0\n",
      "\n",
      "825300,false,\"Nov 22, 2018\",825300,To Trust an Incubus,43,5,\"0 .. 20,000\",1\n",
      "\n",
      "937730,false,\"Dec 24, 2018\",937730,Lady's Hentai Mosaic,21,4,\"0 .. 20,000\",0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r in range(runs):\n",
    "    run_in_master(f\"hdfs dfs -rm -r /spark-result/steam/df\")\n",
    "    measurements_steam_df.append(get_time(run_in_master(\"spark-submit --master yarn --deploy-mode cluster /data/master_volume/spark_scripts/steam_df.py\")))\n",
    "    print(f\"run {r} took {measurements_steam_df[-1]}ms\")\n",
    "    \n",
    "merge_results(\"/spark-result/steam/df\")\n",
    "print_hdfs_output(\"/spark-result/steam/df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72aec32",
   "metadata": {},
   "source": [
    "## Scala Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2764ce3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 0 took 15286ms\n",
      "{\"key\":\"555310\",\"value\":{\"game_id\":555310,\"name\":\"Satellite\",\"positive\":44,\"negative\":9,\"owners\":\"0 .. 20,000\",\"ccu\":0,\"release_date\":\"Feb 23, 2018\"}}\n",
      "\n",
      "{\"key\":\"639780\",\"value\":{\"game_id\":639780,\"name\":\"Deep Space Waifu: FLAT JUSTICE\",\"positive\":1449,\"negative\":67,\"owners\":\"50,000 .. 100,000\",\"ccu\":4,\"release_date\":\"Dec 7, 2017\"}}\n",
      "\n",
      "{\"key\":\"677730\",\"value\":{\"game_id\":677730,\"name\":\"Karmasutra\",\"positive\":13,\"negative\":6,\"owners\":\"0 .. 20,000\",\"ccu\":0,\"release_date\":\"Sep 29, 2017\"}}\n",
      "\n",
      "{\"key\":\"712790\",\"value\":{\"game_id\":712790,\"name\":\"Crimson Memories\",\"positive\":51,\"negative\":21,\"owners\":\"20,000 .. 50,000\",\"ccu\":0,\"release_date\":\"Oct 2, 2017\"}}\n",
      "\n",
      "{\"key\":\"823550\",\"value\":{\"game_id\":823550,\"name\":\"Booty Calls\",\"positive\":255,\"negative\":294,\"owners\":\"100,000 .. 200,000\",\"ccu\":134,\"release_date\":\"Sep 18, 2018\"}}\n",
      "\n",
      "{\"key\":\"868980\",\"value\":{\"game_id\":868980,\"name\":\"DEEP SPACE WAIFU: NEKOMIMI\",\"positive\":362,\"negative\":6,\"owners\":\"0 .. 20,000\",\"ccu\":0,\"release_date\":\"Dec 18, 2018\"}}\n",
      "\n",
      "{\"key\":\"929290\",\"value\":{\"game_id\":929290,\"name\":\"Strip Breaker : Hentai Girls\",\"positive\":8,\"negative\":7,\"owners\":\"0 .. 20,000\",\"ccu\":0,\"release_date\":\"Sep 30, 2018\"}}\n",
      "\n",
      "{\"key\":\"929300\",\"value\":{\"game_id\":929300,\"name\":\"Chroma : Sexy Hentai Girls\",\"positive\":22,\"negative\":6,\"owners\":\"0 .. 20,000\",\"ccu\":0,\"release_date\":\"Oct 2, 2018\"}}\n",
      "\n",
      "{\"key\":\"929310\",\"value\":{\"game_id\":929310,\"name\":\"Kamasutra Connect : Sexy Hentai Girls\",\"positive\":11,\"negative\":5,\"owners\":\"0 .. 20,000\",\"ccu\":0,\"release_date\":\"Nov 13, 2018\"}}\n",
      "\n",
      "{\"key\":\"937730\",\"value\":{\"game_id\":937730,\"name\":\"Lady's Hentai Mosaic\",\"positive\":21,\"negative\":4,\"owners\":\"0 .. 20,000\",\"ccu\":0,\"release_date\":\"Dec 24, 2018\"}}\n",
      "\n",
      "{\"key\":\"962380\",\"value\":{\"game_id\":962380,\"name\":\"HOT FIT!\",\"positive\":28,\"negative\":7,\"owners\":\"0 .. 20,000\",\"ccu\":1,\"release_date\":\"Nov 30, 2018\"}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r in range(runs):\n",
    "    run_in_master(f\"hdfs dfs -rm -r /spark-result/steam01\")\n",
    "    measurements_steam_scala.append(get_time(run_in_master(\"spark-submit \\\n",
    "--master yarn \\\n",
    "--deploy-mode cluster \\\n",
    "--class steam01.Main \\\n",
    "/data/master_volume/spark_scripts/spark.jar \")))\n",
    "    print(f\"run {r} took {measurements_steam_scala[-1]}ms\")\n",
    "    \n",
    "merge_results(\"/spark-result/steam01\")\n",
    "print_hdfs_output(\"/spark-result/steam01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a62e7e1",
   "metadata": {},
   "source": [
    "# Wyniki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e661985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     |  covid  |  steam  |\n",
      "|-----|---------|---------|\n",
      "| df  | 14352.0 | 13904.0 |\n",
      "| sql | 14605.0 | 13701.0 |\n",
      "\n",
      "|         | run 0 |\n",
      "|steam df |13904|\n",
      "|steam sql|13701|\n",
      "|covid df |14352|\n",
      "|covid sql|14605|\n"
     ]
    }
   ],
   "source": [
    "m_s_d = \" | \".join([str(x) for x in measurements_steam_df])\n",
    "m_s_s = \" | \".join([str(x) for x in measurements_steam_sql])\n",
    "m_c_d = \" | \".join([str(x) for x in measurements_covid_df])\n",
    "m_c_s = \" | \".join([str(x) for x in measurements_covid_sql])\n",
    "\n",
    "print(f\"|     |  covid  |  steam  |\")\n",
    "print(f\"|-----|---------|---------|\")\n",
    "print(f\"| df  | {sum(measurements_covid_df) / len(measurements_covid_df)} | {sum(measurements_steam_df) / len(measurements_steam_df)} |\")\n",
    "print(f\"| sql | {sum(measurements_covid_sql) / len(measurements_covid_sql)} | {sum(measurements_steam_sql) / len(measurements_steam_sql)} |\")\n",
    "\n",
    "print()\n",
    "r = \" | \".join([f\"run {x}\" for x in range(runs)])\n",
    "print(f\"|         | {r} |\")\n",
    "print(f\"|steam df |{m_s_d}|\")\n",
    "print(f\"|steam sql|{m_s_s}|\")\n",
    "print(f\"|covid df |{m_c_d}|\")\n",
    "print(f\"|covid sql|{m_c_s}|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69470b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15286]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measurements_steam_scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0ff9f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       |  covid  |  steam  |\n",
      "|-------|---------|---------|\n",
      "| df    | 14352.0 | 13904.0 |\n",
      "| sql   | 14605.0 | 13701.0 |\n",
      "| scala | 16273.0 | 15286.0 |\n",
      "\n",
      "|           | run 0 |\n",
      "|steam df   |13904|\n",
      "|steam sql  |13701|\n",
      "|steam sca  |15286|\n",
      "|covid df   |14352|\n",
      "|covid sql  |14605|\n",
      "|covid scala|16273|\n"
     ]
    }
   ],
   "source": [
    "m_s_d = \" | \".join([str(x) for x in measurements_steam_df])\n",
    "m_s_s = \" | \".join([str(x) for x in measurements_steam_sql])\n",
    "m_s_scala = \" | \".join([str(x) for x in measurements_steam_scala])\n",
    "m_c_d = \" | \".join([str(x) for x in measurements_covid_df])\n",
    "m_c_s = \" | \".join([str(x) for x in measurements_covid_sql])\n",
    "m_c_scala =\" | \".join([str(x) for x in measurements_covid_scala])\n",
    "\n",
    "print(f\"|       |  covid  |  steam  |\")\n",
    "print(f\"|-------|---------|---------|\")\n",
    "print(f\"| df    | {sum(measurements_covid_df) / len(measurements_covid_df)} | {sum(measurements_steam_df) / len(measurements_steam_df)} |\")\n",
    "print(f\"| sql   | {sum(measurements_covid_sql) / len(measurements_covid_sql)} | {sum(measurements_steam_sql) / len(measurements_steam_sql)} |\")\n",
    "print(f\"| scala | {sum(measurements_covid_scala) / len(measurements_covid_scala)} | {sum(measurements_steam_scala) / len(measurements_steam_scala)} |\")\n",
    "\n",
    "print()\n",
    "r = \" | \".join([f\"run {x}\" for x in range(runs)])\n",
    "print(f\"|           | {r} |\")\n",
    "print(f\"|steam df   |{m_s_d}|\")\n",
    "print(f\"|steam sql  |{m_s_s}|\")\n",
    "print(f\"|steam sca  |{m_s_scala}|\")\n",
    "print(f\"|covid df   |{m_c_d}|\")\n",
    "print(f\"|covid sql  |{m_c_s}|\")\n",
    "print(f\"|covid scala|{m_c_scala}|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d4adbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
