{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93e9afb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paramiko\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9eaaca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_in_master(command):\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(\"namenode\", username=\"root\", password=\"pass\")\n",
    "    ssh_stdin, ssh_stdout, ssh_stderr = ssh.exec_command(f\"cd /app/ && . /env_var_path.sh && {command}\")\n",
    "    return (ssh_stdout.readlines(), ssh_stderr.readlines())\n",
    "\n",
    "def merge_results(path):\n",
    "    run_in_master(f\"hdfs dfs -cat {path}/part-* | hdfs dfs -put - {path}/merged.txt\")\n",
    "    \n",
    "def get_data_from_output_path(path):\n",
    "    return f\"{path}/merged.txt\"\n",
    "\n",
    "def print_hdfs_output(path):\n",
    "    raw = run_in_master(f\"hdfs dfs -cat {get_data_from_output_path(path)}\")[0]\n",
    "    print(\"\\n\".join(raw[0:1000]))\n",
    "\n",
    "def get_time(res, max_attempts: int = 4):\n",
    "    def get_id(res):\n",
    "        for line in res[1]:\n",
    "            m = re.search('tracking URL: http://resourcemanager:8088/proxy/(.*)/', line)\n",
    "            if m != None and m.group(1) != '':\n",
    "                return m.group(1)\n",
    "        return ''\n",
    "\n",
    "    def get_time_from_data(data):\n",
    "        sum = 0\n",
    "        for attemp in data['attempts']:\n",
    "            if attemp['completed'] is False:\n",
    "                print('spark history server has not updated (yet)')\n",
    "                return -1\n",
    "            sum = sum + attemp['duration']\n",
    "        return sum\n",
    "        \n",
    "    id = get_id(res)\n",
    "    if id == -1:\n",
    "        return -1\n",
    "\n",
    "    attempt = 0\n",
    "\n",
    "    while attempt < max_attempts:\n",
    "        attempt = attempt + 1\n",
    "        response = requests.get(f'http://namenode:18080/api/v1/applications/{id}')\n",
    "        if not response.ok:\n",
    "            print('WARNING: application error')\n",
    "            return -1\n",
    "            \n",
    "        data = json.loads(response.text)\n",
    "        t = get_time_from_data(data)\n",
    "\n",
    "        if t >= 0:\n",
    "            return t\n",
    "        time.sleep(3)\n",
    "    print('WARNING: maximum attempts exceeded')\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c046400-316d-493f-b65f-959c5959e400",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tutaj odpalenie pythona\n",
    "# Potrzebna akwizycja wcześniej! (dane covid)\n",
    "run_in_master(f\"hdfs dfs -rm -r /spark-result\")\n",
    "res01 = run_in_master(\"spark-submit --master yarn --deploy-mode cluster /data/master_volume/spark_scripts/test.py\")\n",
    "_ = merge_results(\"/spark-result/sql-select\")\n",
    "print(get_time(res01))\n",
    "print_hdfs_output(\"/spark-result/sql-select\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4d8c9bb-6a58-4110-be3c-28aa55e6b6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19492"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_time(res01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b743230-3c27-4aa4-b4da-8992537d3fe7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " ['23/06/17 15:35:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\\n',\n",
       "  '23/06/17 15:35:54 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at resourcemanager/172.25.0.7:8032\\n',\n",
       "  '23/06/17 15:35:54 INFO AHSProxy: Connecting to Application History server at historyserver/172.25.0.5:10200\\n',\n",
       "  '23/06/17 15:35:54 INFO Configuration: resource-types.xml not found\\n',\n",
       "  \"23/06/17 15:35:54 INFO ResourceUtils: Unable to find 'resource-types.xml'.\\n\",\n",
       "  '23/06/17 15:35:54 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (4096 MB per container)\\n',\n",
       "  '23/06/17 15:35:54 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\\n',\n",
       "  '23/06/17 15:35:54 INFO Client: Setting up container launch context for our AM\\n',\n",
       "  '23/06/17 15:35:54 INFO Client: Setting up the launch environment for our AM container\\n',\n",
       "  '23/06/17 15:35:54 INFO Client: Preparing resources for our AM container\\n',\n",
       "  '23/06/17 15:35:54 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\\n',\n",
       "  '23/06/17 15:35:55 INFO Client: Uploading resource file:/tmp/spark-d86e1197-971e-43cf-b693-d0551e773c5b/__spark_libs__8978038135386315388.zip -> hdfs://namenode:9000/user/root/.sparkStaging/application_1687015972839_0002/__spark_libs__8978038135386315388.zip\\n',\n",
       "  '23/06/17 15:35:56 INFO Client: Uploading resource file:/usr/local/spark/examples/jars/spark-examples_2.13-3.4.0.jar -> hdfs://namenode:9000/user/root/.sparkStaging/application_1687015972839_0002/spark-examples_2.13-3.4.0.jar\\n',\n",
       "  '23/06/17 15:35:56 INFO Client: Uploading resource file:/tmp/spark-d86e1197-971e-43cf-b693-d0551e773c5b/__spark_conf__4025029765067558579.zip -> hdfs://namenode:9000/user/root/.sparkStaging/application_1687015972839_0002/__spark_conf__.zip\\n',\n",
       "  '23/06/17 15:35:56 INFO SecurityManager: Changing view acls to: root\\n',\n",
       "  '23/06/17 15:35:56 INFO SecurityManager: Changing modify acls to: root\\n',\n",
       "  '23/06/17 15:35:56 INFO SecurityManager: Changing view acls groups to: \\n',\n",
       "  '23/06/17 15:35:56 INFO SecurityManager: Changing modify acls groups to: \\n',\n",
       "  '23/06/17 15:35:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\\n',\n",
       "  '23/06/17 15:35:56 INFO Client: Submitting application application_1687015972839_0002 to ResourceManager\\n',\n",
       "  '23/06/17 15:35:56 INFO YarnClientImpl: Submitted application application_1687015972839_0002\\n',\n",
       "  '23/06/17 15:35:57 INFO Client: Application report for application_1687015972839_0002 (state: ACCEPTED)\\n',\n",
       "  '23/06/17 15:35:57 INFO Client: \\n',\n",
       "  '\\t client token: N/A\\n',\n",
       "  '\\t diagnostics: AM container is launched, waiting for AM container to Register with RM\\n',\n",
       "  '\\t ApplicationMaster host: N/A\\n',\n",
       "  '\\t ApplicationMaster RPC port: -1\\n',\n",
       "  '\\t queue: default\\n',\n",
       "  '\\t start time: 1687016156696\\n',\n",
       "  '\\t final status: UNDEFINED\\n',\n",
       "  '\\t tracking URL: http://resourcemanager:8088/proxy/application_1687015972839_0002/\\n',\n",
       "  '\\t user: root\\n',\n",
       "  '23/06/17 15:35:58 INFO Client: Application report for application_1687015972839_0002 (state: ACCEPTED)\\n',\n",
       "  '23/06/17 15:35:59 INFO Client: Application report for application_1687015972839_0002 (state: ACCEPTED)\\n',\n",
       "  '23/06/17 15:36:00 INFO Client: Application report for application_1687015972839_0002 (state: ACCEPTED)\\n',\n",
       "  '23/06/17 15:36:01 INFO Client: Application report for application_1687015972839_0002 (state: RUNNING)\\n',\n",
       "  '23/06/17 15:36:01 INFO Client: \\n',\n",
       "  '\\t client token: N/A\\n',\n",
       "  '\\t diagnostics: N/A\\n',\n",
       "  '\\t ApplicationMaster host: 2b1338f877b3\\n',\n",
       "  '\\t ApplicationMaster RPC port: 38813\\n',\n",
       "  '\\t queue: default\\n',\n",
       "  '\\t start time: 1687016156696\\n',\n",
       "  '\\t final status: UNDEFINED\\n',\n",
       "  '\\t tracking URL: http://resourcemanager:8088/proxy/application_1687015972839_0002/\\n',\n",
       "  '\\t user: root\\n',\n",
       "  '23/06/17 15:36:02 INFO Client: Application report for application_1687015972839_0002 (state: RUNNING)\\n',\n",
       "  '23/06/17 15:36:03 INFO Client: Application report for application_1687015972839_0002 (state: RUNNING)\\n',\n",
       "  '23/06/17 15:36:04 INFO Client: Application report for application_1687015972839_0002 (state: RUNNING)\\n',\n",
       "  '23/06/17 15:36:05 INFO Client: Application report for application_1687015972839_0002 (state: RUNNING)\\n',\n",
       "  '23/06/17 15:36:06 INFO Client: Application report for application_1687015972839_0002 (state: RUNNING)\\n',\n",
       "  '23/06/17 15:36:07 INFO Client: Application report for application_1687015972839_0002 (state: FINISHED)\\n',\n",
       "  '23/06/17 15:36:07 INFO Client: \\n',\n",
       "  '\\t client token: N/A\\n',\n",
       "  '\\t diagnostics: N/A\\n',\n",
       "  '\\t ApplicationMaster host: 2b1338f877b3\\n',\n",
       "  '\\t ApplicationMaster RPC port: 38813\\n',\n",
       "  '\\t queue: default\\n',\n",
       "  '\\t start time: 1687016156696\\n',\n",
       "  '\\t final status: SUCCEEDED\\n',\n",
       "  '\\t tracking URL: http://resourcemanager:8088/proxy/application_1687015972839_0002/\\n',\n",
       "  '\\t user: root\\n',\n",
       "  '23/06/17 15:36:07 INFO ShutdownHookManager: Shutdown hook called\\n',\n",
       "  '23/06/17 15:36:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-d86e1197-971e-43cf-b693-d0551e773c5b\\n',\n",
       "  '23/06/17 15:36:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-bd36e333-2754-4ae1-ae4e-9c9f715b1b9d\\n'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tutaj jak odpalić jarke, jakby było trzeba. Scala 2.13 jest potrzebna.\n",
    "run_in_master(\"spark-submit --master yarn --deploy-mode cluster --class org.apache.spark.examples.SparkPi $SPARK_HOME/examples/jars/spark-examples_2.13-3.4.0.jar 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb2f4fa-5001-4562-a2ec-58c0fb868034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
