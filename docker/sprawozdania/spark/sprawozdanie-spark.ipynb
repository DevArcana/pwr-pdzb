{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ef2223e",
   "metadata": {},
   "source": [
    "# Sprawozdanie 11\n",
    "\n",
    "**Grupa A3:**\n",
    "\n",
    "inż. Michał Liss\n",
    "\n",
    "inż. Marceli Sokólski\n",
    "\n",
    "inż. Piotr Krzystanek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d323e4c6",
   "metadata": {},
   "source": [
    "## Definicje funkcji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93e9afb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paramiko\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eaaca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_in_master(command):\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(\"namenode\", username=\"root\", password=\"pass\")\n",
    "    ssh_stdin, ssh_stdout, ssh_stderr = ssh.exec_command(f\"cd /app/ && . /env_var_path.sh && {command}\")\n",
    "    return (ssh_stdout.readlines(), ssh_stderr.readlines())\n",
    "\n",
    "def merge_results(path):\n",
    "    run_in_master(f\"hdfs dfs -cat {path}/part-* | hdfs dfs -put - {path}/merged.txt\")\n",
    "    \n",
    "def get_data_from_output_path(path):\n",
    "    return f\"{path}/merged.txt\"\n",
    "\n",
    "def print_hdfs_output(path):\n",
    "    raw = run_in_master(f\"hdfs dfs -cat {get_data_from_output_path(path)}\")[0]\n",
    "    print(\"\\n\".join(raw[0:11]))\n",
    "    \n",
    "def get_time(res, max_attempts: int = 6):\n",
    "    def get_id(res):\n",
    "        for line in res[1]:\n",
    "            m = re.search('tracking URL: http://resourcemanager:8088/proxy/(.*)/', line)\n",
    "            if m != None and m.group(1) != '':\n",
    "                return m.group(1)\n",
    "        return ''\n",
    "\n",
    "    def get_time_from_data(data):\n",
    "        sum = 0\n",
    "        for attemp in data['attempts']:\n",
    "            if attemp['completed'] is False:\n",
    "                # print('spark history server has not updated (yet)')\n",
    "                return -1\n",
    "            sum = sum + attemp['duration']\n",
    "        return sum\n",
    "        \n",
    "    id = get_id(res)\n",
    "    if id == -1:\n",
    "        return -1\n",
    "\n",
    "    attempt = 0\n",
    "\n",
    "    while attempt < max_attempts:\n",
    "        attempt = attempt + 1\n",
    "        response = requests.get(f'http://namenode:18080/api/v1/applications/{id}')\n",
    "        if not response.ok:\n",
    "            print('WARNING: application error')\n",
    "            return -1\n",
    "            \n",
    "        data = json.loads(response.text)\n",
    "        t = get_time_from_data(data)\n",
    "\n",
    "        if t >= 0:\n",
    "            return t\n",
    "        time.sleep(5)\n",
    "    print('WARNING: maximum attempts exceeded')\n",
    "    return -1\n",
    "\n",
    "runs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f4de77",
   "metadata": {},
   "source": [
    "# Covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51ebc643",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements_covid_sql = []\n",
    "measurements_covid_df = []\n",
    "measurements_covid_scala = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3b5b2c",
   "metadata": {},
   "source": [
    "## SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c046400-316d-493f-b65f-959c5959e400",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 0 took 13758ms\n",
      "run 1 took 14356ms\n",
      "run 2 took 13586ms\n",
      "run 3 took 13635ms\n",
      "run 4 took 14173ms\n",
      "run 5 took 13840ms\n",
      "run 6 took 13625ms\n",
      "run 7 took 13180ms\n",
      "run 8 took 14037ms\n",
      "run 9 took 13305ms\n",
      "date,location,total_cases,new_cases,total_deaths,new_deaths,new_cases_per_million,average_new_cases_per_million\n",
      "\n",
      "2022-05-03,Curacao,42035,0,273,0,0,160.00584439903545\n",
      "\n",
      "2022-05-04,Curacao,42330,295,274,1,1543,160.00584439903545\n",
      "\n",
      "2022-05-05,Curacao,42330,0,274,0,0,160.00584439903545\n",
      "\n",
      "2022-05-06,Curacao,42330,0,274,0,0,160.00584439903545\n",
      "\n",
      "2022-05-07,Curacao,42330,0,274,0,0,160.00584439903545\n",
      "\n",
      "2022-05-08,Curacao,42330,0,274,0,0,160.00584439903545\n",
      "\n",
      "2022-05-09,Curacao,42330,0,274,0,0,160.00584439903545\n",
      "\n",
      "2022-05-10,Curacao,42330,0,274,0,0,160.00584439903545\n",
      "\n",
      "2022-05-11,Curacao,42674,344,275,1,1799,160.00584439903545\n",
      "\n",
      "2022-05-12,Curacao,42674,0,275,0,0,160.00584439903545\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r in range(runs):\n",
    "    run_in_master(f\"hdfs dfs -rm -r /spark-result/covid/sql\")\n",
    "    measurements_covid_sql.append(get_time(run_in_master(\"spark-submit --master yarn --deploy-mode cluster /data/master_volume/spark_scripts/covid_sql.py\")))\n",
    "    print(f\"run {r} took {measurements_covid_sql[-1]}ms\")\n",
    "    \n",
    "merge_results(\"/spark-result/covid/sql\")\n",
    "print_hdfs_output(\"/spark-result/covid/sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6a8ca5",
   "metadata": {},
   "source": [
    "## DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88760500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 0 took 14344ms\n",
      "run 1 took 13400ms\n",
      "run 2 took 13447ms\n",
      "run 3 took 14340ms\n",
      "run 4 took 13724ms\n",
      "run 5 took 13262ms\n",
      "run 6 took 13354ms\n",
      "run 7 took 14398ms\n",
      "run 8 took 13666ms\n",
      "run 9 took 13652ms\n",
      "date,location,total_cases,new_cases,total_deaths,new_deaths,new_cases_per_million,average_new_cases_per_million\n",
      "\n",
      "2022-05-03,Curacao,42035,0,273,0,0,160.00584439903545\n",
      "\n",
      "2022-05-04,Curacao,42330,295,274,1,1543,160.00584439903545\n",
      "\n",
      "2022-05-05,Curacao,42330,0,274,0,0,160.00584439903545\n",
      "\n",
      "2022-05-06,Curacao,42330,0,274,0,0,160.00584439903545\n",
      "\n",
      "2022-05-07,Curacao,42330,0,274,0,0,160.00584439903545\n",
      "\n",
      "2022-05-08,Curacao,42330,0,274,0,0,160.00584439903545\n",
      "\n",
      "2022-05-09,Curacao,42330,0,274,0,0,160.00584439903545\n",
      "\n",
      "2022-05-10,Curacao,42330,0,274,0,0,160.00584439903545\n",
      "\n",
      "2022-05-11,Curacao,42674,344,275,1,1799,160.00584439903545\n",
      "\n",
      "2022-05-12,Curacao,42674,0,275,0,0,160.00584439903545\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r in range(runs):\n",
    "    run_in_master(f\"hdfs dfs -rm -r /spark-result/covid/df\")\n",
    "    measurements_covid_df.append(get_time(run_in_master(\"spark-submit --master yarn --deploy-mode cluster /data/master_volume/spark_scripts/covid_df.py\")))\n",
    "    print(f\"run {r} took {measurements_covid_df[-1]}ms\")\n",
    "    \n",
    "merge_results(\"/spark-result/covid/df\")\n",
    "print_hdfs_output(\"/spark-result/covid/df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c317a8a2",
   "metadata": {},
   "source": [
    "## Scala Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "990dacde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 0 took 15406ms\n",
      "run 1 took 15395ms\n",
      "run 2 took 15660ms\n",
      "run 3 took 16456ms\n",
      "run 4 took 15502ms\n",
      "run 5 took 15895ms\n",
      "run 6 took 16574ms\n",
      "run 7 took 16308ms\n",
      "run 8 took 15907ms\n",
      "run 9 took 15795ms\n",
      "{\"date\":\"2020-01-03\",\"location\":\"Afghanistan\",\"total_cases\":0,\"new_cases\":0,\"total_deaths\":0,\"new_deaths\":0,\"new_cases_per_million\":0.0,\"average_new_cases_per_million\":160.29720723141685}\n",
      "\n",
      "{\"date\":\"2020-01-04\",\"location\":\"Afghanistan\",\"total_cases\":0,\"new_cases\":0,\"total_deaths\":0,\"new_deaths\":0,\"new_cases_per_million\":0.0,\"average_new_cases_per_million\":160.29720723141685}\n",
      "\n",
      "{\"date\":\"2020-01-05\",\"location\":\"Afghanistan\",\"total_cases\":0,\"new_cases\":0,\"total_deaths\":0,\"new_deaths\":0,\"new_cases_per_million\":0.0,\"average_new_cases_per_million\":160.29720723141685}\n",
      "\n",
      "{\"date\":\"2020-01-06\",\"location\":\"Afghanistan\",\"total_cases\":0,\"new_cases\":0,\"total_deaths\":0,\"new_deaths\":0,\"new_cases_per_million\":0.0,\"average_new_cases_per_million\":160.29720723141685}\n",
      "\n",
      "{\"date\":\"2020-01-07\",\"location\":\"Afghanistan\",\"total_cases\":0,\"new_cases\":0,\"total_deaths\":0,\"new_deaths\":0,\"new_cases_per_million\":0.0,\"average_new_cases_per_million\":160.29720723141685}\n",
      "\n",
      "{\"date\":\"2020-01-08\",\"location\":\"Afghanistan\",\"total_cases\":0,\"new_cases\":0,\"total_deaths\":0,\"new_deaths\":0,\"new_cases_per_million\":0.0,\"average_new_cases_per_million\":160.29720723141685}\n",
      "\n",
      "{\"date\":\"2020-01-09\",\"location\":\"Afghanistan\",\"total_cases\":0,\"new_cases\":0,\"total_deaths\":0,\"new_deaths\":0,\"new_cases_per_million\":0.0,\"average_new_cases_per_million\":160.29720723141685}\n",
      "\n",
      "{\"date\":\"2020-01-10\",\"location\":\"Afghanistan\",\"total_cases\":0,\"new_cases\":0,\"total_deaths\":0,\"new_deaths\":0,\"new_cases_per_million\":0.0,\"average_new_cases_per_million\":160.29720723141685}\n",
      "\n",
      "{\"date\":\"2020-01-11\",\"location\":\"Afghanistan\",\"total_cases\":0,\"new_cases\":0,\"total_deaths\":0,\"new_deaths\":0,\"new_cases_per_million\":0.0,\"average_new_cases_per_million\":160.29720723141685}\n",
      "\n",
      "{\"date\":\"2020-01-12\",\"location\":\"Afghanistan\",\"total_cases\":0,\"new_cases\":0,\"total_deaths\":0,\"new_deaths\":0,\"new_cases_per_million\":0.0,\"average_new_cases_per_million\":160.29720723141685}\n",
      "\n",
      "{\"date\":\"2020-01-13\",\"location\":\"Afghanistan\",\"total_cases\":0,\"new_cases\":0,\"total_deaths\":0,\"new_deaths\":0,\"new_cases_per_million\":0.0,\"average_new_cases_per_million\":160.29720723141685}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r in range(runs):\n",
    "    run_in_master(f\"hdfs dfs -rm -r /spark-result/covid01\")\n",
    "    measurements_covid_scala.append(get_time(run_in_master(\"spark-submit \\\n",
    "--master yarn \\\n",
    "--deploy-mode cluster \\\n",
    "--class covid01.Main \\\n",
    "/data/master_volume/spark_scripts/spark.jar \")))\n",
    "    print(f\"run {r} took {measurements_covid_scala[-1]}ms\")\n",
    "    \n",
    "merge_results(\"/spark-result/covid01\")\n",
    "print_hdfs_output(\"/spark-result/covid01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d686c3",
   "metadata": {},
   "source": [
    "# Steam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2898d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements_steam_sql = []\n",
    "measurements_steam_df = []\n",
    "measurements_steam_scala = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec69493",
   "metadata": {},
   "source": [
    "## SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c55026d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 0 took 14267ms\n",
      "run 1 took 14052ms\n",
      "run 2 took 13415ms\n",
      "run 3 took 13748ms\n",
      "run 4 took 13712ms\n",
      "run 5 took 13854ms\n",
      "run 6 took 13764ms\n",
      "run 7 took 13811ms\n",
      "run 8 took 14529ms\n",
      "run 9 took 13927ms\n",
      "steam_appid,coming_soon,date,appid,name,positive,negative,owners,ccu\n",
      "\n",
      "907680,false,\"Aug 17, 2018\",907680,Wwbit,710,566,\"100,000 .. 200,000\",0\n",
      "\n",
      "823550,false,\"Sep 18, 2018\",823550,Booty Calls,255,294,\"100,000 .. 200,000\",134\n",
      "\n",
      "639780,false,\"Dec 7, 2017\",639780,Deep Space Waifu: FLAT JUSTICE,1449,67,\"50,000 .. 100,000\",4\n",
      "\n",
      "steam_appid,coming_soon,date,appid,name,positive,negative,owners,ccu\n",
      "\n",
      "896890,false,\"Dec 23, 2019\",896890,VR Paradise - Steam Edition,138,50,\"20,000 .. 50,000\",11\n",
      "\n",
      "726360,false,\"May 22, 2020\",726360,BOOBS SAGA: Prepare To Hentai Edition,367,103,\"20,000 .. 50,000\",0\n",
      "\n",
      "723090,false,\"Oct 24, 2017\",723090,Meltys Quest,446,10,\"20,000 .. 50,000\",36\n",
      "\n",
      "712790,false,\"Oct 2, 2017\",712790,Crimson Memories,51,21,\"20,000 .. 50,000\",0\n",
      "\n",
      "825300,false,\"Nov 22, 2018\",825300,To Trust an Incubus,43,5,\"0 .. 20,000\",1\n",
      "\n",
      "937730,false,\"Dec 24, 2018\",937730,Lady's Hentai Mosaic,21,4,\"0 .. 20,000\",0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r in range(runs):\n",
    "    run_in_master(f\"hdfs dfs -rm -r /spark-result/steam/sql\")\n",
    "    measurements_steam_sql.append(get_time(run_in_master(\"spark-submit --master yarn --deploy-mode cluster /data/master_volume/spark_scripts/steam_sql.py\")))\n",
    "    print(f\"run {r} took {measurements_steam_sql[-1]}ms\")\n",
    "    \n",
    "merge_results(\"/spark-result/steam/sql\")\n",
    "print_hdfs_output(\"/spark-result/steam/sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5297f9",
   "metadata": {},
   "source": [
    "## DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bbc603e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 0 took 13859ms\n",
      "run 1 took 13817ms\n",
      "run 2 took 13227ms\n",
      "run 3 took 14443ms\n",
      "run 4 took 14377ms\n",
      "run 5 took 14525ms\n",
      "run 6 took 13436ms\n",
      "run 7 took 13964ms\n",
      "run 8 took 13672ms\n",
      "run 9 took 13799ms\n",
      "steam_appid,coming_soon,date,appid,name,positive,negative,owners,ccu\n",
      "\n",
      "907680,false,\"Aug 17, 2018\",907680,Wwbit,710,566,\"100,000 .. 200,000\",0\n",
      "\n",
      "823550,false,\"Sep 18, 2018\",823550,Booty Calls,255,294,\"100,000 .. 200,000\",134\n",
      "\n",
      "639780,false,\"Dec 7, 2017\",639780,Deep Space Waifu: FLAT JUSTICE,1449,67,\"50,000 .. 100,000\",4\n",
      "\n",
      "steam_appid,coming_soon,date,appid,name,positive,negative,owners,ccu\n",
      "\n",
      "896890,false,\"Dec 23, 2019\",896890,VR Paradise - Steam Edition,138,50,\"20,000 .. 50,000\",11\n",
      "\n",
      "726360,false,\"May 22, 2020\",726360,BOOBS SAGA: Prepare To Hentai Edition,367,103,\"20,000 .. 50,000\",0\n",
      "\n",
      "723090,false,\"Oct 24, 2017\",723090,Meltys Quest,446,10,\"20,000 .. 50,000\",36\n",
      "\n",
      "712790,false,\"Oct 2, 2017\",712790,Crimson Memories,51,21,\"20,000 .. 50,000\",0\n",
      "\n",
      "825300,false,\"Nov 22, 2018\",825300,To Trust an Incubus,43,5,\"0 .. 20,000\",1\n",
      "\n",
      "937730,false,\"Dec 24, 2018\",937730,Lady's Hentai Mosaic,21,4,\"0 .. 20,000\",0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r in range(runs):\n",
    "    run_in_master(f\"hdfs dfs -rm -r /spark-result/steam/df\")\n",
    "    measurements_steam_df.append(get_time(run_in_master(\"spark-submit --master yarn --deploy-mode cluster /data/master_volume/spark_scripts/steam_df.py\")))\n",
    "    print(f\"run {r} took {measurements_steam_df[-1]}ms\")\n",
    "    \n",
    "merge_results(\"/spark-result/steam/df\")\n",
    "print_hdfs_output(\"/spark-result/steam/df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8559ee21",
   "metadata": {},
   "source": [
    "## Scala Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef447ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 0 took 14840ms\n",
      "run 1 took 15516ms\n",
      "run 2 took 16064ms\n",
      "run 3 took 15516ms\n",
      "run 4 took 15416ms\n",
      "run 5 took 16240ms\n",
      "run 6 took 15695ms\n",
      "run 7 took 16246ms\n",
      "run 8 took 15082ms\n",
      "run 9 took 15463ms\n",
      "{\"key\":\"555310\",\"value\":{\"game_id\":555310,\"name\":\"Satellite\",\"positive\":44,\"negative\":9,\"owners\":\"0 .. 20,000\",\"ccu\":0,\"release_date\":\"Feb 23, 2018\"}}\n",
      "\n",
      "{\"key\":\"639780\",\"value\":{\"game_id\":639780,\"name\":\"Deep Space Waifu: FLAT JUSTICE\",\"positive\":1449,\"negative\":67,\"owners\":\"50,000 .. 100,000\",\"ccu\":4,\"release_date\":\"Dec 7, 2017\"}}\n",
      "\n",
      "{\"key\":\"677730\",\"value\":{\"game_id\":677730,\"name\":\"Karmasutra\",\"positive\":13,\"negative\":6,\"owners\":\"0 .. 20,000\",\"ccu\":0,\"release_date\":\"Sep 29, 2017\"}}\n",
      "\n",
      "{\"key\":\"712790\",\"value\":{\"game_id\":712790,\"name\":\"Crimson Memories\",\"positive\":51,\"negative\":21,\"owners\":\"20,000 .. 50,000\",\"ccu\":0,\"release_date\":\"Oct 2, 2017\"}}\n",
      "\n",
      "{\"key\":\"823550\",\"value\":{\"game_id\":823550,\"name\":\"Booty Calls\",\"positive\":255,\"negative\":294,\"owners\":\"100,000 .. 200,000\",\"ccu\":134,\"release_date\":\"Sep 18, 2018\"}}\n",
      "\n",
      "{\"key\":\"868980\",\"value\":{\"game_id\":868980,\"name\":\"DEEP SPACE WAIFU: NEKOMIMI\",\"positive\":362,\"negative\":6,\"owners\":\"0 .. 20,000\",\"ccu\":0,\"release_date\":\"Dec 18, 2018\"}}\n",
      "\n",
      "{\"key\":\"929290\",\"value\":{\"game_id\":929290,\"name\":\"Strip Breaker : Hentai Girls\",\"positive\":8,\"negative\":7,\"owners\":\"0 .. 20,000\",\"ccu\":0,\"release_date\":\"Sep 30, 2018\"}}\n",
      "\n",
      "{\"key\":\"929300\",\"value\":{\"game_id\":929300,\"name\":\"Chroma : Sexy Hentai Girls\",\"positive\":22,\"negative\":6,\"owners\":\"0 .. 20,000\",\"ccu\":0,\"release_date\":\"Oct 2, 2018\"}}\n",
      "\n",
      "{\"key\":\"929310\",\"value\":{\"game_id\":929310,\"name\":\"Kamasutra Connect : Sexy Hentai Girls\",\"positive\":11,\"negative\":5,\"owners\":\"0 .. 20,000\",\"ccu\":0,\"release_date\":\"Nov 13, 2018\"}}\n",
      "\n",
      "{\"key\":\"937730\",\"value\":{\"game_id\":937730,\"name\":\"Lady's Hentai Mosaic\",\"positive\":21,\"negative\":4,\"owners\":\"0 .. 20,000\",\"ccu\":0,\"release_date\":\"Dec 24, 2018\"}}\n",
      "\n",
      "{\"key\":\"962380\",\"value\":{\"game_id\":962380,\"name\":\"HOT FIT!\",\"positive\":28,\"negative\":7,\"owners\":\"0 .. 20,000\",\"ccu\":1,\"release_date\":\"Nov 30, 2018\"}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r in range(runs):\n",
    "    run_in_master(f\"hdfs dfs -rm -r /spark-result/steam01\")\n",
    "    measurements_steam_scala.append(get_time(run_in_master(\"spark-submit \\\n",
    "--master yarn \\\n",
    "--deploy-mode cluster \\\n",
    "--class steam01.Main \\\n",
    "/data/master_volume/spark_scripts/spark.jar \")))\n",
    "    print(f\"run {r} took {measurements_steam_scala[-1]}ms\")\n",
    "    \n",
    "merge_results(\"/spark-result/steam01\")\n",
    "print_hdfs_output(\"/spark-result/steam01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a62e7e1",
   "metadata": {},
   "source": [
    "# Wyniki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e661985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     |  covid  |  steam  |\n",
      "|-----|---------|---------|\n",
      "| df  | 13758.7 | 13911.9 |\n",
      "| sql | 13749.5 | 13907.9 |\n",
      "\n",
      "|         | run 0 | run 1 | run 2 | run 3 | run 4 | run 5 | run 6 | run 7 | run 8 | run 9 |\n",
      "|steam df |13859 | 13817 | 13227 | 14443 | 14377 | 14525 | 13436 | 13964 | 13672 | 13799|\n",
      "|steam sql|14267 | 14052 | 13415 | 13748 | 13712 | 13854 | 13764 | 13811 | 14529 | 13927|\n",
      "|covid df |14344 | 13400 | 13447 | 14340 | 13724 | 13262 | 13354 | 14398 | 13666 | 13652|\n",
      "|covid sql|13758 | 14356 | 13586 | 13635 | 14173 | 13840 | 13625 | 13180 | 14037 | 13305|\n"
     ]
    }
   ],
   "source": [
    "m_s_d = \" | \".join([str(x) for x in measurements_steam_df])\n",
    "m_s_s = \" | \".join([str(x) for x in measurements_steam_sql])\n",
    "m_c_d = \" | \".join([str(x) for x in measurements_covid_df])\n",
    "m_c_s = \" | \".join([str(x) for x in measurements_covid_sql])\n",
    "\n",
    "print(f\"|     |  covid  |  steam  |\")\n",
    "print(f\"|-----|---------|---------|\")\n",
    "print(f\"| df  | {sum(measurements_covid_df) / len(measurements_covid_df)} | {sum(measurements_steam_df) / len(measurements_steam_df)} |\")\n",
    "print(f\"| sql | {sum(measurements_covid_sql) / len(measurements_covid_sql)} | {sum(measurements_steam_sql) / len(measurements_steam_sql)} |\")\n",
    "\n",
    "print()\n",
    "r = \" | \".join([f\"run {x}\" for x in range(runs)])\n",
    "print(f\"|         | {r} |\")\n",
    "print(f\"|steam df |{m_s_d}|\")\n",
    "print(f\"|steam sql|{m_s_s}|\")\n",
    "print(f\"|covid df |{m_c_d}|\")\n",
    "print(f\"|covid sql|{m_c_s}|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc48ed4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14840, 15516, 16064, 15516, 15416, 16240, 15695, 16246, 15082, 15463]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measurements_steam_scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5b17658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       |  covid  |  steam  |\n",
      "|-------|---------|---------|\n",
      "| df    | 13758.7 | 13911.9 |\n",
      "| sql   | 13749.5 | 13907.9 |\n",
      "| scala | 15889.8 | 15607.8 |\n",
      "\n",
      "|           | run 0 | run 1 | run 2 | run 3 | run 4 | run 5 | run 6 | run 7 | run 8 | run 9 |\n",
      "|steam df   |13859 | 13817 | 13227 | 14443 | 14377 | 14525 | 13436 | 13964 | 13672 | 13799|\n",
      "|steam sql  |14267 | 14052 | 13415 | 13748 | 13712 | 13854 | 13764 | 13811 | 14529 | 13927|\n",
      "|steam sca  |14840 | 15516 | 16064 | 15516 | 15416 | 16240 | 15695 | 16246 | 15082 | 15463|\n",
      "|covid df   |14344 | 13400 | 13447 | 14340 | 13724 | 13262 | 13354 | 14398 | 13666 | 13652|\n",
      "|covid sql  |13758 | 14356 | 13586 | 13635 | 14173 | 13840 | 13625 | 13180 | 14037 | 13305|\n",
      "|covid scala|15406 | 15395 | 15660 | 16456 | 15502 | 15895 | 16574 | 16308 | 15907 | 15795|\n"
     ]
    }
   ],
   "source": [
    "m_s_d = \" | \".join([str(x) for x in measurements_steam_df])\n",
    "m_s_s = \" | \".join([str(x) for x in measurements_steam_sql])\n",
    "m_s_scala = \" | \".join([str(x) for x in measurements_steam_scala])\n",
    "m_c_d = \" | \".join([str(x) for x in measurements_covid_df])\n",
    "m_c_s = \" | \".join([str(x) for x in measurements_covid_sql])\n",
    "m_c_scala =\" | \".join([str(x) for x in measurements_covid_scala])\n",
    "\n",
    "print(f\"|       |  covid  |  steam  |\")\n",
    "print(f\"|-------|---------|---------|\")\n",
    "print(f\"| df    | {sum(measurements_covid_df) / len(measurements_covid_df)} | {sum(measurements_steam_df) / len(measurements_steam_df)} |\")\n",
    "print(f\"| sql   | {sum(measurements_covid_sql) / len(measurements_covid_sql)} | {sum(measurements_steam_sql) / len(measurements_steam_sql)} |\")\n",
    "print(f\"| scala | {sum(measurements_covid_scala) / len(measurements_covid_scala)} | {sum(measurements_steam_scala) / len(measurements_steam_scala)} |\")\n",
    "\n",
    "print()\n",
    "r = \" | \".join([f\"run {x}\" for x in range(runs)])\n",
    "print(f\"|           | {r} |\")\n",
    "print(f\"|steam df   |{m_s_d}|\")\n",
    "print(f\"|steam sql  |{m_s_s}|\")\n",
    "print(f\"|steam sca  |{m_s_scala}|\")\n",
    "print(f\"|covid df   |{m_c_d}|\")\n",
    "print(f\"|covid sql  |{m_c_s}|\")\n",
    "print(f\"|covid scala|{m_c_scala}|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422dbe15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
