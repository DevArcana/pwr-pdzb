{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ef2223e",
   "metadata": {},
   "source": [
    "# Sprawozdanie 11\n",
    "\n",
    "**Grupa A3:**\n",
    "\n",
    "inż. Michał Liss\n",
    "\n",
    "inż. Marceli Sokólski\n",
    "\n",
    "inż. Piotr Krzystanek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d323e4c6",
   "metadata": {},
   "source": [
    "## Definicje funkcji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93e9afb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paramiko\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eaaca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_in_master(command):\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(\"namenode\", username=\"root\", password=\"pass\")\n",
    "    ssh_stdin, ssh_stdout, ssh_stderr = ssh.exec_command(f\"cd /app/ && . /env_var_path.sh && {command}\")\n",
    "    return (ssh_stdout.readlines(), ssh_stderr.readlines())\n",
    "\n",
    "def merge_results(path):\n",
    "    run_in_master(f\"hdfs dfs -cat {path}/part-* | hdfs dfs -put - {path}/merged.txt\")\n",
    "    \n",
    "def get_data_from_output_path(path):\n",
    "    return f\"{path}/merged.txt\"\n",
    "\n",
    "def print_hdfs_output(path):\n",
    "    raw = run_in_master(f\"hdfs dfs -cat {get_data_from_output_path(path)}\")[0]\n",
    "    print(\"\\n\".join(raw[0:11]))\n",
    "    \n",
    "def get_time(res, max_attempts: int = 6):\n",
    "    def get_id(res):\n",
    "        for line in res[1]:\n",
    "            m = re.search('tracking URL: http://resourcemanager:8088/proxy/(.*)/', line)\n",
    "            if m != None and m.group(1) != '':\n",
    "                return m.group(1)\n",
    "        return ''\n",
    "\n",
    "    def get_time_from_data(data):\n",
    "        sum = 0\n",
    "        for attemp in data['attempts']:\n",
    "            if attemp['completed'] is False:\n",
    "                # print('spark history server has not updated (yet)')\n",
    "                return -1\n",
    "            sum = sum + attemp['duration']\n",
    "        return sum\n",
    "        \n",
    "    id = get_id(res)\n",
    "    if id == -1:\n",
    "        return -1\n",
    "\n",
    "    attempt = 0\n",
    "\n",
    "    while attempt < max_attempts:\n",
    "        attempt = attempt + 1\n",
    "        response = requests.get(f'http://namenode:18080/api/v1/applications/{id}')\n",
    "        if not response.ok:\n",
    "            print('WARNING: application error')\n",
    "            return -1\n",
    "            \n",
    "        data = json.loads(response.text)\n",
    "        t = get_time_from_data(data)\n",
    "\n",
    "        if t >= 0:\n",
    "            return t\n",
    "        time.sleep(5)\n",
    "    print('WARNING: maximum attempts exceeded')\n",
    "    return -1\n",
    "\n",
    "runs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f4de77",
   "metadata": {},
   "source": [
    "# Covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51ebc643",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements_covid_sql = []\n",
    "measurements_covid_df = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3b5b2c",
   "metadata": {},
   "source": [
    "## SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c046400-316d-493f-b65f-959c5959e400",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 0 took 18062ms\n",
      "run 1 took 17356ms\n",
      "run 2 took 14562ms\n",
      "run 3 took 14290ms\n",
      "run 4 took 14166ms\n",
      "run 5 took 13484ms\n",
      "run 6 took 13527ms\n",
      "run 7 took 13840ms\n",
      "run 8 took 13995ms\n",
      "run 9 took 13635ms\n",
      "date,location,total_cases,new_cases,total_deaths,new_deaths,new_cases_per_million,average_new_cases_per_million\n",
      "\n",
      "2020-01-03,Afghanistan,,0,,0,0,160.00584439903545\n",
      "\n",
      "2020-01-04,Afghanistan,,0,,0,0,160.00584439903545\n",
      "\n",
      "2020-01-05,Afghanistan,,0,,0,0,160.00584439903545\n",
      "\n",
      "2020-01-06,Afghanistan,,0,,0,0,160.00584439903545\n",
      "\n",
      "2020-01-07,Afghanistan,,0,,0,0,160.00584439903545\n",
      "\n",
      "2020-01-08,Afghanistan,,0,,0,0,160.00584439903545\n",
      "\n",
      "2020-01-09,Afghanistan,,0,,0,0,160.00584439903545\n",
      "\n",
      "2020-01-10,Afghanistan,,0,,0,0,160.00584439903545\n",
      "\n",
      "2020-01-11,Afghanistan,,0,,0,0,160.00584439903545\n",
      "\n",
      "2020-01-12,Afghanistan,,0,,0,0,160.00584439903545\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r in range(runs):\n",
    "    run_in_master(f\"hdfs dfs -rm -r /spark-result/covid/sql\")\n",
    "    measurements_covid_sql.append(get_time(run_in_master(\"spark-submit --master yarn --deploy-mode cluster /data/master_volume/spark_scripts/covid_sql.py\")))\n",
    "    print(f\"run {r} took {measurements_covid_sql[-1]}ms\")\n",
    "    \n",
    "merge_results(\"/spark-result/covid/sql\")\n",
    "print_hdfs_output(\"/spark-result/covid/sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6a8ca5",
   "metadata": {},
   "source": [
    "## DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88760500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 0 took 16242ms\n",
      "run 1 took 15807ms\n",
      "run 2 took 15567ms\n",
      "run 3 took 15292ms\n",
      "run 4 took 15817ms\n",
      "run 5 took 15751ms\n",
      "run 6 took 15171ms\n",
      "run 7 took 15048ms\n",
      "run 8 took 15236ms\n",
      "run 9 took 19541ms\n",
      "date,location,total_cases,new_cases,total_deaths,new_deaths,new_cases_per_million,average_new_cases_per_million\n",
      "\n",
      "2022-05-03,Curacao,42035,0,273,0,0,160.00584439903545\n",
      "\n",
      "2022-05-04,Curacao,42330,295,274,1,1543,160.00584439903545\n",
      "\n",
      "2022-05-05,Curacao,42330,0,274,0,0,160.00584439903545\n",
      "\n",
      "2022-05-06,Curacao,42330,0,274,0,0,160.00584439903545\n",
      "\n",
      "2022-05-07,Curacao,42330,0,274,0,0,160.00584439903545\n",
      "\n",
      "2022-05-08,Curacao,42330,0,274,0,0,160.00584439903545\n",
      "\n",
      "2022-05-09,Curacao,42330,0,274,0,0,160.00584439903545\n",
      "\n",
      "2022-05-10,Curacao,42330,0,274,0,0,160.00584439903545\n",
      "\n",
      "2022-05-11,Curacao,42674,344,275,1,1799,160.00584439903545\n",
      "\n",
      "2022-05-12,Curacao,42674,0,275,0,0,160.00584439903545\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r in range(runs):\n",
    "    run_in_master(f\"hdfs dfs -rm -r /spark-result/covid/df\")\n",
    "    measurements_covid_df.append(get_time(run_in_master(\"spark-submit --master yarn --deploy-mode cluster /data/master_volume/spark_scripts/covid_df.py\")))\n",
    "    print(f\"run {r} took {measurements_covid_df[-1]}ms\")\n",
    "    \n",
    "merge_results(\"/spark-result/covid/df\")\n",
    "print_hdfs_output(\"/spark-result/covid/df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d686c3",
   "metadata": {},
   "source": [
    "# Steam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2898d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements_steam_sql = []\n",
    "measurements_steam_df = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec69493",
   "metadata": {},
   "source": [
    "## SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c55026d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 0 took 17226ms\n",
      "run 1 took 17619ms\n",
      "run 2 took 15960ms\n",
      "run 3 took 15022ms\n",
      "run 4 took 15969ms\n",
      "run 5 took 16073ms\n",
      "run 6 took 16086ms\n",
      "run 7 took 15478ms\n",
      "run 8 took 15842ms\n",
      "run 9 took 14839ms\n",
      "steam_appid,coming_soon,date,appid,name,positive,negative,owners,ccu\n",
      "\n",
      "907680,false,\"Aug 17, 2018\",907680,Wwbit,710,566,\"100,000 .. 200,000\",0\n",
      "\n",
      "823550,false,\"Sep 18, 2018\",823550,Booty Calls,255,294,\"100,000 .. 200,000\",134\n",
      "\n",
      "639780,false,\"Dec 7, 2017\",639780,Deep Space Waifu: FLAT JUSTICE,1449,67,\"50,000 .. 100,000\",4\n",
      "\n",
      "steam_appid,coming_soon,date,appid,name,positive,negative,owners,ccu\n",
      "\n",
      "896890,false,\"Dec 23, 2019\",896890,VR Paradise - Steam Edition,138,50,\"20,000 .. 50,000\",11\n",
      "\n",
      "726360,false,\"May 22, 2020\",726360,BOOBS SAGA: Prepare To Hentai Edition,367,103,\"20,000 .. 50,000\",0\n",
      "\n",
      "723090,false,\"Oct 24, 2017\",723090,Meltys Quest,446,10,\"20,000 .. 50,000\",36\n",
      "\n",
      "712790,false,\"Oct 2, 2017\",712790,Crimson Memories,51,21,\"20,000 .. 50,000\",0\n",
      "\n",
      "825300,false,\"Nov 22, 2018\",825300,To Trust an Incubus,43,5,\"0 .. 20,000\",1\n",
      "\n",
      "937730,false,\"Dec 24, 2018\",937730,Lady's Hentai Mosaic,21,4,\"0 .. 20,000\",0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r in range(runs):\n",
    "    run_in_master(f\"hdfs dfs -rm -r /spark-result/steam/sql\")\n",
    "    measurements_steam_sql.append(get_time(run_in_master(\"spark-submit --master yarn --deploy-mode cluster /data/master_volume/spark_scripts/steam_sql.py\")))\n",
    "    print(f\"run {r} took {measurements_steam_sql[-1]}ms\")\n",
    "    \n",
    "merge_results(\"/spark-result/steam/sql\")\n",
    "print_hdfs_output(\"/spark-result/steam/sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5297f9",
   "metadata": {},
   "source": [
    "## DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bbc603e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 0 took 18774ms\n",
      "run 1 took 15032ms\n",
      "run 2 took 15259ms\n",
      "run 3 took 16165ms\n",
      "run 4 took 14997ms\n",
      "run 5 took 15087ms\n",
      "run 6 took 14952ms\n",
      "run 7 took 15014ms\n",
      "run 8 took 15275ms\n",
      "run 9 took 15554ms\n",
      "steam_appid,coming_soon,date,appid,name,positive,negative,owners,ccu\n",
      "\n",
      "907680,false,\"Aug 17, 2018\",907680,Wwbit,710,566,\"100,000 .. 200,000\",0\n",
      "\n",
      "823550,false,\"Sep 18, 2018\",823550,Booty Calls,255,294,\"100,000 .. 200,000\",134\n",
      "\n",
      "639780,false,\"Dec 7, 2017\",639780,Deep Space Waifu: FLAT JUSTICE,1449,67,\"50,000 .. 100,000\",4\n",
      "\n",
      "steam_appid,coming_soon,date,appid,name,positive,negative,owners,ccu\n",
      "\n",
      "896890,false,\"Dec 23, 2019\",896890,VR Paradise - Steam Edition,138,50,\"20,000 .. 50,000\",11\n",
      "\n",
      "726360,false,\"May 22, 2020\",726360,BOOBS SAGA: Prepare To Hentai Edition,367,103,\"20,000 .. 50,000\",0\n",
      "\n",
      "723090,false,\"Oct 24, 2017\",723090,Meltys Quest,446,10,\"20,000 .. 50,000\",36\n",
      "\n",
      "712790,false,\"Oct 2, 2017\",712790,Crimson Memories,51,21,\"20,000 .. 50,000\",0\n",
      "\n",
      "825300,false,\"Nov 22, 2018\",825300,To Trust an Incubus,43,5,\"0 .. 20,000\",1\n",
      "\n",
      "937730,false,\"Dec 24, 2018\",937730,Lady's Hentai Mosaic,21,4,\"0 .. 20,000\",0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r in range(runs):\n",
    "    run_in_master(f\"hdfs dfs -rm -r /spark-result/steam/df\")\n",
    "    measurements_steam_df.append(get_time(run_in_master(\"spark-submit --master yarn --deploy-mode cluster /data/master_volume/spark_scripts/steam_df.py\")))\n",
    "    print(f\"run {r} took {measurements_steam_df[-1]}ms\")\n",
    "    \n",
    "merge_results(\"/spark-result/steam/df\")\n",
    "print_hdfs_output(\"/spark-result/steam/df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a62e7e1",
   "metadata": {},
   "source": [
    "# Wyniki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e661985",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_s_d = \" | \".join([str(x) for x in measurements_steam_df])\n",
    "m_s_s = \" | \".join([str(x) for x in measurements_steam_sql])\n",
    "m_c_d = \" | \".join([str(x) for x in measurements_covid_df])\n",
    "m_c_s = \" | \".join([str(x) for x in measurements_covid_sql])\n",
    "\n",
    "print(f\"|     |  covid  |  steam  |\")\n",
    "print(f\"|-----|---------|---------|\")\n",
    "print(f\"| df  | {sum(measurements_covid_df) / len(measurements_covid_df)} | {sum(measurements_steam_df) / len(measurements_steam_df)} |\")\n",
    "print(f\"| sql | {sum(measurements_covid_sql) / len(measurements_covid_sql)} | {sum(measurements_steam_sql) / len(measurements_steam_sql)} |\")\n",
    "\n",
    "print()\n",
    "r = \" | \".join([f\"run {x}\" for x in range(runs)])\n",
    "print(f\"|         | {r} |\")\n",
    "print(f\"|steam df |{m_s_d}|\")\n",
    "print(f\"|steam sql|{m_s_s}|\")\n",
    "print(f\"|covid df |{m_c_d}|\")\n",
    "print(f\"|covid sql|{m_c_s}|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d69a90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
