{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18d719c7-6d9e-42b3-abaf-f0e4cad11db4",
   "metadata": {},
   "source": [
    "# How to run commands on hadoop using jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98b8abc-9233-40a5-80c7-8ff991a48b64",
   "metadata": {},
   "source": [
    "## Import utility functions\n",
    "run_in_master runs provided command on namenode. Use it to start map-reduce, pig or spark application. \n",
    "\n",
    "run_in_hive runs provided command on hive-server. Use it to start hive application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91ae2f6a-fe7c-4543-aaf1-e347de4b7d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import run_in_master, run_in_hive, print_hdfs_output, hdfs_upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b9e7712-ab2d-4908-bb5b-87cc85b0d592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['test\\n'], [])\n",
      "(['test\\n'], [])\n"
     ]
    }
   ],
   "source": [
    "print(run_in_master(\"echo 'test'\"))\n",
    "print(run_in_hive(\"echo 'test'\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c403ac5-3250-470a-b203-483a09dd983e",
   "metadata": {},
   "source": [
    "# Map reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4254bb1-7424-4935-9e61-5a1d8358d911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Number of Maps  = 2\\n',\n",
       "  'Samples per Map = 5\\n',\n",
       "  'Wrote input for Map #0\\n',\n",
       "  'Wrote input for Map #1\\n',\n",
       "  'Starting Job\\n',\n",
       "  'Job Finished in 11.888 seconds\\n',\n",
       "  'Estimated value of Pi is 3.60000000000000000000\\n'],\n",
       " ['2023-06-19 22:19:40,819 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at resourcemanager/172.19.0.9:8032\\n',\n",
       "  '2023-06-19 22:19:40,882 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.19.0.12:10200\\n',\n",
       "  '2023-06-19 22:19:40,950 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1687211407465_0006\\n',\n",
       "  '2023-06-19 22:19:41,011 INFO input.FileInputFormat: Total input files to process : 2\\n',\n",
       "  '2023-06-19 22:19:41,059 INFO mapreduce.JobSubmitter: number of splits:2\\n',\n",
       "  '2023-06-19 22:19:41,132 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1687211407465_0006\\n',\n",
       "  '2023-06-19 22:19:41,132 INFO mapreduce.JobSubmitter: Executing with tokens: []\\n',\n",
       "  '2023-06-19 22:19:41,214 INFO conf.Configuration: resource-types.xml not found\\n',\n",
       "  \"2023-06-19 22:19:41,214 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\\n\",\n",
       "  '2023-06-19 22:19:41,444 INFO impl.YarnClientImpl: Submitted application application_1687211407465_0006\\n',\n",
       "  '2023-06-19 22:19:41,462 INFO mapreduce.Job: The url to track the job: http://resourcemanager:8088/proxy/application_1687211407465_0006/\\n',\n",
       "  '2023-06-19 22:19:41,462 INFO mapreduce.Job: Running job: job_1687211407465_0006\\n',\n",
       "  '2023-06-19 22:19:45,497 INFO mapreduce.Job: Job job_1687211407465_0006 running in uber mode : false\\n',\n",
       "  '2023-06-19 22:19:45,498 INFO mapreduce.Job:  map 0% reduce 0%\\n',\n",
       "  '2023-06-19 22:19:48,525 INFO mapreduce.Job:  map 50% reduce 0%\\n',\n",
       "  '2023-06-19 22:19:50,532 INFO mapreduce.Job:  map 100% reduce 0%\\n',\n",
       "  '2023-06-19 22:19:52,538 INFO mapreduce.Job:  map 100% reduce 100%\\n',\n",
       "  '2023-06-19 22:19:52,542 INFO mapreduce.Job: Job job_1687211407465_0006 completed successfully\\n',\n",
       "  '2023-06-19 22:19:52,590 INFO mapreduce.Job: Counters: 54\\n',\n",
       "  '\\tFile System Counters\\n',\n",
       "  '\\t\\tFILE: Number of bytes read=40\\n',\n",
       "  '\\t\\tFILE: Number of bytes written=829867\\n',\n",
       "  '\\t\\tFILE: Number of read operations=0\\n',\n",
       "  '\\t\\tFILE: Number of large read operations=0\\n',\n",
       "  '\\t\\tFILE: Number of write operations=0\\n',\n",
       "  '\\t\\tHDFS: Number of bytes read=526\\n',\n",
       "  '\\t\\tHDFS: Number of bytes written=215\\n',\n",
       "  '\\t\\tHDFS: Number of read operations=13\\n',\n",
       "  '\\t\\tHDFS: Number of large read operations=0\\n',\n",
       "  '\\t\\tHDFS: Number of write operations=3\\n',\n",
       "  '\\t\\tHDFS: Number of bytes read erasure-coded=0\\n',\n",
       "  '\\tJob Counters \\n',\n",
       "  '\\t\\tLaunched map tasks=2\\n',\n",
       "  '\\t\\tLaunched reduce tasks=1\\n',\n",
       "  '\\t\\tRack-local map tasks=2\\n',\n",
       "  '\\t\\tTotal time spent by all maps in occupied slots (ms)=8332\\n',\n",
       "  '\\t\\tTotal time spent by all reduces in occupied slots (ms)=4940\\n',\n",
       "  '\\t\\tTotal time spent by all map tasks (ms)=2083\\n',\n",
       "  '\\t\\tTotal time spent by all reduce tasks (ms)=1235\\n',\n",
       "  '\\t\\tTotal vcore-milliseconds taken by all map tasks=2083\\n',\n",
       "  '\\t\\tTotal vcore-milliseconds taken by all reduce tasks=1235\\n',\n",
       "  '\\t\\tTotal megabyte-milliseconds taken by all map tasks=8531968\\n',\n",
       "  '\\t\\tTotal megabyte-milliseconds taken by all reduce tasks=5058560\\n',\n",
       "  '\\tMap-Reduce Framework\\n',\n",
       "  '\\t\\tMap input records=2\\n',\n",
       "  '\\t\\tMap output records=4\\n',\n",
       "  '\\t\\tMap output bytes=36\\n',\n",
       "  '\\t\\tMap output materialized bytes=49\\n',\n",
       "  '\\t\\tInput split bytes=290\\n',\n",
       "  '\\t\\tCombine input records=0\\n',\n",
       "  '\\t\\tCombine output records=0\\n',\n",
       "  '\\t\\tReduce input groups=2\\n',\n",
       "  '\\t\\tReduce shuffle bytes=49\\n',\n",
       "  '\\t\\tReduce input records=4\\n',\n",
       "  '\\t\\tReduce output records=0\\n',\n",
       "  '\\t\\tSpilled Records=8\\n',\n",
       "  '\\t\\tShuffled Maps =2\\n',\n",
       "  '\\t\\tFailed Shuffles=0\\n',\n",
       "  '\\t\\tMerged Map outputs=2\\n',\n",
       "  '\\t\\tGC time elapsed (ms)=70\\n',\n",
       "  '\\t\\tCPU time spent (ms)=610\\n',\n",
       "  '\\t\\tPhysical memory (bytes) snapshot=990044160\\n',\n",
       "  '\\t\\tVirtual memory (bytes) snapshot=15115862016\\n',\n",
       "  '\\t\\tTotal committed heap usage (bytes)=1105199104\\n',\n",
       "  '\\t\\tPeak Map Physical memory (bytes)=376307712\\n',\n",
       "  '\\t\\tPeak Map Virtual memory (bytes)=5036761088\\n',\n",
       "  '\\t\\tPeak Reduce Physical memory (bytes)=272994304\\n',\n",
       "  '\\t\\tPeak Reduce Virtual memory (bytes)=5042835456\\n',\n",
       "  '\\tShuffle Errors\\n',\n",
       "  '\\t\\tBAD_ID=0\\n',\n",
       "  '\\t\\tCONNECTION=0\\n',\n",
       "  '\\t\\tIO_ERROR=0\\n',\n",
       "  '\\t\\tWRONG_LENGTH=0\\n',\n",
       "  '\\t\\tWRONG_MAP=0\\n',\n",
       "  '\\t\\tWRONG_REDUCE=0\\n',\n",
       "  '\\tFile Input Format Counters \\n',\n",
       "  '\\t\\tBytes Read=236\\n',\n",
       "  '\\tFile Output Format Counters \\n',\n",
       "  '\\t\\tBytes Written=97\\n'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_in_master(\"yarn jar /opt/hadoop-3.3.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar pi 2 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8983a8a6-873f-496d-9c2a-71d6712f750a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05aef1e7-3d5a-48df-84f1-3b56140d5a94",
   "metadata": {},
   "source": [
    "# Pig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b300a4b-dbbf-4782-913f-da20e857f9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02c63ab2-5e30-4282-8432-859149fd4667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs dfs -put /data/master_volume/examples/pig.pig /examples\n",
      "exit code []\n",
      "[\"put: `/examples/pig.pig': File exists\\n\"]\n",
      "hdfs dfs -put /data/master_volume/examples/data.jsonl /examples\n",
      "exit code []\n",
      "[\"put: `/examples/data.jsonl': File exists\\n\"]\n"
     ]
    }
   ],
   "source": [
    "hdfs_upload(\"examples/pig.pig\")\n",
    "hdfs_upload(\"examples/data.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0917877a-508b-403f-a64b-0b296fedd65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['(Micha≈Ç,24)\\n'],\n",
       " ['2023-06-19 22:19:58,960 INFO pig.ExecTypeProvider: Trying ExecType : LOCAL\\n',\n",
       "  '2023-06-19 22:19:58,961 INFO pig.ExecTypeProvider: Trying ExecType : MAPREDUCE\\n',\n",
       "  '2023-06-19 22:19:58,961 INFO pig.ExecTypeProvider: Picked MAPREDUCE as the ExecType\\n',\n",
       "  '2023-06-19 22:19:59,000 [main] INFO  org.apache.pig.Main - Apache Pig version 0.17.0 (r1797386) compiled Jun 02 2017, 15:41:58\\n',\n",
       "  '2023-06-19 22:19:59,000 [main] INFO  org.apache.pig.Main - Logging error messages to: /app/pig_1687213198994.log\\n',\n",
       "  '2023-06-19 22:19:59,210 [main] INFO  org.apache.pig.impl.util.Utils - Default bootup file /root/.pigbootup not found\\n',\n",
       "  '2023-06-19 22:19:59,248 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\\n',\n",
       "  '2023-06-19 22:19:59,248 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: hdfs://namenode:9000\\n',\n",
       "  '2023-06-19 22:19:59,633 [main] INFO  org.apache.pig.PigServer - Pig Script ID for the session: PIG-pig.pig-6017b282-437c-417b-919f-48721ad8515b\\n',\n",
       "  '2023-06-19 22:19:59,795 [main] INFO  org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl - Timeline service address: historyserver:8188\\n',\n",
       "  '2023-06-19 22:19:59,992 [main] INFO  org.apache.pig.backend.hadoop.PigATSClient - Created ATS Hook\\n',\n",
       "  '2023-06-19 22:20:00,005 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled\\n',\n",
       "  '2023-06-19 22:20:00,245 [main] INFO  org.apache.pig.impl.util.SpillableMemoryManager - Selected heap (PS Old Gen) of size 699400192 to monitor. collectionUsageThreshold = 489580128, usageThreshold = 489580128\\n',\n",
       "  '2023-06-19 22:20:00,307 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled\\n',\n",
       "  '2023-06-19 22:20:00,320 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled\\n',\n",
       "  '2023-06-19 22:20:00,357 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled\\n',\n",
       "  '2023-06-19 22:20:00,365 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig features used in the script: UNKNOWN\\n',\n",
       "  '2023-06-19 22:20:00,374 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled\\n',\n",
       "  '2023-06-19 22:20:00,377 [main] INFO  org.apache.pig.data.SchemaTupleBackend - Key [pig.schematuple] was not set... will not generate code.\\n',\n",
       "  '2023-06-19 22:20:00,391 [main] INFO  org.apache.pig.newplan.logical.optimizer.LogicalPlanOptimizer - {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, ConstantCalculator, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, NestedLimitOptimizer, PartitionFilterOptimizer, PredicatePushdownOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter]}\\n',\n",
       "  '2023-06-19 22:20:00,427 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - File concatenation threshold: 100 optimistic? false\\n',\n",
       "  '2023-06-19 22:20:00,436 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: 1\\n',\n",
       "  '2023-06-19 22:20:00,436 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: 1\\n',\n",
       "  '2023-06-19 22:20:00,445 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled\\n',\n",
       "  '2023-06-19 22:20:00,470 [main] INFO  org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider - Connecting to ResourceManager at resourcemanager/172.19.0.9:8032\\n',\n",
       "  '2023-06-19 22:20:00,543 [main] INFO  org.apache.hadoop.yarn.client.AHSProxy - Connecting to Application History server at historyserver/172.19.0.12:10200\\n',\n",
       "  '2023-06-19 22:20:00,579 [main] INFO  org.apache.pig.tools.pigstats.mapreduce.MRScriptState - Pig script settings are added to the job\\n',\n",
       "  '2023-06-19 22:20:00,581 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\\n',\n",
       "  '2023-06-19 22:20:00,581 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3\\n',\n",
       "  '2023-06-19 22:20:00,583 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress\\n',\n",
       "  '2023-06-19 22:20:00,584 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - This job cannot be converted run in-process\\n',\n",
       "  '2023-06-19 22:20:00,587 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.submit.replication is deprecated. Instead, use mapreduce.client.submit.file.replication\\n',\n",
       "  '2023-06-19 22:20:00,696 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Added jar file:/usr/local/pig/pig-0.17.0-core-h2.jar to DistributedCache through /tmp/temp-1625848144/tmp-1353835462/pig-0.17.0-core-h2.jar\\n',\n",
       "  '2023-06-19 22:20:00,715 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Added jar file:/opt/hadoop-3.3.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar to DistributedCache through /tmp/temp-1625848144/tmp-1355529443/jackson-core-asl-1.9.13.jar\\n',\n",
       "  '2023-06-19 22:20:00,733 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Added jar file:/usr/local/pig/lib/automaton-1.11-8.jar to DistributedCache through /tmp/temp-1625848144/tmp518531068/automaton-1.11-8.jar\\n',\n",
       "  '2023-06-19 22:20:00,753 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Added jar file:/usr/local/pig/lib/antlr-runtime-3.4.jar to DistributedCache through /tmp/temp-1625848144/tmp-1935072910/antlr-runtime-3.4.jar\\n',\n",
       "  '2023-06-19 22:20:00,772 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Added jar file:/usr/local/pig/lib/joda-time-2.9.3.jar to DistributedCache through /tmp/temp-1625848144/tmp-1326750130/joda-time-2.9.3.jar\\n',\n",
       "  '2023-06-19 22:20:00,777 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job\\n',\n",
       "  '2023-06-19 22:20:00,800 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map-reduce job(s) waiting for submission.\\n',\n",
       "  '2023-06-19 22:20:00,804 [JobControl] INFO  org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider - Connecting to ResourceManager at resourcemanager/172.19.0.9:8032\\n',\n",
       "  '2023-06-19 22:20:00,804 [JobControl] INFO  org.apache.hadoop.yarn.client.AHSProxy - Connecting to Application History server at historyserver/172.19.0.12:10200\\n',\n",
       "  '2023-06-19 22:20:00,810 [JobControl] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\\n',\n",
       "  '2023-06-19 22:20:00,812 [JobControl] INFO  org.apache.hadoop.conf.Configuration.deprecation - yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled\\n',\n",
       "  '2023-06-19 22:20:00,853 [JobControl] INFO  org.apache.hadoop.mapreduce.JobResourceUploader - Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1687211407465_0007\\n',\n",
       "  '2023-06-19 22:20:00,859 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\\n',\n",
       "  '2023-06-19 22:20:00,889 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\\n',\n",
       "  '2023-06-19 22:20:00,898 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths (combined) to process : 1\\n',\n",
       "  '2023-06-19 22:20:00,933 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\\n',\n",
       "  '2023-06-19 22:20:00,949 [JobControl] INFO  org.apache.hadoop.conf.Configuration.deprecation - yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled\\n',\n",
       "  '2023-06-19 22:20:00,992 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1687211407465_0007\\n',\n",
       "  '2023-06-19 22:20:00,992 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Executing with tokens: []\\n',\n",
       "  '2023-06-19 22:20:01,046 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\\n',\n",
       "  '2023-06-19 22:20:01,069 [JobControl] INFO  org.apache.hadoop.conf.Configuration - resource-types.xml not found\\n',\n",
       "  \"2023-06-19 22:20:01,070 [JobControl] INFO  org.apache.hadoop.yarn.util.resource.ResourceUtils - Unable to find 'resource-types.xml'.\\n\",\n",
       "  '2023-06-19 22:20:01,301 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1687211407465_0007\\n',\n",
       "  '2023-06-19 22:20:01,319 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://resourcemanager:8088/proxy/application_1687211407465_0007/\\n',\n",
       "  '2023-06-19 22:20:01,319 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_1687211407465_0007\\n',\n",
       "  '2023-06-19 22:20:01,319 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Processing aliases lines\\n',\n",
       "  '2023-06-19 22:20:01,319 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - detailed locations: M: lines[1,8] C:  R: \\n',\n",
       "  '2023-06-19 22:20:01,322 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete\\n',\n",
       "  '2023-06-19 22:20:01,322 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Running jobs are [job_1687211407465_0007]\\n',\n",
       "  '2023-06-19 22:20:08,351 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 50% complete\\n',\n",
       "  '2023-06-19 22:20:08,351 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Running jobs are [job_1687211407465_0007]\\n',\n",
       "  '2023-06-19 22:20:11,357 [main] INFO  org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider - Connecting to ResourceManager at resourcemanager/172.19.0.9:8032\\n',\n",
       "  '2023-06-19 22:20:11,357 [main] INFO  org.apache.hadoop.yarn.client.AHSProxy - Connecting to Application History server at historyserver/172.19.0.12:10200\\n',\n",
       "  '2023-06-19 22:20:11,359 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\\n',\n",
       "  '2023-06-19 22:20:11,462 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\\n',\n",
       "  '2023-06-19 22:20:11,565 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\\n',\n",
       "  '2023-06-19 22:20:11,667 [main] WARN  org.apache.pig.tools.pigstats.mapreduce.MRJobStats - Failed to get map task report\\n',\n",
       "  'java.io.IOException: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length\\n',\n",
       "  '\\tat org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:345)\\n',\n",
       "  '\\tat org.apache.hadoop.mapred.ClientServiceDelegate.getJobStatus(ClientServiceDelegate.java:430)\\n',\n",
       "  '\\tat org.apache.hadoop.mapred.YARNRunner.getJobStatus(YARNRunner.java:872)\\n',\n",
       "  '\\tat org.apache.hadoop.mapreduce.Cluster.getJob(Cluster.java:215)\\n',\n",
       "  '\\tat org.apache.pig.tools.pigstats.mapreduce.MRJobStats.getTaskReports(MRJobStats.java:528)\\n',\n",
       "  '\\tat org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addMapReduceStatistics(MRJobStats.java:355)\\n',\n",
       "  '\\tat org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:232)\\n',\n",
       "  '\\tat org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:164)\\n',\n",
       "  '\\tat org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:379)\\n',\n",
       "  '\\tat org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:290)\\n',\n",
       "  '\\tat org.apache.pig.PigServer.launchPlan(PigServer.java:1475)\\n',\n",
       "  '\\tat org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1460)\\n',\n",
       "  '\\tat org.apache.pig.PigServer.storeEx(PigServer.java:1119)\\n',\n",
       "  '\\tat org.apache.pig.PigServer.store(PigServer.java:1082)\\n',\n",
       "  '\\tat org.apache.pig.PigServer.openIterator(PigServer.java:995)\\n',\n",
       "  '\\tat org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:782)\\n',\n",
       "  '\\tat org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:383)\\n',\n",
       "  '\\tat org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:230)\\n',\n",
       "  '\\tat org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:205)\\n',\n",
       "  '\\tat org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:81)\\n',\n",
       "  '\\tat org.apache.pig.Main.run(Main.java:630)\\n',\n",
       "  '\\tat org.apache.pig.Main.main(Main.java:175)\\n',\n",
       "  '\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n',\n",
       "  '\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n',\n",
       "  '\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n',\n",
       "  '\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n',\n",
       "  '\\tat org.apache.hadoop.util.RunJar.run(RunJar.java:323)\\n',\n",
       "  '\\tat org.apache.hadoop.util.RunJar.main(RunJar.java:236)\\n',\n",
       "  'Caused by: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length\\n',\n",
       "  '\\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)\\n',\n",
       "  '\\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1202)\\n',\n",
       "  '\\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1098)\\n',\n",
       "  '2023-06-19 22:20:11,670 [main] INFO  org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider - Connecting to ResourceManager at resourcemanager/172.19.0.9:8032\\n',\n",
       "  '2023-06-19 22:20:11,671 [main] INFO  org.apache.hadoop.yarn.client.AHSProxy - Connecting to Application History server at historyserver/172.19.0.12:10200\\n',\n",
       "  '2023-06-19 22:20:11,674 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\\n',\n",
       "  '2023-06-19 22:20:11,776 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\\n',\n",
       "  '2023-06-19 22:20:11,880 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\\n',\n",
       "  '2023-06-19 22:20:11,981 [main] WARN  org.apache.pig.tools.pigstats.mapreduce.MRJobStats - Failed to get reduce task report\\n',\n",
       "  'java.io.IOException: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length\\n',\n",
       "  '\\tat org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:345)\\n',\n",
       "  '\\tat org.apache.hadoop.mapred.ClientServiceDelegate.getJobStatus(ClientServiceDelegate.java:430)\\n',\n",
       "  '\\tat org.apache.hadoop.mapred.YARNRunner.getJobStatus(YARNRunner.java:872)\\n',\n",
       "  '\\tat org.apache.hadoop.mapreduce.Cluster.getJob(Cluster.java:215)\\n',\n",
       "  '\\tat org.apache.pig.tools.pigstats.mapreduce.MRJobStats.getTaskReports(MRJobStats.java:528)\\n',\n",
       "  '\\tat org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addMapReduceStatistics(MRJobStats.java:361)\\n',\n",
       "  '\\tat org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:232)\\n',\n",
       "  '\\tat org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:164)\\n',\n",
       "  '\\tat org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:379)\\n',\n",
       "  '\\tat org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:290)\\n',\n",
       "  '\\tat org.apache.pig.PigServer.launchPlan(PigServer.java:1475)\\n',\n",
       "  '\\tat org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1460)\\n',\n",
       "  '\\tat org.apache.pig.PigServer.storeEx(PigServer.java:1119)\\n',\n",
       "  '\\tat org.apache.pig.PigServer.store(PigServer.java:1082)\\n',\n",
       "  '\\tat org.apache.pig.PigServer.openIterator(PigServer.java:995)\\n',\n",
       "  '\\tat org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:782)\\n',\n",
       "  '\\tat org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:383)\\n',\n",
       "  '\\tat org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:230)\\n',\n",
       "  '\\tat org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:205)\\n',\n",
       "  '\\tat org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:81)\\n',\n",
       "  '\\tat org.apache.pig.Main.run(Main.java:630)\\n',\n",
       "  '\\tat org.apache.pig.Main.main(Main.java:175)\\n',\n",
       "  '\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n',\n",
       "  '\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n',\n",
       "  '\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n',\n",
       "  '\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n',\n",
       "  '\\tat org.apache.hadoop.util.RunJar.run(RunJar.java:323)\\n',\n",
       "  '\\tat org.apache.hadoop.util.RunJar.main(RunJar.java:236)\\n',\n",
       "  'Caused by: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length\\n',\n",
       "  '\\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)\\n',\n",
       "  '\\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1202)\\n',\n",
       "  '\\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1098)\\n',\n",
       "  '2023-06-19 22:20:11,981 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\\n',\n",
       "  '2023-06-19 22:20:11,981 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\\n',\n",
       "  '2023-06-19 22:20:11,983 [main] INFO  org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider - Connecting to ResourceManager at resourcemanager/172.19.0.9:8032\\n',\n",
       "  '2023-06-19 22:20:11,983 [main] INFO  org.apache.hadoop.yarn.client.AHSProxy - Connecting to Application History server at historyserver/172.19.0.12:10200\\n',\n",
       "  '2023-06-19 22:20:11,986 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\\n',\n",
       "  '2023-06-19 22:20:12,089 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\\n',\n",
       "  '2023-06-19 22:20:12,192 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\\n',\n",
       "  '2023-06-19 22:20:12,293 [main] WARN  org.apache.pig.tools.pigstats.mapreduce.MRJobStats - Unable to get job counters\\n',\n",
       "  'java.io.IOException: java.io.IOException: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length\\n',\n",
       "  '\\tat org.apache.pig.tools.pigstats.mapreduce.MRJobStats.getCounters(MRJobStats.java:548)\\n',\n",
       "  '\\tat org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addCounters(MRJobStats.java:287)\\n',\n",
       "  '\\tat org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:234)\\n',\n",
       "  '\\tat org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:164)\\n',\n",
       "  '\\tat org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:379)\\n',\n",
       "  '\\tat org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:290)\\n',\n",
       "  '\\tat org.apache.pig.PigServer.launchPlan(PigServer.java:1475)\\n',\n",
       "  '\\tat org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1460)\\n',\n",
       "  '\\tat org.apache.pig.PigServer.storeEx(PigServer.java:1119)\\n',\n",
       "  '\\tat org.apache.pig.PigServer.store(PigServer.java:1082)\\n',\n",
       "  '\\tat org.apache.pig.PigServer.openIterator(PigServer.java:995)\\n',\n",
       "  '\\tat org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:782)\\n',\n",
       "  '\\tat org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:383)\\n',\n",
       "  '\\tat org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:230)\\n',\n",
       "  '\\tat org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:205)\\n',\n",
       "  '\\tat org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:81)\\n',\n",
       "  '\\tat org.apache.pig.Main.run(Main.java:630)\\n',\n",
       "  '\\tat org.apache.pig.Main.main(Main.java:175)\\n',\n",
       "  '\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n',\n",
       "  '\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n',\n",
       "  '\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n',\n",
       "  '\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n',\n",
       "  '\\tat org.apache.hadoop.util.RunJar.run(RunJar.java:323)\\n',\n",
       "  '\\tat org.apache.hadoop.util.RunJar.main(RunJar.java:236)\\n',\n",
       "  'Caused by: java.io.IOException: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length\\n',\n",
       "  '\\tat org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:345)\\n',\n",
       "  '\\tat org.apache.hadoop.mapred.ClientServiceDelegate.getJobStatus(ClientServiceDelegate.java:430)\\n',\n",
       "  '\\tat org.apache.hadoop.mapred.YARNRunner.getJobStatus(YARNRunner.java:872)\\n',\n",
       "  '\\tat org.apache.hadoop.mapreduce.Cluster.getJob(Cluster.java:215)\\n',\n",
       "  '\\tat org.apache.pig.tools.pigstats.mapreduce.MRJobStats.getCounters(MRJobStats.java:542)\\n',\n",
       "  '\\t... 23 more\\n',\n",
       "  'Caused by: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length\\n',\n",
       "  '\\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)\\n',\n",
       "  '\\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1202)\\n',\n",
       "  '\\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1098)\\n',\n",
       "  '2023-06-19 22:20:12,295 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete\\n',\n",
       "  '2023-06-19 22:20:12,297 [main] INFO  org.apache.pig.tools.pigstats.mapreduce.SimplePigStats - Script Statistics: \\n',\n",
       "  '\\n',\n",
       "  'HadoopVersion\\tPigVersion\\tUserId\\tStartedAt\\tFinishedAt\\tFeatures\\n',\n",
       "  '3.3.1\\t0.17.0\\troot\\t2023-06-19 22:20:00\\t2023-06-19 22:20:12\\tUNKNOWN\\n',\n",
       "  '\\n',\n",
       "  'Success!\\n',\n",
       "  '\\n',\n",
       "  'Job Stats (time in seconds):\\n',\n",
       "  'JobId\\tMaps\\tReduces\\tMaxMapTime\\tMinMapTime\\tAvgMapTime\\tMedianMapTime\\tMaxReduceTime\\tMinReduceTime\\tAvgReduceTime\\tMedianReducetime\\tAlias\\tFeature\\tOutputs\\n',\n",
       "  'job_1687211407465_0007\\t1\\t0\\tn/a\\tn/a\\tn/a\\tn/a\\t0\\t0\\t0\\t0\\tlines\\tMAP_ONLY\\thdfs://namenode:9000/tmp/temp-1625848144/tmp479705198,\\n',\n",
       "  '\\n',\n",
       "  'Input(s):\\n',\n",
       "  'Successfully read 0 records from: \"/examples/data.jsonl\"\\n',\n",
       "  '\\n',\n",
       "  'Output(s):\\n',\n",
       "  'Successfully stored 0 records in: \"hdfs://namenode:9000/tmp/temp-1625848144/tmp479705198\"\\n',\n",
       "  '\\n',\n",
       "  'Counters:\\n',\n",
       "  'Total records written : 0\\n',\n",
       "  'Total bytes written : 0\\n',\n",
       "  'Spillable Memory Manager spill count : 0\\n',\n",
       "  'Total bags proactively spilled: 0\\n',\n",
       "  'Total records proactively spilled: 0\\n',\n",
       "  '\\n',\n",
       "  'Job DAG:\\n',\n",
       "  'job_1687211407465_0007\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  '2023-06-19 22:20:12,299 [main] INFO  org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider - Connecting to ResourceManager at resourcemanager/172.19.0.9:8032\\n',\n",
       "  '2023-06-19 22:20:12,299 [main] INFO  org.apache.hadoop.yarn.client.AHSProxy - Connecting to Application History server at historyserver/172.19.0.12:10200\\n',\n",
       "  '2023-06-19 22:20:12,301 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\\n',\n",
       "  '2023-06-19 22:20:12,404 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\\n',\n",
       "  '2023-06-19 22:20:12,506 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\\n',\n",
       "  '2023-06-19 22:20:12,608 [main] WARN  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Unable to get job related diagnostics\\n',\n",
       "  '2023-06-19 22:20:12,610 [main] INFO  org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider - Connecting to ResourceManager at resourcemanager/172.19.0.9:8032\\n',\n",
       "  '2023-06-19 22:20:12,610 [main] INFO  org.apache.hadoop.yarn.client.AHSProxy - Connecting to Application History server at historyserver/172.19.0.12:10200\\n',\n",
       "  '2023-06-19 22:20:12,612 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\\n',\n",
       "  '2023-06-19 22:20:12,715 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\\n',\n",
       "  '2023-06-19 22:20:12,817 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\\n',\n",
       "  '2023-06-19 22:20:12,919 [main] WARN  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Unable to retrieve job to compute warning aggregation.\\n',\n",
       "  '2023-06-19 22:20:12,919 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!\\n',\n",
       "  '2023-06-19 22:20:12,921 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled\\n',\n",
       "  '2023-06-19 22:20:12,921 [main] WARN  org.apache.pig.data.SchemaTupleBackend - SchemaTupleBackend has already been initialized\\n',\n",
       "  '2023-06-19 22:20:12,930 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\\n',\n",
       "  '2023-06-19 22:20:12,930 [main] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1\\n',\n",
       "  '2023-06-19 22:20:12,969 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled\\n',\n",
       "  '2023-06-19 22:20:12,980 [main] INFO  org.apache.pig.Main - Pig script completed in 14 seconds and 57 milliseconds (14057 ms)\\n'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_in_master(\"pig -x mapreduce /data/master_volume/examples/pig.pig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db7d2f7-4616-42c0-8408-db4cd60fe72c",
   "metadata": {},
   "source": [
    "# Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1a3e5d0-1a04-425e-8ba2-af502d8af2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs dfs -put /data/master_volume/examples/employee.csv /examples\n",
      "exit code []\n",
      "[\"put: `/examples/employee.csv': File exists\\n\"]\n"
     ]
    }
   ],
   "source": [
    "hdfs_upload(\"examples/employee.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c32bce82-69af-4529-8efa-d33ce61e78d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " ['SLF4J: Class path contains multiple SLF4J bindings.\\n',\n",
       "  'SLF4J: Found binding in [jar:file:/opt/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\\n',\n",
       "  'SLF4J: Found binding in [jar:file:/opt/hadoop-3.3.1/share/hadoop/common/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\\n',\n",
       "  'SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\\n',\n",
       "  'SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\\n',\n",
       "  'Hive Session ID = 9edf19df-48de-4b88-9cce-0087788a6255\\n',\n",
       "  '\\n',\n",
       "  'Logging initialized using configuration in file:/opt/hive/conf/hive-log4j2.properties Async: true\\n',\n",
       "  'Hive Session ID = a537331a-ef44-46f5-a09e-1e13291a4624\\n',\n",
       "  'OK\\n',\n",
       "  'Time taken: 0.406 seconds\\n',\n",
       "  'OK\\n',\n",
       "  'Time taken: 0.015 seconds\\n',\n",
       "  'OK\\n',\n",
       "  'Time taken: 0.066 seconds\\n'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_in_hive(\"hive -f /data/master_volume/examples/employee_table.hql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56b6969b-6ade-4c41-8a39-a92c58ec1e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " ['SLF4J: Class path contains multiple SLF4J bindings.\\n',\n",
       "  'SLF4J: Found binding in [jar:file:/opt/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\\n',\n",
       "  'SLF4J: Found binding in [jar:file:/opt/hadoop-3.3.1/share/hadoop/common/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\\n',\n",
       "  'SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\\n',\n",
       "  'SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\\n',\n",
       "  'Hive Session ID = 114f4db2-199b-43dc-8fd2-b0c26dbba9d2\\n',\n",
       "  '\\n',\n",
       "  'Logging initialized using configuration in file:/opt/hive/conf/hive-log4j2.properties Async: true\\n',\n",
       "  'Hive Session ID = c67d3b63-2821-46bc-a683-69c3f31fa3c1\\n',\n",
       "  'OK\\n',\n",
       "  'Time taken: 0.403 seconds\\n',\n",
       "  'Loading data to table testdb.employee\\n',\n",
       "  'OK\\n',\n",
       "  'Time taken: 0.468 seconds\\n'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_in_hive(\"hive -f /data/master_volume/examples/employee_load.hql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4395759b-d00e-4190-bf5e-da5162232354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " ['SLF4J: Class path contains multiple SLF4J bindings.\\n',\n",
       "  'SLF4J: Found binding in [jar:file:/opt/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\\n',\n",
       "  'SLF4J: Found binding in [jar:file:/opt/hadoop-3.3.1/share/hadoop/common/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\\n',\n",
       "  'SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\\n',\n",
       "  'SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\\n',\n",
       "  'Hive Session ID = 14346eba-3f1f-400b-ab2e-4cc175bee879\\n',\n",
       "  '\\n',\n",
       "  'Logging initialized using configuration in file:/opt/hive/conf/hive-log4j2.properties Async: true\\n',\n",
       "  'Hive Session ID = 3a5dd44e-3f06-4691-b275-0c936e1eda99\\n',\n",
       "  'OK\\n',\n",
       "  'Time taken: 0.396 seconds\\n',\n",
       "  'Query ID = root_20230619222025_f4ccd1bd-b446-44b3-8c2e-c330202b6f88\\n',\n",
       "  'Total jobs = 1\\n',\n",
       "  'Launching Job 1 out of 1\\n',\n",
       "  'Number of reduce tasks not specified. Estimated from input data size: 1\\n',\n",
       "  'In order to change the average load for a reducer (in bytes):\\n',\n",
       "  '  set hive.exec.reducers.bytes.per.reducer=<number>\\n',\n",
       "  'In order to limit the maximum number of reducers:\\n',\n",
       "  '  set hive.exec.reducers.max=<number>\\n',\n",
       "  'In order to set a constant number of reducers:\\n',\n",
       "  '  set mapreduce.job.reduces=<number>\\n',\n",
       "  'Starting Job = job_1687211407465_0008, Tracking URL = http://resourcemanager:8088/proxy/application_1687211407465_0008/\\n',\n",
       "  'Kill Command = /opt/hadoop-3.3.1/bin/mapred job  -kill job_1687211407465_0008\\n',\n",
       "  'Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\\n',\n",
       "  '2023-06-19 22:20:31,214 Stage-1 map = 0%,  reduce = 0%\\n',\n",
       "  '2023-06-19 22:20:35,304 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.39 sec\\n',\n",
       "  '2023-06-19 22:20:40,386 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.71 sec\\n',\n",
       "  'MapReduce Total cumulative CPU time: 2 seconds 710 msec\\n',\n",
       "  'Ended Job = job_1687211407465_0008\\n',\n",
       "  'Moving data to directory hdfs://namenode:9000/user/hive/warehouse/results\\n',\n",
       "  'MapReduce Jobs Launched: \\n',\n",
       "  'Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.71 sec   HDFS Read: 173970 HDFS Write: 11665 SUCCESS\\n',\n",
       "  'Total MapReduce CPU Time Spent: 2 seconds 710 msec\\n',\n",
       "  'OK\\n',\n",
       "  'Time taken: 15.735 seconds\\n'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_in_hive(\"hive -f /data/master_volume/examples/test_group.hql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92266f81-f14e-49dd-8dd0-74451a7b004e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88249\u000133\n",
      "\n",
      "14555\u0001152\n",
      "\n",
      "13918\u0001264\n",
      "\n",
      "22224\u0001443\n",
      "\n",
      "83793\u0001498\n",
      "\n",
      "77255\u0001506\n",
      "\n",
      "92432\u0001520\n",
      "\n",
      "23576\u0001545\n",
      "\n",
      "37365\u0001571\n",
      "\n",
      "26259\u0001585\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_hdfs_output(\"/user/hive/warehouse/results/000000_0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841b42e7-8782-4459-8c7e-1be51b1787e6",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06fae32b-eddd-4157-ac81-c0255bb1dbed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " ['23/06/19 22:21:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\\n',\n",
       "  '23/06/19 22:21:43 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at resourcemanager/172.19.0.9:8032\\n',\n",
       "  '23/06/19 22:21:43 INFO AHSProxy: Connecting to Application History server at historyserver/172.19.0.12:10200\\n',\n",
       "  '23/06/19 22:21:43 INFO Configuration: resource-types.xml not found\\n',\n",
       "  \"23/06/19 22:21:43 INFO ResourceUtils: Unable to find 'resource-types.xml'.\\n\",\n",
       "  '23/06/19 22:21:43 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (4096 MB per container)\\n',\n",
       "  '23/06/19 22:21:43 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\\n',\n",
       "  '23/06/19 22:21:43 INFO Client: Setting up container launch context for our AM\\n',\n",
       "  '23/06/19 22:21:43 INFO Client: Setting up the launch environment for our AM container\\n',\n",
       "  '23/06/19 22:21:43 INFO Client: Preparing resources for our AM container\\n',\n",
       "  '23/06/19 22:21:43 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\\n',\n",
       "  '23/06/19 22:21:44 INFO Client: Uploading resource file:/tmp/spark-1bf861e3-afbd-4f0b-a578-fa18ed4017eb/__spark_libs__1367961581539792053.zip -> hdfs://namenode:9000/user/root/.sparkStaging/application_1687211407465_0009/__spark_libs__1367961581539792053.zip\\n',\n",
       "  '23/06/19 22:21:45 INFO Client: Uploading resource file:/data/master_volume/examples/spark.py -> hdfs://namenode:9000/user/root/.sparkStaging/application_1687211407465_0009/spark.py\\n',\n",
       "  '23/06/19 22:21:45 INFO Client: Uploading resource file:/usr/local/spark/python/lib/pyspark.zip -> hdfs://namenode:9000/user/root/.sparkStaging/application_1687211407465_0009/pyspark.zip\\n',\n",
       "  '23/06/19 22:21:45 INFO Client: Uploading resource file:/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://namenode:9000/user/root/.sparkStaging/application_1687211407465_0009/py4j-0.10.9.7-src.zip\\n',\n",
       "  '23/06/19 22:21:45 INFO Client: Uploading resource file:/tmp/spark-1bf861e3-afbd-4f0b-a578-fa18ed4017eb/__spark_conf__716924399380474134.zip -> hdfs://namenode:9000/user/root/.sparkStaging/application_1687211407465_0009/__spark_conf__.zip\\n',\n",
       "  '23/06/19 22:21:45 INFO SecurityManager: Changing view acls to: root\\n',\n",
       "  '23/06/19 22:21:45 INFO SecurityManager: Changing modify acls to: root\\n',\n",
       "  '23/06/19 22:21:45 INFO SecurityManager: Changing view acls groups to: \\n',\n",
       "  '23/06/19 22:21:45 INFO SecurityManager: Changing modify acls groups to: \\n',\n",
       "  '23/06/19 22:21:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\\n',\n",
       "  '23/06/19 22:21:45 INFO Client: Submitting application application_1687211407465_0009 to ResourceManager\\n',\n",
       "  '23/06/19 22:21:45 INFO YarnClientImpl: Submitted application application_1687211407465_0009\\n',\n",
       "  '23/06/19 22:21:46 INFO Client: Application report for application_1687211407465_0009 (state: ACCEPTED)\\n',\n",
       "  '23/06/19 22:21:46 INFO Client: \\n',\n",
       "  '\\t client token: N/A\\n',\n",
       "  '\\t diagnostics: AM container is launched, waiting for AM container to Register with RM\\n',\n",
       "  '\\t ApplicationMaster host: N/A\\n',\n",
       "  '\\t ApplicationMaster RPC port: -1\\n',\n",
       "  '\\t queue: default\\n',\n",
       "  '\\t start time: 1687213305470\\n',\n",
       "  '\\t final status: UNDEFINED\\n',\n",
       "  '\\t tracking URL: http://resourcemanager:8088/proxy/application_1687211407465_0009/\\n',\n",
       "  '\\t user: root\\n',\n",
       "  '23/06/19 22:21:47 INFO Client: Application report for application_1687211407465_0009 (state: ACCEPTED)\\n',\n",
       "  '23/06/19 22:21:48 INFO Client: Application report for application_1687211407465_0009 (state: ACCEPTED)\\n',\n",
       "  '23/06/19 22:21:49 INFO Client: Application report for application_1687211407465_0009 (state: RUNNING)\\n',\n",
       "  '23/06/19 22:21:49 INFO Client: \\n',\n",
       "  '\\t client token: N/A\\n',\n",
       "  '\\t diagnostics: N/A\\n',\n",
       "  '\\t ApplicationMaster host: aeac6827ffc9\\n',\n",
       "  '\\t ApplicationMaster RPC port: 43211\\n',\n",
       "  '\\t queue: default\\n',\n",
       "  '\\t start time: 1687213305470\\n',\n",
       "  '\\t final status: UNDEFINED\\n',\n",
       "  '\\t tracking URL: http://resourcemanager:8088/proxy/application_1687211407465_0009/\\n',\n",
       "  '\\t user: root\\n',\n",
       "  '23/06/19 22:21:50 INFO Client: Application report for application_1687211407465_0009 (state: RUNNING)\\n',\n",
       "  '23/06/19 22:21:51 INFO Client: Application report for application_1687211407465_0009 (state: RUNNING)\\n',\n",
       "  '23/06/19 22:21:52 INFO Client: Application report for application_1687211407465_0009 (state: RUNNING)\\n',\n",
       "  '23/06/19 22:21:53 INFO Client: Application report for application_1687211407465_0009 (state: RUNNING)\\n',\n",
       "  '23/06/19 22:21:54 INFO Client: Application report for application_1687211407465_0009 (state: RUNNING)\\n',\n",
       "  '23/06/19 22:21:55 INFO Client: Application report for application_1687211407465_0009 (state: RUNNING)\\n',\n",
       "  '23/06/19 22:21:56 INFO Client: Application report for application_1687211407465_0009 (state: RUNNING)\\n',\n",
       "  '23/06/19 22:21:57 INFO Client: Application report for application_1687211407465_0009 (state: RUNNING)\\n',\n",
       "  '23/06/19 22:21:58 INFO Client: Application report for application_1687211407465_0009 (state: RUNNING)\\n',\n",
       "  '23/06/19 22:21:59 INFO Client: Application report for application_1687211407465_0009 (state: RUNNING)\\n',\n",
       "  '23/06/19 22:22:00 INFO Client: Application report for application_1687211407465_0009 (state: FINISHED)\\n',\n",
       "  '23/06/19 22:22:00 INFO Client: \\n',\n",
       "  '\\t client token: N/A\\n',\n",
       "  '\\t diagnostics: N/A\\n',\n",
       "  '\\t ApplicationMaster host: aeac6827ffc9\\n',\n",
       "  '\\t ApplicationMaster RPC port: 43211\\n',\n",
       "  '\\t queue: default\\n',\n",
       "  '\\t start time: 1687213305470\\n',\n",
       "  '\\t final status: SUCCEEDED\\n',\n",
       "  '\\t tracking URL: http://resourcemanager:8088/proxy/application_1687211407465_0009/\\n',\n",
       "  '\\t user: root\\n',\n",
       "  '23/06/19 22:22:00 INFO ShutdownHookManager: Shutdown hook called\\n',\n",
       "  '23/06/19 22:22:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-1bf861e3-afbd-4f0b-a578-fa18ed4017eb\\n',\n",
       "  '23/06/19 22:22:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-cc1fbf1c-4bbe-43b7-905d-439ff2c5f7a5\\n'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_in_master(\"spark-submit --master yarn --deploy-mode cluster /data/master_volume/examples/spark.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ee6e9c-7866-4753-8df4-48790e9e12dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_hdfs_output(\"/user/hive/warehouse/results/000000_0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
