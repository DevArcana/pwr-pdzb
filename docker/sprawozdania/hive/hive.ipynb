{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d616878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docker\n",
    "import uuid\n",
    "import paramiko\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "import requests\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40c2f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_in_master(command):\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(\"namenode\", username=\"root\", password=\"pass\")\n",
    "    ssh_stdin, ssh_stdout, ssh_stderr = ssh.exec_command(f\"cd /app/ && . /env_var_path.sh && {command}\")\n",
    "    return (ssh_stdout.readlines(), ssh_stderr.readlines())\n",
    "\n",
    "def run_in_hive(command):\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(\"hive-server\", username=\"root\", password=\"pass\")\n",
    "    ssh_stdin, ssh_stdout, ssh_stderr = ssh.exec_command(f\"bash -c '. /env_var_path.sh && {command}'\")\n",
    "    return (ssh_stdout.readlines(), ssh_stderr.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1423206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_test_table():\n",
    "    return run_in_hive(\"hive -f /data/master_volume/hive_scripts/employee_table.hql\")\n",
    "    \n",
    "def copy_test_file():\n",
    "    path = \"/data/master_volume/hive_scripts/employee.csv\"\n",
    "    dest = \"/user/hive/warehouse/testdb.db/employee\"\n",
    "    run_in_master(f\"hdfs dfs -mkdir -p {dest}\")\n",
    "    return run_in_master(f\"hdfs dfs -put {path} {dest}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7115061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " [\"put: `/user/hive/warehouse/testdb.db/employee/employee.csv': File exists\\n\"])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup_test_table()\n",
    "copy_test_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bdf5d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " ['SLF4J: Class path contains multiple SLF4J bindings.\\n',\n",
       "  'SLF4J: Found binding in [jar:file:/opt/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\\n',\n",
       "  'SLF4J: Found binding in [jar:file:/opt/hadoop-3.3.1/share/hadoop/common/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\\n',\n",
       "  'SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\\n',\n",
       "  'SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\\n',\n",
       "  'Hive Session ID = 4e7c2ff0-33e9-4acf-996e-a99711bcbfbe\\n',\n",
       "  '\\n',\n",
       "  'Logging initialized using configuration in file:/opt/hive/conf/hive-log4j2.properties Async: true\\n',\n",
       "  'Hive Session ID = 4ebe5a73-cb05-424f-9d37-f3b524a58b07\\n',\n",
       "  'OK\\n',\n",
       "  'Time taken: 0.391 seconds\\n',\n",
       "  'Query ID = root_20230525185710_c1385f1a-8b71-463a-99fa-d06ad3204705\\n',\n",
       "  'Total jobs = 1\\n',\n",
       "  'Launching Job 1 out of 1\\n',\n",
       "  'Number of reduce tasks not specified. Estimated from input data size: 3\\n',\n",
       "  'In order to change the average load for a reducer (in bytes):\\n',\n",
       "  '  set hive.exec.reducers.bytes.per.reducer=<number>\\n',\n",
       "  'In order to limit the maximum number of reducers:\\n',\n",
       "  '  set hive.exec.reducers.max=<number>\\n',\n",
       "  'In order to set a constant number of reducers:\\n',\n",
       "  '  set mapreduce.job.reduces=<number>\\n',\n",
       "  'Starting Job = job_1685040515443_0003, Tracking URL = http://resourcemanager:8088/proxy/application_1685040515443_0003/\\n',\n",
       "  'Kill Command = /opt/hadoop-3.3.1/bin/mapred job  -kill job_1685040515443_0003\\n',\n",
       "  'Hadoop job information for Stage-1: number of mappers: 3; number of reducers: 3\\n',\n",
       "  '2023-05-25 18:57:16,226 Stage-1 map = 0%,  reduce = 0%\\n',\n",
       "  '2023-05-25 18:57:30,440 Stage-1 map = 33%,  reduce = 0%, Cumulative CPU 12.54 sec\\n',\n",
       "  '2023-05-25 18:57:31,456 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 24.42 sec\\n',\n",
       "  '2023-05-25 18:57:34,506 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 27.85 sec\\n',\n",
       "  '2023-05-25 18:57:38,581 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 33.27 sec\\n',\n",
       "  '2023-05-25 18:57:39,596 Stage-1 map = 100%,  reduce = 67%, Cumulative CPU 38.52 sec\\n',\n",
       "  '2023-05-25 18:57:42,640 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 42.84 sec\\n',\n",
       "  'MapReduce Total cumulative CPU time: 42 seconds 840 msec\\n',\n",
       "  'Ended Job = job_1685040515443_0003\\n',\n",
       "  'Moving data to directory hdfs://namenode:9000/user/hive/warehouse/results\\n',\n",
       "  'MapReduce Jobs Launched: \\n',\n",
       "  'Stage-Stage-1: Map: 3  Reduce: 3   Cumulative CPU: 42.84 sec   HDFS Read: 574506024 HDFS Write: 1188878 SUCCESS\\n',\n",
       "  'Total MapReduce CPU Time Spent: 42 seconds 840 msec\\n',\n",
       "  'OK\\n',\n",
       "  'Time taken: 33.946 seconds\\n'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_in_hive(\"hive -f /data/master_volume/hive_scripts/test_group.hql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa578485",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
