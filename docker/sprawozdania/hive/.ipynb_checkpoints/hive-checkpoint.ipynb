{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d616878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docker\n",
    "import uuid\n",
    "import paramiko\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "import requests\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40c2f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_in_master(command):\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(\"namenode\", username=\"root\", password=\"pass\")\n",
    "    ssh_stdin, ssh_stdout, ssh_stderr = ssh.exec_command(f\"cd /app/ && . /env_var_path.sh && {command}\")\n",
    "    return (ssh_stdout.readlines(), ssh_stderr.readlines())\n",
    "\n",
    "def run_in_hive(command):\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(\"hive-server\", username=\"root\", password=\"pass\")\n",
    "    ssh_stdin, ssh_stdout, ssh_stderr = ssh.exec_command(f\"bash -c '. /env_var_path.sh && {command}'\")\n",
    "    return (ssh_stdout.readlines(), ssh_stderr.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1423206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_test_table():\n",
    "    return run_in_hive(\"hive -f /data/master_volume/hive_scripts/employee_table.hql\")\n",
    "    \n",
    "def copy_test_file():\n",
    "    path = \"/data/master_volume/hive_scripts/employee.csv\"\n",
    "    dest = \"/user/hive/warehouse/testdb.db/employee\"\n",
    "    return run_in_master(f\"hdfs dfs -put {path} {dest}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61216dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Linux 1492895640be 6.3.4-zen1-1-zen #1 ZEN SMP PREEMPT_DYNAMIC Wed, 24 May 2023 17:43:43 +0000 x86_64 GNU/Linux\\n'],\n",
       " [])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_in_hive(\"uname -a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7115061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " [\"put: `hdfs://namenode:9000/user/hive/warehouse/testdb.db/employee': No such file or directory\\n\"])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup_test_table()\n",
    "copy_test_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bdf5d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " ['SLF4J: Class path contains multiple SLF4J bindings.\\n',\n",
       "  'SLF4J: Found binding in [jar:file:/opt/apache-hive-2.3.2-bin/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\\n',\n",
       "  'SLF4J: Found binding in [jar:file:/opt/hadoop-2.7.4/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\\n',\n",
       "  'SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\\n',\n",
       "  'SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\\n',\n",
       "  '\\n',\n",
       "  'Logging initialized using configuration in file:/opt/apache-hive-2.3.2-bin/conf/hive-log4j2.properties Async: true\\n',\n",
       "  'OK\\n',\n",
       "  'Time taken: 0.658 seconds\\n',\n",
       "  'WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\\n',\n",
       "  'Query ID = root_20230526012003_8e7e6b10-2b8a-4546-98a1-7a99c4971df2\\n',\n",
       "  'Total jobs = 3\\n',\n",
       "  'Launching Job 1 out of 3\\n',\n",
       "  \"Number of reduce tasks is set to 0 since there's no reduce operator\\n\",\n",
       "  'Job running in-process (local Hadoop)\\n',\n",
       "  '2023-05-26 01:20:05,442 Stage-1 map = 0%,  reduce = 0%\\n',\n",
       "  '2023-05-26 01:20:06,446 Stage-1 map = 100%,  reduce = 0%\\n',\n",
       "  'Ended Job = job_local1084995992_0001\\n',\n",
       "  'Stage-3 is selected by condition resolver.\\n',\n",
       "  'Stage-2 is filtered out by condition resolver.\\n',\n",
       "  'Stage-4 is filtered out by condition resolver.\\n',\n",
       "  'Moving data to directory hdfs://namenode:9000/user/hive/warehouse/results/.hive-staging_hive_2023-05-26_01-20-03_179_2245484300072754604-1/-ext-10000\\n',\n",
       "  'Moving data to directory hdfs://namenode:9000/user/hive/warehouse/results\\n',\n",
       "  'MapReduce Jobs Launched: \\n',\n",
       "  'Stage-Stage-1:  HDFS Read: 48888890 HDFS Write: 48888890 SUCCESS\\n',\n",
       "  'Total MapReduce CPU Time Spent: 0 msec\\n',\n",
       "  'OK\\n',\n",
       "  'Time taken: 3.329 seconds\\n',\n",
       "  'WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\\n',\n",
       "  'Query ID = root_20230526012006_0b2c6f08-c08f-42ca-9b61-da9173af076c\\n',\n",
       "  'Total jobs = 1\\n',\n",
       "  'Launching Job 1 out of 1\\n',\n",
       "  'Number of reduce tasks not specified. Estimated from input data size: 1\\n',\n",
       "  'In order to change the average load for a reducer (in bytes):\\n',\n",
       "  '  set hive.exec.reducers.bytes.per.reducer=<number>\\n',\n",
       "  'In order to limit the maximum number of reducers:\\n',\n",
       "  '  set hive.exec.reducers.max=<number>\\n',\n",
       "  'In order to set a constant number of reducers:\\n',\n",
       "  '  set mapreduce.job.reduces=<number>\\n',\n",
       "  'Job running in-process (local Hadoop)\\n',\n",
       "  '2023-05-26 01:20:07,771 Stage-1 map = 100%,  reduce = 100%\\n',\n",
       "  'Ended Job = job_local932123703_0002\\n',\n",
       "  'Moving data to directory hdfs://namenode:9000/user/hive/warehouse/results\\n',\n",
       "  'MapReduce Jobs Launched: \\n',\n",
       "  'Stage-Stage-1:  HDFS Read: 195555560 HDFS Write: 97777784 SUCCESS\\n',\n",
       "  'Total MapReduce CPU Time Spent: 0 msec\\n',\n",
       "  'OK\\n',\n",
       "  'Time taken: 1.286 seconds\\n'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_in_hive(\"hive -f /data/master_volume/hive_scripts/test_group.hql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa578485",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
