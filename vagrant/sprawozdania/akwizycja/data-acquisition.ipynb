{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sprawozdanie 6 - akwizycja danych\n",
    "\n",
    "## Środowisko\n",
    "\n",
    "Mamy maszynę wirtualną z Ubuntu postawioną za pomocą Vagrant'a (korzystającego pod spodem z VirtualBox'a). Na tej maszynie wirtualnej stawiamy kontenery Docker'a.\n",
    "\n",
    "Aby ułatwić sobie późniejszą pracę z obrazami na których postawiony jest hadoop postanowiliśmy dodać do master-node volumen na dane (modyfikując skrypty generujące docker-compose). Dzięki temu możemy w wygodny sposób (tj. poprzez wrzucenie do odpowiedniego folderu) przenosić pliki do miejsca, do którego możemy się dostać z poziomu maszyny z hadoopem. Warto zwrócić uwagę, że maszyna wirtualna także posiada taki wolumen, który zapewnia wykorzystanie Vagrant'a.\n",
    "\n",
    "```yaml\n",
    "master:\n",
    "    image: hjben/hadoop-eco:$hadoop_version\n",
    "    hostname: master\n",
    "    container_name: master\n",
    "    privileged: true\n",
    "    ports:\n",
    "      - 8088:8088\n",
    "      - 9870:9870\n",
    "      - 8042:8042\n",
    "      - 10000:10000\n",
    "      - 10002:10002\n",
    "      - 16010:16010\n",
    "    volumes:\n",
    "      - /sys/fs/cgroup:/sys/fs/cgroup\n",
    "      - $hdfs_path:/data/hadoop\n",
    "      - $hadoop_log_path:/usr/local/hadoop/logs\n",
    "      - $hbase_log_path/master:/usr/local/hbase/logs\n",
    "      - $hive_log_path:/usr/local/hive/logs\n",
    "      - $sqoop_log_path:/usr/local/sqoop/logs\n",
    "      - /vagrant/master_volume:/data/master_volume <-------------- dodany volumen\n",
    "    networks:\n",
    "      hadoop-cluster:\n",
    "        ipv4_address: 10.1.2.3\n",
    "    extra_hosts:\n",
    "      - \"mariadb:10.1.2.2\"\n",
    "      - \"master:10.1.2.3\"\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pobieranie danych\n",
    "Niestety przez potrzebę generowania i podania klucza API do serwisu kaggle przed pobraniem danych należy wykonać kilka czynności.\n",
    "\n",
    "1. Pobrać ze strony kaggle klucz API (kaggle.json)\n",
    "2. Stworzyć folder .kaggle w głównym katalogu użytkownika i skopiować tam klucz API\n",
    "3. Poniższe funkcje:\n",
    "    1. Pobierają z kaggle:\n",
    "       * [YouTube Trending Video Dataset](https://www.kaggle.com/datasets/rsrishav/youtube-trending-video-dataset)\n",
    "       * [Steam Dataset](https://www.kaggle.com/datasets/souyama/steam-dataset)\n",
    "    2. Pobierają z sieci [dane Covid'owe](https://covid.ourworldindata.org/data/owid-covid-data.csv)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "from timeit import default_timer as timer\n",
    "import requests\n",
    "import docker\n",
    "import json\n",
    "import opendatasets as od"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T12:52:16.073489Z",
     "end_time": "2023-04-23T12:52:16.081337Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "if os.getcwd().startswith(\"/tmp\"):\n",
    "    os.chdir(\"/vagrant/sprawozdania/akwizycja\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "output_dir = \"../../master_volume/datasets\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T12:42:01.739906Z",
     "end_time": "2023-04-23T12:42:01.755541Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset steam-dataset already exists, skipping download\n",
      "Dataset youtube-trending-video-dataset already exists, skipping download\n",
      "1.64 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "if not os.path.isdir(f\"{output_dir}/steam-dataset\"):\n",
    "    od.download(\"https://www.kaggle.com/datasets/souyama/steam-dataset\", f\"{output_dir}\")\n",
    "else:\n",
    "    print(\"Dataset steam-dataset already exists, skipping download\")\n",
    "\n",
    "if not os.path.isdir(f\"{output_dir}/youtube-trending-video-dataset\"):\n",
    "    od.download(\"https://www.kaggle.com/datasets/rsrishav/youtube-trending-video-dataset\", f\"{output_dir}\")\n",
    "else:\n",
    "    print(\"Dataset youtube-trending-video-dataset already exists, skipping download\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T12:42:05.522809Z",
     "end_time": "2023-04-23T12:42:05.534496Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset covid-dataset already exists, skipping download\n",
      "covid-dataset.csv: 77.76MB\n",
      "973 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "path = f\"{output_dir}/covid-dataset.csv\"\n",
    "\n",
    "if not os.path.isfile(path):\n",
    "    print(f\"Downloading covid-dataset to {path}\")\n",
    "    start = timer()\n",
    "    r = requests.get(\"https://covid.ourworldindata.org/data/owid-covid-data.csv\", allow_redirects=True)\n",
    "    with open(path, 'wb') as file:\n",
    "        file.write(r.content)\n",
    "    end = timer()\n",
    "    print(f\"Download finished in {end - start:.02f}s\")\n",
    "else:\n",
    "    print(\"Dataset covid-dataset already exists, skipping download\")\n",
    "\n",
    "print(f\"covid-dataset.csv: {os.stat(path).st_size / (1024 * 1024):.02f}MB\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T12:42:10.207948Z",
     "end_time": "2023-04-23T12:42:10.223587Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Formatowanie danych\n",
    "Po rozpakowaniu danych widać, że część z nich ma format trudny do późniejszej pracy. Ostatecznie postanowiliśmy przed wrzuceniem plików do hdfs wszystkie przetransformować do dormatu .jsonl. Format .jsonl zawiera obiekty json, każdy w kolejnej linii. Dzięki zastosowaniu takiego formatu na kolejnych laboratoriach będzie można wykorzystywać odpowiednie mappery.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting JSON to JSONL\n",
      "../../master_volume/datasets/steam-dataset/appinfo/dlc_data/missing.json\n",
      "../../master_volume/datasets/steam-dataset/appinfo/dlc_data/missing.jsonl\n",
      "../../master_volume/datasets/steam-dataset/appinfo/dlc_data/steam_dlc_data.json\n",
      "../../master_volume/datasets/steam-dataset/appinfo/dlc_data/steam_dlc_data.jsonl\n",
      "../../master_volume/datasets/steam-dataset/appinfo/store_data/steam_store_data.json\n",
      "../../master_volume/datasets/steam-dataset/appinfo/store_data/steam_store_data.jsonl\n",
      "../../master_volume/datasets/steam-dataset/news_data/missing.json\n",
      "../../master_volume/datasets/steam-dataset/news_data/missing.jsonl\n",
      "../../master_volume/datasets/steam-dataset/news_data/steam_news_data.json\n",
      "../../master_volume/datasets/steam-dataset/news_data/steam_news_data.jsonl\n",
      "../../master_volume/datasets/steam-dataset/steamspy/basic/steam_spy_scrap.json\n",
      "../../master_volume/datasets/steam-dataset/steamspy/basic/steam_spy_scrap.jsonl\n",
      "../../master_volume/datasets/steam-dataset/steamspy/detailed/steam_spy_detailed.json\n",
      "../../master_volume/datasets/steam-dataset/steamspy/detailed/steam_spy_detailed.jsonl\n",
      "../../master_volume/datasets/steam-dataset/steam_charts/missing.json\n",
      "../../master_volume/datasets/steam-dataset/steam_charts/missing.jsonl\n",
      "../../master_volume/datasets/steam-dataset/steam_charts/steam_charts.json\n",
      "../../master_volume/datasets/steam-dataset/steam_charts/steam_charts.jsonl\n",
      "../../master_volume/datasets/youtube-trending-video-dataset/BR_category_id.json\n",
      "../../master_volume/datasets/youtube-trending-video-dataset/BR_category_id.jsonl\n",
      "../../master_volume/datasets/youtube-trending-video-dataset/CA_category_id.json\n",
      "../../master_volume/datasets/youtube-trending-video-dataset/CA_category_id.jsonl\n",
      "../../master_volume/datasets/youtube-trending-video-dataset/DE_category_id.json\n",
      "../../master_volume/datasets/youtube-trending-video-dataset/DE_category_id.jsonl\n",
      "../../master_volume/datasets/youtube-trending-video-dataset/FR_category_id.json\n",
      "../../master_volume/datasets/youtube-trending-video-dataset/FR_category_id.jsonl\n",
      "../../master_volume/datasets/youtube-trending-video-dataset/GB_category_id.json\n",
      "../../master_volume/datasets/youtube-trending-video-dataset/GB_category_id.jsonl\n",
      "../../master_volume/datasets/youtube-trending-video-dataset/IN_category_id.json\n",
      "../../master_volume/datasets/youtube-trending-video-dataset/IN_category_id.jsonl\n",
      "../../master_volume/datasets/youtube-trending-video-dataset/JP_category_id.json\n",
      "../../master_volume/datasets/youtube-trending-video-dataset/JP_category_id.jsonl\n",
      "../../master_volume/datasets/youtube-trending-video-dataset/KR_category_id.json\n",
      "../../master_volume/datasets/youtube-trending-video-dataset/KR_category_id.jsonl\n",
      "../../master_volume/datasets/youtube-trending-video-dataset/MX_category_id.json\n",
      "../../master_volume/datasets/youtube-trending-video-dataset/MX_category_id.jsonl\n",
      "../../master_volume/datasets/youtube-trending-video-dataset/RU_category_id.json\n",
      "../../master_volume/datasets/youtube-trending-video-dataset/RU_category_id.jsonl\n",
      "../../master_volume/datasets/youtube-trending-video-dataset/US_category_id.json\n",
      "../../master_volume/datasets/youtube-trending-video-dataset/US_category_id.jsonl\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "\n",
    "print(\"Converting JSON to JSONL\")\n",
    "for root, directories, files in os.walk(f\"{output_dir}\"):\n",
    "        for filename in files:\n",
    "            path = os.path.join(root,filename)\n",
    "            output_path = f\"{path}l\"\n",
    "\n",
    "            if os.path.isfile(output_path):\n",
    "                continue\n",
    "\n",
    "            if path.endswith(\".json\"):\n",
    "                print(path)\n",
    "                with open(path, \"r\") as file:\n",
    "                    data = json.load(file)\n",
    "                    if type(data) is dict:\n",
    "                        data = [{\"key\": key, \"value\": data[key]} for key in data]\n",
    "\n",
    "                    with open(output_path, \"w\") as jsonl_file:\n",
    "                        for obj in data:\n",
    "                            json.dump(obj, jsonl_file)\n",
    "                            jsonl_file.write(\"\\n\")\n",
    "                    print(output_path)\n",
    "\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T12:42:59.162844Z",
     "end_time": "2023-04-23T12:42:59.178869Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dodanie plików do hdfs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "client = docker.from_env()\n",
    "container = client.containers.get('master')\n",
    "\n",
    "def hdfs_mkdir(path):\n",
    "    container.exec_run(f\"hdfs dfs -mkdir -p /{path}/\")\n",
    "\n",
    "def hdfs_upload(path):\n",
    "    directory = \"/\".join(path.split(\"/\")[:-1])\n",
    "    hdfs_mkdir(directory)\n",
    "    cmd = f\"hdfs dfs -put /data/master_volume/{path} /{directory}\"\n",
    "    print(cmd)\n",
    "    code, output = container.exec_run(cmd)\n",
    "    print(f\"exit code {code}\")\n",
    "    print(output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading to HDFS\n",
      "hdfs dfs -put /data/master_volume/datasets/covid-dataset.csv /datasets\n",
      "exit code 1\n",
      "b\"put: `/datasets/covid-dataset.csv': File exists\\n\"\n",
      "HDFS upload of 77.76MB took 2.67s\n",
      "hdfs dfs -put /data/master_volume/datasets/steam-dataset/steam_dataset/appinfo/dlc_data/missing.jsonl /datasets/steam-dataset/steam_dataset/appinfo/dlc_data\n",
      "exit code 1\n",
      "b\"put: `/datasets/steam-dataset/steam_dataset/appinfo/dlc_data/missing.jsonl': File exists\\n\"\n",
      "HDFS upload of 0.00MB took 2.66s\n",
      "hdfs dfs -put /data/master_volume/datasets/steam-dataset/steam_dataset/appinfo/dlc_data/steam_dlc_data.jsonl /datasets/steam-dataset/steam_dataset/appinfo/dlc_data\n",
      "exit code 1\n",
      "b\"put: `/datasets/steam-dataset/steam_dataset/appinfo/dlc_data/steam_dlc_data.jsonl': File exists\\n\"\n",
      "HDFS upload of 230.57MB took 2.62s\n",
      "hdfs dfs -put /data/master_volume/datasets/steam-dataset/steam_dataset/appinfo/dlc_data/timestamp.txt /datasets/steam-dataset/steam_dataset/appinfo/dlc_data\n",
      "exit code 1\n",
      "b\"put: `/datasets/steam-dataset/steam_dataset/appinfo/dlc_data/timestamp.txt': File exists\\n\"\n",
      "HDFS upload of 0.00MB took 2.61s\n",
      "hdfs dfs -put /data/master_volume/datasets/steam-dataset/steam_dataset/appinfo/store_data/steam_store_data.jsonl /datasets/steam-dataset/steam_dataset/appinfo/store_data\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[28], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mget_ipython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_cell_magic\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtimeit\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m-r 1 -n 1\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43mprint(\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mUploading to HDFS\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m)\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43mfor root, directories, files in os.walk(f\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{output_dir}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m):\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m        for filename in files:\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m            if filename.endswith(\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m):\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m                continue # skip JSON files, we have JSONL from previous step\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m            filepath = path = os.path\u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m                .join(root,filename)\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m            \u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m            path = filepath\u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m                .replace(\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m../../master_volume/\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m, \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m)\u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m                .replace(\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m, \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m)\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m            \u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m            start_single = timer()\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m            hdfs_upload(path)\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m            end_single = timer()\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m            print(f\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mHDFS upload of \u001B[39;49m\u001B[38;5;124;43m{\u001B[39;49m\u001B[38;5;124;43mos.stat(filepath).st_size / (1024 * 1024):.02f}MB took \u001B[39;49m\u001B[38;5;124;43m{\u001B[39;49m\u001B[38;5;124;43mend_single - start_single:.02f}s\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m)\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Uni\\pwr-pdzb\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2475\u001B[0m, in \u001B[0;36mInteractiveShell.run_cell_magic\u001B[1;34m(self, magic_name, line, cell)\u001B[0m\n\u001B[0;32m   2473\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[0;32m   2474\u001B[0m     args \u001B[38;5;241m=\u001B[39m (magic_arg_s, cell)\n\u001B[1;32m-> 2475\u001B[0m     result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   2477\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n\u001B[0;32m   2478\u001B[0m \u001B[38;5;66;03m# when using magics with decodator @output_can_be_silenced\u001B[39;00m\n\u001B[0;32m   2479\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n\u001B[0;32m   2480\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
      "File \u001B[1;32mE:\\Uni\\pwr-pdzb\\venv\\lib\\site-packages\\IPython\\core\\magics\\execution.py:1174\u001B[0m, in \u001B[0;36mExecutionMagics.timeit\u001B[1;34m(self, line, cell, local_ns)\u001B[0m\n\u001B[0;32m   1171\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m time_number \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.2\u001B[39m:\n\u001B[0;32m   1172\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m-> 1174\u001B[0m all_runs \u001B[38;5;241m=\u001B[39m \u001B[43mtimer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrepeat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrepeat\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnumber\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1175\u001B[0m best \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m(all_runs) \u001B[38;5;241m/\u001B[39m number\n\u001B[0;32m   1176\u001B[0m worst \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(all_runs) \u001B[38;5;241m/\u001B[39m number\n",
      "File \u001B[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\timeit.py:206\u001B[0m, in \u001B[0;36mTimer.repeat\u001B[1;34m(self, repeat, number)\u001B[0m\n\u001B[0;32m    204\u001B[0m r \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    205\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(repeat):\n\u001B[1;32m--> 206\u001B[0m     t \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtimeit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnumber\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    207\u001B[0m     r\u001B[38;5;241m.\u001B[39mappend(t)\n\u001B[0;32m    208\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m r\n",
      "File \u001B[1;32mE:\\Uni\\pwr-pdzb\\venv\\lib\\site-packages\\IPython\\core\\magics\\execution.py:158\u001B[0m, in \u001B[0;36mTimer.timeit\u001B[1;34m(self, number)\u001B[0m\n\u001B[0;32m    156\u001B[0m gc\u001B[38;5;241m.\u001B[39mdisable()\n\u001B[0;32m    157\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 158\u001B[0m     timing \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minner\u001B[49m\u001B[43m(\u001B[49m\u001B[43mit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtimer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    159\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    160\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gcold:\n",
      "File \u001B[1;32m<magic-timeit>:15\u001B[0m, in \u001B[0;36minner\u001B[1;34m(_it, _timer)\u001B[0m\n",
      "Cell \u001B[1;32mIn[16], line 12\u001B[0m, in \u001B[0;36mhdfs_upload\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m     10\u001B[0m cmd \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhdfs dfs -put /data/master_volume/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m /\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdirectory\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(cmd)\n\u001B[1;32m---> 12\u001B[0m code, output \u001B[38;5;241m=\u001B[39m \u001B[43mcontainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexec_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcmd\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexit code \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcode\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(output)\n",
      "File \u001B[1;32mE:\\Uni\\pwr-pdzb\\venv\\lib\\site-packages\\docker\\models\\containers.py:198\u001B[0m, in \u001B[0;36mContainer.exec_run\u001B[1;34m(self, cmd, stdout, stderr, stdin, tty, privileged, user, detach, stream, socket, environment, workdir, demux)\u001B[0m\n\u001B[0;32m    155\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    156\u001B[0m \u001B[38;5;124;03mRun a command inside this container. Similar to\u001B[39;00m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;124;03m``docker exec``.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    191\u001B[0m \u001B[38;5;124;03m        If the server returns an error.\u001B[39;00m\n\u001B[0;32m    192\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    193\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mapi\u001B[38;5;241m.\u001B[39mexec_create(\n\u001B[0;32m    194\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mid, cmd, stdout\u001B[38;5;241m=\u001B[39mstdout, stderr\u001B[38;5;241m=\u001B[39mstderr, stdin\u001B[38;5;241m=\u001B[39mstdin, tty\u001B[38;5;241m=\u001B[39mtty,\n\u001B[0;32m    195\u001B[0m     privileged\u001B[38;5;241m=\u001B[39mprivileged, user\u001B[38;5;241m=\u001B[39muser, environment\u001B[38;5;241m=\u001B[39menvironment,\n\u001B[0;32m    196\u001B[0m     workdir\u001B[38;5;241m=\u001B[39mworkdir,\n\u001B[0;32m    197\u001B[0m )\n\u001B[1;32m--> 198\u001B[0m exec_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexec_start\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresp\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mId\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdetach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdetach\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtty\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtty\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msocket\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msocket\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    200\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdemux\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdemux\u001B[49m\n\u001B[0;32m    201\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    202\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m socket \u001B[38;5;129;01mor\u001B[39;00m stream:\n\u001B[0;32m    203\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ExecResult(\u001B[38;5;28;01mNone\u001B[39;00m, exec_output)\n",
      "File \u001B[1;32mE:\\Uni\\pwr-pdzb\\venv\\lib\\site-packages\\docker\\utils\\decorators.py:19\u001B[0m, in \u001B[0;36mcheck_resource.<locals>.decorator.<locals>.wrapped\u001B[1;34m(self, resource_id, *args, **kwargs)\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m resource_id:\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m errors\u001B[38;5;241m.\u001B[39mNullResource(\n\u001B[0;32m     17\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mResource ID was not provided\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     18\u001B[0m     )\n\u001B[1;32m---> 19\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;28mself\u001B[39m, resource_id, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mE:\\Uni\\pwr-pdzb\\venv\\lib\\site-packages\\docker\\api\\exec_api.py:167\u001B[0m, in \u001B[0;36mExecApiMixin.exec_start\u001B[1;34m(self, exec_id, detach, tty, stream, socket, demux)\u001B[0m\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m socket:\n\u001B[0;32m    166\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_raw_response_socket(res)\n\u001B[1;32m--> 167\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_read_from_socket\u001B[49m\u001B[43m(\u001B[49m\u001B[43mres\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtty\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtty\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdemux\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdemux\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Uni\\pwr-pdzb\\venv\\lib\\site-packages\\docker\\api\\client.py:424\u001B[0m, in \u001B[0;36mAPIClient._read_from_socket\u001B[1;34m(self, response, stream, tty, demux)\u001B[0m\n\u001B[0;32m    421\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m gen\n\u001B[0;32m    422\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    423\u001B[0m     \u001B[38;5;66;03m# Wait for all the frames, concatenate them, and return the result\u001B[39;00m\n\u001B[1;32m--> 424\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mconsume_socket_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgen\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdemux\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdemux\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Uni\\pwr-pdzb\\venv\\lib\\site-packages\\docker\\utils\\socket.py:149\u001B[0m, in \u001B[0;36mconsume_socket_output\u001B[1;34m(frames, demux)\u001B[0m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;124;03mIterate through frames read from the socket and return the result.\u001B[39;00m\n\u001B[0;32m    137\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    144\u001B[0m \u001B[38;5;124;03m        concatenation of frames belonging to the same stream.\u001B[39;00m\n\u001B[0;32m    145\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    146\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m demux \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[0;32m    147\u001B[0m     \u001B[38;5;66;03m# If the streams are multiplexed, the generator returns strings, that\u001B[39;00m\n\u001B[0;32m    148\u001B[0m     \u001B[38;5;66;03m# we just need to concatenate.\u001B[39;00m\n\u001B[1;32m--> 149\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mbytes\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    151\u001B[0m \u001B[38;5;66;03m# If the streams are demultiplexed, the generator yields tuples\u001B[39;00m\n\u001B[0;32m    152\u001B[0m \u001B[38;5;66;03m# (stdout, stderr)\u001B[39;00m\n\u001B[0;32m    153\u001B[0m out \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m]\n",
      "File \u001B[1;32mE:\\Uni\\pwr-pdzb\\venv\\lib\\site-packages\\docker\\api\\client.py:418\u001B[0m, in \u001B[0;36m<genexpr>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    415\u001B[0m     gen \u001B[38;5;241m=\u001B[39m (demux_adaptor(\u001B[38;5;241m*\u001B[39mframe) \u001B[38;5;28;01mfor\u001B[39;00m frame \u001B[38;5;129;01min\u001B[39;00m gen)\n\u001B[0;32m    416\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    417\u001B[0m     \u001B[38;5;66;03m# The generator will output strings\u001B[39;00m\n\u001B[1;32m--> 418\u001B[0m     gen \u001B[38;5;241m=\u001B[39m (data \u001B[38;5;28;01mfor\u001B[39;00m (_, data) \u001B[38;5;129;01min\u001B[39;00m gen)\n\u001B[0;32m    420\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stream:\n\u001B[0;32m    421\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m gen\n",
      "File \u001B[1;32mE:\\Uni\\pwr-pdzb\\venv\\lib\\site-packages\\docker\\utils\\socket.py:106\u001B[0m, in \u001B[0;36mframes_iter_no_tty\u001B[1;34m(socket)\u001B[0m\n\u001B[0;32m    101\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    102\u001B[0m \u001B[38;5;124;03mReturns a generator of data read from the socket when the tty setting is\u001B[39;00m\n\u001B[0;32m    103\u001B[0m \u001B[38;5;124;03mnot enabled.\u001B[39;00m\n\u001B[0;32m    104\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    105\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 106\u001B[0m     (stream, n) \u001B[38;5;241m=\u001B[39m \u001B[43mnext_frame_header\u001B[49m\u001B[43m(\u001B[49m\u001B[43msocket\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    107\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    108\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32mE:\\Uni\\pwr-pdzb\\venv\\lib\\site-packages\\docker\\utils\\socket.py:78\u001B[0m, in \u001B[0;36mnext_frame_header\u001B[1;34m(socket)\u001B[0m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     72\u001B[0m \u001B[38;5;124;03mReturns the stream and size of the next frame of data waiting to be read\u001B[39;00m\n\u001B[0;32m     73\u001B[0m \u001B[38;5;124;03mfrom socket, according to the protocol defined here:\u001B[39;00m\n\u001B[0;32m     74\u001B[0m \n\u001B[0;32m     75\u001B[0m \u001B[38;5;124;03mhttps://docs.docker.com/engine/api/v1.24/#attach-to-a-container\u001B[39;00m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 78\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[43mread_exactly\u001B[49m\u001B[43m(\u001B[49m\u001B[43msocket\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m SocketError:\n\u001B[0;32m     80\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32mE:\\Uni\\pwr-pdzb\\venv\\lib\\site-packages\\docker\\utils\\socket.py:63\u001B[0m, in \u001B[0;36mread_exactly\u001B[1;34m(socket, n)\u001B[0m\n\u001B[0;32m     61\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mbytes\u001B[39m()\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(data) \u001B[38;5;241m<\u001B[39m n:\n\u001B[1;32m---> 63\u001B[0m     next_data \u001B[38;5;241m=\u001B[39m \u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43msocket\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     64\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m next_data:\n\u001B[0;32m     65\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m SocketError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnexpected EOF\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mE:\\Uni\\pwr-pdzb\\venv\\lib\\site-packages\\docker\\utils\\socket.py:38\u001B[0m, in \u001B[0;36mread\u001B[1;34m(socket, n)\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(socket, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrecv\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m---> 38\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msocket\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     39\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(socket, \u001B[38;5;28mgetattr\u001B[39m(pysocket, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSocketIO\u001B[39m\u001B[38;5;124m'\u001B[39m)):\n\u001B[0;32m     40\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m socket\u001B[38;5;241m.\u001B[39mread(n)\n",
      "File \u001B[1;32mE:\\Uni\\pwr-pdzb\\venv\\lib\\site-packages\\docker\\transport\\npipesocket.py:22\u001B[0m, in \u001B[0;36mcheck_closed.<locals>.wrapped\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_closed:\n\u001B[0;32m     19\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m     20\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCan not reuse socket after connection was closed.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     21\u001B[0m     )\n\u001B[1;32m---> 22\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mE:\\Uni\\pwr-pdzb\\venv\\lib\\site-packages\\docker\\transport\\npipesocket.py:116\u001B[0m, in \u001B[0;36mNpipeSocket.recv\u001B[1;34m(self, bufsize, flags)\u001B[0m\n\u001B[0;32m    114\u001B[0m \u001B[38;5;129m@check_closed\u001B[39m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrecv\u001B[39m(\u001B[38;5;28mself\u001B[39m, bufsize, flags\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m):\n\u001B[1;32m--> 116\u001B[0m     err, data \u001B[38;5;241m=\u001B[39m \u001B[43mwin32file\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mReadFile\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbufsize\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "\n",
    "print(\"Uploading to HDFS\")\n",
    "\n",
    "for root, directories, files in os.walk(f\"{output_dir}\"):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".json\"):\n",
    "                continue # skip JSON files, we have JSONL from previous step\n",
    "            filepath = path = os.path\\\n",
    "                .join(root,filename)\n",
    "\n",
    "            path = filepath\\\n",
    "                .replace(\"../../master_volume/\", \"\")\\\n",
    "                .replace(\"\\\\\", \"/\")\n",
    "\n",
    "            start_single = timer()\n",
    "            hdfs_upload(path)\n",
    "            end_single = timer()\n",
    "            print(f\"HDFS upload of {os.stat(filepath).st_size / (1024 * 1024):.02f}MB took {end_single - start_single:.02f}s\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T12:43:31.174676Z",
     "end_time": "2023-04-23T12:43:31.190297Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "def hdfs_set_replication_level(number):\n",
    "    container.exec_run(f\"hdfs dfs -setrep -R {number} /\")\n",
    "\n",
    "hdfs_set_replication_level(3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T12:00:55.251739Z",
     "end_time": "2023-04-23T12:00:56.718692Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Akwizycja danych zmiennych\n",
    "\n",
    "Nasz proces zakłada, że na podstawie części danych niezmiennych dotyczących gier z serwisu Steam (steam_dataset) wytypujemy te gry, o których liczby graczy na przestrzeni czasu będziemy pytać SteamCharts API za pomocą bardzo prostego zapytania\n",
    "```\n",
    "https://steamcharts.com/app/<appid>/chart-data.json\n",
    "```\n",
    "\n",
    "W celu akwizycji tych danych przygotowaliśmy proces map-reduce, który przyjmuje na wejściu potrzebne id i zwraca wyniki zapytania w odpowiedniej postaci. Na potrzeby tej listy zadań umieściliśmy w hdfs odpowiedni plik z wybranymi przez nas identyfikatorami do celów testowych. W przyszłości ten krok będzie wykorzystywał wyniki z poprzednich podprocesów."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
