{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sprawozdanie 6 - akwizycja danych\n",
    "\n",
    "## Środowisko\n",
    "Mamy maszynę wirtualną z Ubuntu postawioną za pomocą Vagrant'a (korzystającego pod spodem z VirtualBox'a). Na tej maszynie wirtualnej stawiamy kontenery Docker'a.\n",
    "\n",
    "Aby ułatwić sobie późniejszą pracę z obrazami na których postawiony jest hadoop postanowiliśmy dodać do master-node volumen na dane (modyfikując skrypty generujące docker-compose). Dzięki temu możemy w wygodny sposób (tj. poprzez wrzucenie do odpowiedniego folderu) przenosić pliki do miejsca, do którego możemy się dostać z poziomu maszyny z hadoopem. Warto zwrócić uwagę, że maszyna wirtualna także posiada taki wolumen, który zapewnia wykorzystanie Vagrant'a.\n",
    "\n",
    "```yaml\n",
    "master:\n",
    "    image: hjben/hadoop-eco:$hadoop_version\n",
    "    hostname: master\n",
    "    container_name: master\n",
    "    privileged: true\n",
    "    ports:\n",
    "      - 8088:8088\n",
    "      - 9870:9870\n",
    "      - 8042:8042\n",
    "      - 10000:10000\n",
    "      - 10002:10002\n",
    "      - 16010:16010\n",
    "    volumes:\n",
    "      - /sys/fs/cgroup:/sys/fs/cgroup\n",
    "      - $hdfs_path:/data/hadoop\n",
    "      - $hadoop_log_path:/usr/local/hadoop/logs\n",
    "      - $hbase_log_path/master:/usr/local/hbase/logs\n",
    "      - $hive_log_path:/usr/local/hive/logs\n",
    "      - $sqoop_log_path:/usr/local/sqoop/logs\n",
    "      - /vagrant/master_volume:/data/master_volume <-------------- dodany volumen\n",
    "    networks:\n",
    "      hadoop-cluster:\n",
    "        ipv4_address: 10.1.2.3\n",
    "    extra_hosts:\n",
    "      - \"mariadb:10.1.2.2\"\n",
    "      - \"master:10.1.2.3\"\n",
    "```\n",
    "\n",
    "## Pobieranie danych\n",
    "Niestety przez potrzebę generowania i podania klucza API do serwisu kaggle przed pobraniem danych należy wykonać kilka czynności.\n",
    "\n",
    "1. Pobrać ze strony kaggle klucz API (kaggle.json)\n",
    "2. Stworzyć folder .kaggle w głównym katalogu użytkownika i skopiować tam klucz API\n",
    "3. Wywołać nasz skrypt setup.sh, który:\n",
    "    1. Przygotowuje środowisko pythonowe\n",
    "    2. Pobiera z kaggle:\n",
    "       * [YouTube Trending Video Dataset](https://www.kaggle.com/datasets/rsrishav/youtube-trending-video-dataset)\n",
    "       * [Steam Dataset](https://www.kaggle.com/datasets/souyama/steam-dataset)\n",
    "    3. Pobiera z sieci [dane Covid'owe](https://covid.ourworldindata.org/data/owid-covid-data.csv)\n",
    "\n",
    "W tym momencie na dysku powinny znajdować się spakowane pliki z danymi\n",
    "\n",
    "## Formatowanie danych\n",
    "Po rozpakowaniu danych widać, że część z nich ma format trudny do późniejszej pracy. Ostatecznie postanowiliśmy przed wrzuceniem plików do hdfs wszystkie przetransformować do dormatu .jsonl. Format .jsonl zawiera obiekty json, każdy w kolejnej linii. Dzięki zastosowaniu takiego formatu na kolejnych laboratoriach będzie można wykorzystywać odpowiednie mappery.\n",
    "\n",
    "W tym momencie mamy pliki z danymi w formacie .jsonl (+ covid.csv) Pliki te są umieszczone na volumenie, który widzi kontener z hdoop'em.\n",
    "\n",
    "## Dodanie plików do hdfs\n",
    "\n",
    "Stworzyliśmy skrypt w języku python, który dodaje wszystkie pliki .jsonl do hdfs. Skrypt ten musi być uruchomiony z wirtualnej maszyny z Ubuntu. Skrypt ten ustawia liczbę replik danych na 4.\n",
    "\n",
    "```py\n",
    "import docker\n",
    "client = docker.from_env()\n",
    "container = client.containers.get('master')\n",
    "\n",
    "def hdfs_mkdir(path):\n",
    "    container.exec_run(f\"hdfs dfs -mkdir -p /{path}/\")\n",
    "\n",
    "def hdfs_upload(path):\n",
    "    directory = \"/\".join(path.split(\"/\")[:-1])\n",
    "    hdfs_mkdir(directory)\n",
    "    container.exec_run(f\"hdfs dfs -put /data/master_volume/{path} /{directory}\")\n",
    "\n",
    "def hdfs_set_replication_level(number):\n",
    "    container.exec_run(f\"hdfs dfs -setrep -R {number} /\")\n",
    "\n",
    "hdfs_upload(\"covid.csv\")\n",
    "hdfs_upload(\"steam_dataset\")\n",
    "hdfs_upload(\"test.txt\")\n",
    "hdfs_set_replication_level(4)\n",
    "```\n",
    "\n",
    "W tym momencie mamy dane, które uznajemy jako niezmienne w hdfs\n",
    "\n",
    "![](images/DaneWhdfsConsole.png)\n",
    "![](images/DaneWhdfsClient.png)\n",
    "\n",
    "## Akwizycja danych zmiennych\n",
    "\n",
    "Nasz proces zakłada, że na podstawie części danych niezmiennych dotyczących gier z serwisu Steam (steam_dataset) wytypujemy te gry, o których liczby graczy na przestrzeni czasu będziemy pytać SteamCharts API za pomocą bardzo prostego zapytania\n",
    "```\n",
    "https://steamcharts.com/app/<appid>/chart-data.json\n",
    "```\n",
    "\n",
    "W celu akwizycji tych danych przygotowaliśmy proces map-reduce, który przyjmuje na wejściu potrzebne id i zwraca wyniki zapytania w odpowiedniej postaci. Na potrzeby tej listy zadań umieściliśmy w hdfs odpowiedni plik z wybranymi przez nas identyfikatorami do celów testowych. W przyszłości ten krok będzie wykorzystywał wyniki z poprzednich podprocesów.\n",
    "\n",
    "![](images/SteamCharts.png)\n",
    "\n",
    "```scala\n",
    "```\n",
    "\n",
    "![](images/DaneWhdfsSteam.png)\n",
    "\n",
    "## Testowe map-reduce\n",
    "\n",
    "Do każdego pliku przygotowaliśmy testowy map-reduce zliczający konkretną wartość, aby sprawdzić, czy pliki mają poprawny, możliwy do przetworzenia format"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import os\n",
    "from timeit import default_timer as timer\n",
    "import requests\n",
    "import docker\n",
    "import json\n",
    "import opendatasets as od"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T12:52:16.073489Z",
     "end_time": "2023-04-23T12:52:16.081337Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-23T12:41:37.548418Z",
     "end_time": "2023-04-23T12:41:37.582316Z"
    }
   },
   "outputs": [],
   "source": [
    "client = docker.from_env()\n",
    "container = client.containers.get('master')\n",
    "\n",
    "def hdfs_mkdir(path):\n",
    "    container.exec_run(f\"hdfs dfs -mkdir -p /{path}/\")\n",
    "\n",
    "def hdfs_upload(path):\n",
    "    directory = \"/\".join(path.split(\"/\")[:-1])\n",
    "    hdfs_mkdir(directory)\n",
    "    cmd = f\"hdfs dfs -put /data/master_volume/{path} /{directory}\"\n",
    "    print(cmd)\n",
    "    code, output = container.exec_run(cmd)\n",
    "    print(f\"exit code {code}\")\n",
    "    print(output)\n",
    "\n",
    "def hdfs_set_replication_level(number):\n",
    "    container.exec_run(f\"hdfs dfs -setrep -R {number} /\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ensure `kaggle.json` is located in the root of the repository or at `~/.kaggle/kaggle.json`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "output_dir = \"../../master_volume/datasets\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T12:42:01.739906Z",
     "end_time": "2023-04-23T12:42:01.755541Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset steam-dataset already exists, skipping download\n",
      "Dataset youtube-trending-video-dataset already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(f\"{output_dir}/steam-dataset\"):\n",
    "    od.download(\"https://www.kaggle.com/datasets/souyama/steam-dataset\", f\"{output_dir}\")\n",
    "else:\n",
    "    print(\"Dataset steam-dataset already exists, skipping download\")\n",
    "\n",
    "if not os.path.isdir(f\"{output_dir}/youtube-trending-video-dataset\"):\n",
    "    od.download(\"https://www.kaggle.com/datasets/rsrishav/youtube-trending-video-dataset\", f\"{output_dir}\")\n",
    "else:\n",
    "    print(\"Dataset youtube-trending-video-dataset already exists, skipping download\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T12:42:05.522809Z",
     "end_time": "2023-04-23T12:42:05.534496Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset covid-dataset already exists, skipping download\n",
      "covid-dataset.csv: 77.76MB\n"
     ]
    }
   ],
   "source": [
    "path = f\"{output_dir}/covid-dataset.csv\"\n",
    "\n",
    "if not os.path.isfile(path):\n",
    "    print(f\"Downloading covid-dataset to {path}\")\n",
    "    start = timer()\n",
    "    r = requests.get(\"https://covid.ourworldindata.org/data/owid-covid-data.csv\", allow_redirects=True)\n",
    "    with open(path, 'wb') as file:\n",
    "        file.write(r.content)\n",
    "    end = timer()\n",
    "else:\n",
    "    print(\"Dataset covid-dataset already exists, skipping download\")\n",
    "\n",
    "print(f\"covid-dataset.csv: {os.stat(path).st_size / (1024 * 1024):.02f}MB\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T12:42:10.207948Z",
     "end_time": "2023-04-23T12:42:10.223587Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting JSON to JSONL\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Converting JSON to JSONL\")\n",
    "for root, directories, files in os.walk(f\"{output_dir}\"):\n",
    "        for filename in files:\n",
    "            path = os.path.join(root,filename)\n",
    "            output_path = f\"{path}l\"\n",
    "\n",
    "            if os.path.isfile(output_path):\n",
    "                continue\n",
    "\n",
    "            if path.endswith(\".json\"):\n",
    "                print(path)\n",
    "                with open(path, \"r\") as file:\n",
    "                    data = json.load(file)\n",
    "                    if type(data) is dict:\n",
    "                        data = [{\"key\": key, \"value\": data[key]} for key in data]\n",
    "\n",
    "                    with open(output_path, \"w\") as jsonl_file:\n",
    "                        for obj in data:\n",
    "                            json.dump(obj, jsonl_file)\n",
    "                            jsonl_file.write(\"\\n\")\n",
    "                    print(output_path)\n",
    "\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T12:42:59.162844Z",
     "end_time": "2023-04-23T12:42:59.178869Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading to HDFS\n",
      "datasets/covid-dataset.csv\n",
      "datasets/steam-dataset/steam_dataset/appinfo/dlc_data/missing.jsonl\n",
      "datasets/steam-dataset/steam_dataset/appinfo/dlc_data/steam_dlc_data.jsonl\n",
      "datasets/steam-dataset/steam_dataset/appinfo/dlc_data/timestamp.txt\n",
      "datasets/steam-dataset/steam_dataset/appinfo/store_data/steam_store_data.jsonl\n",
      "datasets/steam-dataset/steam_dataset/appinfo/store_data/timestamp.txt\n",
      "datasets/steam-dataset/steam_dataset/news_data/missing.jsonl\n",
      "datasets/steam-dataset/steam_dataset/news_data/steam_news_data.jsonl\n",
      "datasets/steam-dataset/steam_dataset/news_data/timestamp.txt\n",
      "datasets/steam-dataset/steam_dataset/steamspy/basic/steam_spy_scrap.jsonl\n",
      "datasets/steam-dataset/steam_dataset/steamspy/basic/timestamp.txt\n",
      "datasets/steam-dataset/steam_dataset/steamspy/detailed/steam_spy_detailed.jsonl\n",
      "datasets/steam-dataset/steam_dataset/steamspy/detailed/timestamp.txt\n",
      "datasets/steam-dataset/steam_dataset/steam_charts/missing.jsonl\n",
      "datasets/steam-dataset/steam_dataset/steam_charts/steam_charts.jsonl\n",
      "datasets/steam-dataset/steam_dataset/steam_charts/timestamp.txt\n",
      "datasets/youtube-trending-video-dataset/BR_category_id.jsonl\n",
      "datasets/youtube-trending-video-dataset/BR_youtube_trending_data.csv\n",
      "datasets/youtube-trending-video-dataset/CA_category_id.jsonl\n",
      "datasets/youtube-trending-video-dataset/CA_youtube_trending_data.csv\n",
      "datasets/youtube-trending-video-dataset/DE_category_id.jsonl\n",
      "datasets/youtube-trending-video-dataset/DE_youtube_trending_data.csv\n",
      "datasets/youtube-trending-video-dataset/FR_category_id.jsonl\n",
      "datasets/youtube-trending-video-dataset/FR_youtube_trending_data.csv\n",
      "datasets/youtube-trending-video-dataset/GB_category_id.jsonl\n",
      "datasets/youtube-trending-video-dataset/GB_youtube_trending_data.csv\n",
      "datasets/youtube-trending-video-dataset/IN_category_id.jsonl\n",
      "datasets/youtube-trending-video-dataset/IN_youtube_trending_data.csv\n",
      "datasets/youtube-trending-video-dataset/JP_category_id.jsonl\n",
      "datasets/youtube-trending-video-dataset/JP_youtube_trending_data.csv\n",
      "datasets/youtube-trending-video-dataset/KR_category_id.jsonl\n",
      "datasets/youtube-trending-video-dataset/KR_youtube_trending_data.csv\n",
      "datasets/youtube-trending-video-dataset/MX_category_id.jsonl\n",
      "datasets/youtube-trending-video-dataset/MX_youtube_trending_data.csv\n",
      "datasets/youtube-trending-video-dataset/RU_category_id.jsonl\n",
      "datasets/youtube-trending-video-dataset/RU_youtube_trending_data.csv\n",
      "datasets/youtube-trending-video-dataset/US_category_id.jsonl\n",
      "datasets/youtube-trending-video-dataset/US_youtube_trending_data.csv\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Uploading to HDFS\")\n",
    "for root, directories, files in os.walk(f\"{output_dir}\"):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".json\"):\n",
    "                continue # skip JSON files, we have JSONL from previous step\n",
    "            path = os.path\\\n",
    "                .join(root,filename)\\\n",
    "                .removeprefix(\"../../master_volume/\")\\\n",
    "                .replace(\"\\\\\", \"/\")\n",
    "\n",
    "            hdfs_upload(path)\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T12:43:31.174676Z",
     "end_time": "2023-04-23T12:43:31.190297Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "hdfs_set_replication_level(3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T12:00:55.251739Z",
     "end_time": "2023-04-23T12:00:56.718692Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
