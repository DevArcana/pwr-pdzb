{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprawozdanie 6 - akwizycja danych\n",
    "\n",
    "## Środowisko\n",
    "\n",
    "Mamy maszynę wirtualną z Ubuntu postawioną za pomocą Vagrant'a (korzystającego pod spodem z VirtualBox'a). Na tej maszynie wirtualnej stawiamy kontenery Docker'a.\n",
    "\n",
    "Aby ułatwić sobie późniejszą pracę z obrazami na których postawiony jest hadoop postanowiliśmy dodać do master-node volumen na dane (modyfikując skrypty generujące docker-compose). Dzięki temu możemy w wygodny sposób (tj. poprzez wrzucenie do odpowiedniego folderu) przenosić pliki do miejsca, do którego możemy się dostać z poziomu maszyny z hadoopem. Warto zwrócić uwagę, że maszyna wirtualna także posiada taki wolumen, który zapewnia wykorzystanie Vagrant'a.\n",
    "\n",
    "```yaml\n",
    "master:\n",
    "    image: hjben/hadoop-eco:$hadoop_version\n",
    "    hostname: master\n",
    "    container_name: master\n",
    "    privileged: true\n",
    "    ports:\n",
    "      - 8088:8088\n",
    "      - 9870:9870\n",
    "      - 8042:8042\n",
    "      - 10000:10000\n",
    "      - 10002:10002\n",
    "      - 16010:16010\n",
    "    volumes:\n",
    "      - /sys/fs/cgroup:/sys/fs/cgroup\n",
    "      - $hdfs_path:/data/hadoop\n",
    "      - $hadoop_log_path:/usr/local/hadoop/logs\n",
    "      - $hbase_log_path/master:/usr/local/hbase/logs\n",
    "      - $hive_log_path:/usr/local/hive/logs\n",
    "      - $sqoop_log_path:/usr/local/sqoop/logs\n",
    "      - /vagrant/master_volume:/data/master_volume <-------------- dodany volumen\n",
    "    networks:\n",
    "      hadoop-cluster:\n",
    "        ipv4_address: 10.1.2.3\n",
    "    extra_hosts:\n",
    "      - \"mariadb:10.1.2.2\"\n",
    "      - \"master:10.1.2.3\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pobieranie danych\n",
    "Niestety przez potrzebę generowania i podania klucza API do serwisu kaggle przed pobraniem danych należy wykonać kilka czynności.\n",
    "\n",
    "1. Pobrać ze strony kaggle klucz API (kaggle.json)\n",
    "2. Stworzyć folder .kaggle w głównym katalogu użytkownika i skopiować tam klucz API\n",
    "3. Poniższe funkcje:\n",
    "    1. Pobierają z kaggle:\n",
    "       * [YouTube Trending Video Dataset](https://www.kaggle.com/datasets/rsrishav/youtube-trending-video-dataset)\n",
    "       * [Steam Dataset](https://www.kaggle.com/datasets/souyama/steam-dataset)\n",
    "    2. Pobierają z sieci [dane Covid'owe](https://covid.ourworldindata.org/data/owid-covid-data.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-06T14:16:38.652199300Z",
     "start_time": "2023-05-06T14:16:38.050540100Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from timeit import default_timer as timer\n",
    "import requests\n",
    "import docker\n",
    "import json\n",
    "import opendatasets as od\n",
    "import csv\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-06T14:11:36.137358Z",
     "start_time": "2023-05-06T14:11:36.121207700Z"
    }
   },
   "outputs": [],
   "source": [
    "output_dir = \"data/master_volume/datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T12:42:05.534496Z",
     "start_time": "2023-04-23T12:42:05.522809Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(f\"{output_dir}/youtube-trending-video-dataset\"):\n",
    "    od.download(\"https://www.kaggle.com/datasets/rsrishav/youtube-trending-video-dataset\", f\"{output_dir}\")\n",
    "else:\n",
    "    print(\"Dataset youtube-trending-video-dataset already exists, skipping download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(f\"{output_dir}/steam-dataset\"):\n",
    "    od.download(\"https://www.kaggle.com/datasets/souyama/steam-dataset\", f\"{output_dir}\")\n",
    "else:\n",
    "    print(\"Dataset steam-dataset already exists, skipping download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T12:42:10.223587Z",
     "start_time": "2023-04-23T12:42:10.207948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading covid-dataset to data/master_volume/datasets/covid-dataset.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/master_volume/datasets/covid-dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m start \u001b[38;5;241m=\u001b[39m timer()\n\u001b[1;32m      6\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://covid.ourworldindata.org/data/owid-covid-data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      8\u001b[0m     file\u001b[38;5;241m.\u001b[39mwrite(r\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m      9\u001b[0m end \u001b[38;5;241m=\u001b[39m timer()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/master_volume/datasets/covid-dataset.csv'"
     ]
    }
   ],
   "source": [
    "path = f\"{output_dir}/covid-dataset.csv\"\n",
    "\n",
    "if not os.path.isfile(path):\n",
    "    print(f\"Downloading covid-dataset to {path}\")\n",
    "    start = timer()\n",
    "    r = requests.get(\"https://covid.ourworldindata.org/data/owid-covid-data.csv\", allow_redirects=True)\n",
    "    with open(path, 'wb') as file:\n",
    "        file.write(r.content)\n",
    "    end = timer()\n",
    "    print(f\"Download finished in {end - start:.02f}s\")\n",
    "else:\n",
    "    print(\"Dataset covid-dataset already exists, skipping download\")\n",
    "\n",
    "print(f\"covid-dataset.csv: {os.stat(path).st_size / (1024 * 1024):.02f}MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatowanie danych\n",
    "Po rozpakowaniu danych widać, że część z nich ma format trudny do późniejszej pracy. Ostatecznie postanowiliśmy przed wrzuceniem plików do hdfs wszystkie przetransformować do dormatu .jsonl. Format .jsonl zawiera obiekty json, każdy w kolejnej linii. Dzięki zastosowaniu takiego formatu łatwo będzie można implementować procesy map-reduce.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-06T14:37:42.716725600Z",
     "start_time": "2023-05-06T14:37:31.658548100Z"
    }
   },
   "outputs": [],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "\n",
    "print(\"Converting CSV to JSONL\")\n",
    "for root, directories, files in os.walk(f\"{output_dir}\"):\n",
    "        for filename in files:\n",
    "            path = os.path.join(root,filename)\n",
    "            output_path = path.replace(\".csv\", \".jsonl\")\n",
    "\n",
    "            if os.path.isfile(output_path):\n",
    "                # os.remove(output_path)\n",
    "                continue\n",
    "\n",
    "            if path.endswith(\".csv\"):\n",
    "                try:\n",
    "                    print(path)\n",
    "                    with open(path, 'r', encoding='utf-8', errors='replace') as infile, open(output_path, 'w', encoding='utf-8', errors='replace') as outfile:\n",
    "                        reader = csv.reader(x.replace('\\0', '') for x in infile)\n",
    "                        headers = next(reader)\n",
    "                        data = list(reader)\n",
    "                        df = pandas.DataFrame(data, columns=headers)\n",
    "                        for row in df.to_dict('records'):\n",
    "                            json.dump(row, outfile)\n",
    "                            outfile.write('\\n')\n",
    "                    print(output_path)\n",
    "                except Exception as e:\n",
    "                    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T12:42:59.178869Z",
     "start_time": "2023-04-23T12:42:59.162844Z"
    }
   },
   "outputs": [],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "\n",
    "print(\"Converting JSON to JSONL\")\n",
    "for root, directories, files in os.walk(f\"{output_dir}\"):\n",
    "        for filename in files:\n",
    "            path = os.path.join(root,filename)\n",
    "            output_path = f\"{path}l\"\n",
    "\n",
    "            if os.path.isfile(output_path):\n",
    "                continue\n",
    "\n",
    "            if path.endswith(\".json\"):\n",
    "                print(path)\n",
    "                with open(path, \"r\") as file:\n",
    "                    data = json.load(file)\n",
    "                    if type(data) is dict:\n",
    "                        data = [{\"key\": key, \"value\": data[key]} for key in data]\n",
    "\n",
    "                    with open(output_path, \"w\") as jsonl_file:\n",
    "                        for obj in data:\n",
    "                            json.dump(obj, jsonl_file)\n",
    "                            jsonl_file.write(\"\\n\")\n",
    "                    print(output_path)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dodanie plików do hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = docker.from_env()\n",
    "#container = client.containers.get('namenode')\n",
    "\n",
    "def hdfs_mkdir(path):\n",
    "    container.exec_run(f\"hdfs dfs -mkdir -p /{path}/\")\n",
    "\n",
    "def hdfs_upload(path):\n",
    "    directory = \"/\".join(path.split(\"/\")[:-1])\n",
    "    hdfs_mkdir(directory)\n",
    "    cmd = f\"hdfs dfs -put /data/master_volume/{path} /{directory}\"\n",
    "    print(cmd)\n",
    "    code, output = container.exec_run(cmd)\n",
    "    print(f\"exit code {code}\")\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T12:43:31.190297Z",
     "start_time": "2023-04-23T12:43:31.174676Z"
    }
   },
   "outputs": [],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "\n",
    "print(\"Uploading to HDFS\")\n",
    "\n",
    "for root, directories, files in os.walk(f\"{output_dir}\"):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".json\"):\n",
    "                continue # skip JSON files, we have JSONL from previous step\n",
    "            filepath = path = os.path\\\n",
    "                .join(root,filename)\n",
    "\n",
    "            path = filepath\\\n",
    "                .replace(\"../../master_volume/\", \"\")\\\n",
    "                .replace(\"\\\\\", \"/\")\n",
    "\n",
    "            start_single = timer()\n",
    "            hdfs_upload(path)\n",
    "            end_single = timer()\n",
    "            print(f\"HDFS upload of {os.stat(filepath).st_size / (1024 * 1024):.02f}MB took {end_single - start_single:.02f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T12:00:56.718692Z",
     "start_time": "2023-04-23T12:00:55.251739Z"
    }
   },
   "outputs": [],
   "source": [
    "def hdfs_set_replication_level(number):\n",
    "    container.exec_run(f\"hdfs dfs -setrep -R {number} /\")\n",
    "\n",
    "hdfs_set_replication_level(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Akwizycja danych zmiennych\n",
    "\n",
    "Nasz proces zakłada, że na podstawie części danych niezmiennych dotyczących gier z serwisu Steam (steam_dataset) wytypujemy te gry, o których liczby graczy na przestrzeni czasu będziemy pytać SteamCharts API za pomocą bardzo prostego zapytania\n",
    "```\n",
    "https://steamcharts.com/app/<appid>/chart-data.json\n",
    "```\n",
    "\n",
    "W celu akwizycji tych danych przygotowaliśmy proces map-reduce, który przyjmuje na wejściu potrzebne id i zwraca wyniki zapytania w odpowiedniej postaci (zgodnie z poniższą ilustracją). Na dzień dzisiejszy wybór odpowiednich gier jest symulowany (wybierane jest po prostu pierwsze 20). Pierwszy podproces, który agreguje dane został poprawnie zaimplementowany jako proces map-reduce.\n",
    "\n",
    "![](images/SteamChartsBig.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregacja danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = docker.from_env()\n",
    "container = client.containers.get('master')\n",
    "\n",
    "res1 = container.exec_run((\"yarn jar /data/master_volume/map_reduce_jars/1.jar \"\n",
    "\"/datasets/steam-dataset/steam_dataset/appinfo/store_data/steam_store_data.jsonl \"\n",
    "\"/datasets/steam-dataset/steam_dataset/steamspy/basic/steam_spy_scrap.jsonl \"\n",
    "\"/out_steam_1\"))\n",
    "\n",
    "res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = container.exec_run(\"hdfs dfs -cat /out_steam_1/part-r-00000\").output.decode('utf-8')\n",
    "print(f\"{raw[0:10000]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = container.exec_run((\"yarn jar /data/master_volume/map_reduce_jars/2.jar \"\n",
    "\"/out_steam_1/part-r-00000 \"\n",
    "\"/out_steam_2\"))\n",
    "\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = container.exec_run(\"hdfs dfs -cat /out_steam_2/part-r-00000\").output.decode('utf-8')\n",
    "print(f\"{raw[0:10000]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res3 = container.exec_run((\"yarn jar /data/master_volume/map_reduce_jars/3.jar \"\n",
    "\"/out_steam_2/part-r-00000 \"\n",
    "\"/out_steam_3\"))\n",
    "\n",
    "res3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = container.exec_run(\"hdfs dfs -cat /out_steam_3/part-r-00000\").output.decode('utf-8')\n",
    "print(f\"{raw[0:1000000]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Akwizycja danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snippety kodu map-reduce\n",
    "\n",
    "Jak zapewne łatwo zauważyć, poniższy kod napisany jest w języku scala. Największą rzeczą jaką trzeba było zrobić, aby dostosować map-reduce do scali było napisanie funkcji konwertującej iterator w reduce. Iterator ten nie jest zgodny z API, co powodowało konieczność wcześniejszej konwersji jego ArrayList.\n",
    "\n",
    "```scala\n",
    "  class MyMapper extends HadoopJob.HadoopMapper[AnyRef, Text, Text, Text] {\n",
    "    override def myMap(key: AnyRef, value: Text, emit: (Text, Text) => Unit): Unit = {\n",
    "      val jsons  = value.toString.split(\"\\n\").map(x => x.dropWhile(!_.isWhitespace)).map(_.trim).toList\n",
    "      val mapped = jsons.flatMap(x => Input.decoder.decodeJson(x).toOption)\n",
    "\n",
    "      def downloadTimestamps(id: Int) = for {\n",
    "        res  <- zio.http.Client.request(f\"\"\"https://steamcharts.com/app/$id/chart-data.json\"\"\")\n",
    "        data <- res.body.asString\n",
    "        json  = data.fromJson[ApiResult].getOrElse(List.empty)\n",
    "      } yield json\n",
    "\n",
    "      val workflow = ZIO.foreach(mapped)(x => downloadTimestamps(x.game_id).map(res => (x, res)))\n",
    "\n",
    "      val result = runZIO(\n",
    "        workflow\n",
    "          .tapError(err => { ZIO.succeed(emit(Text(\"error!\"), Text(err.getMessage))) })\n",
    "          .orElse(ZIO.succeed(List.empty))\n",
    "          .provide(zio.http.Client.default)\n",
    "      ).map(x => Result(x._1.game_id, x._2.map(y => PlayCount(y.head, y.last))))\n",
    "\n",
    "      result.foreach { x =>\n",
    "        emit(new Text(x.game_id.toString), Text(x.playcounts.toJson))\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  class MyReducer extends HadoopJob.HadoopReducer[Text, Text, Text] {\n",
    "    override def myReduce(key: Text, values: List[String], emit: (Text, Text) => Unit): Unit = {\n",
    "      emit(key, Text(values.toString()))\n",
    "    }\n",
    "  }\n",
    "\n",
    "  def main(args: Array[String]) = {\n",
    "\n",
    "    java.lang.System.setProperty(\"java.net.preferIPv4Stack\", \"true\")\n",
    "\n",
    "    val conf = new Configuration\n",
    "    val job  = Job.getInstance(conf, \"word count\")\n",
    "\n",
    "    job.setJarByClass(classOf[Main.type])\n",
    "    job.setMapperClass(classOf[MyMapper])\n",
    "    job.setReducerClass(classOf[MyReducer])\n",
    "    job.setOutputKeyClass(classOf[Text])\n",
    "    job.setOutputValueClass(classOf[Text])\n",
    "\n",
    "    FileInputFormat.addInputPath(job, new Path(args(0)))\n",
    "    FileOutputFormat.setOutputPath(job, new Path(args(1)))\n",
    "\n",
    "    java.lang.System.exit(\n",
    "      if (job.waitForCompletion(true)) 0\n",
    "      else 1\n",
    "    )\n",
    "  }\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
